{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d041b96",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "512e8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e1f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model import VAE_priorCategorical, VAE_priorHFM\n",
    "import metadata as md\n",
    "from train import train\n",
    "from datasets import Dataset_HFM, Dataset_pureHFM\n",
    "from utilities import sample_images\n",
    "from find_gauge import get_feature_frequencies_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d58f0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(md.seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(md.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1647",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09921586",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f7171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0bae",
   "metadata": {},
   "source": [
    "## train over FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2094def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682e2",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2822c7",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db343",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9adabe",
   "metadata": {},
   "source": [
    "# Prior Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7130fe",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b489f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac8c5d",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ced2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 387.2214\n",
      "Epoch: 1/15, Average loss: 273.8701\n",
      "Epoch: 2/15, Average loss: 226.0219\n",
      "Epoch: 3/15, Average loss: 198.7820\n",
      "Epoch: 4/15, Average loss: 179.1189\n",
      "Epoch: 5/15, Average loss: 165.0554\n",
      "Epoch: 6/15, Average loss: 155.2160\n",
      "Epoch: 7/15, Average loss: 147.9891\n",
      "Epoch: 8/15, Average loss: 142.7795\n",
      "Epoch: 9/15, Average loss: 138.7503\n",
      "Epoch: 10/15, Average loss: 135.5485\n",
      "Epoch: 11/15, Average loss: 132.9366\n",
      "Epoch: 12/15, Average loss: 130.8681\n",
      "Epoch: 13/15, Average loss: 129.1318\n",
      "Epoch: 14/15, Average loss: 127.7810\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b53d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 389.5810\n",
      "Epoch: 1/15, Average loss: 272.9245\n",
      "Epoch: 2/15, Average loss: 224.3924\n",
      "Epoch: 3/15, Average loss: 196.1950\n",
      "Epoch: 4/15, Average loss: 175.8703\n",
      "Epoch: 5/15, Average loss: 161.4573\n",
      "Epoch: 6/15, Average loss: 151.1268\n",
      "Epoch: 7/15, Average loss: 143.5675\n",
      "Epoch: 8/15, Average loss: 137.9279\n",
      "Epoch: 9/15, Average loss: 133.4935\n",
      "Epoch: 10/15, Average loss: 130.0795\n",
      "Epoch: 11/15, Average loss: 127.3539\n",
      "Epoch: 12/15, Average loss: 125.0256\n",
      "Epoch: 13/15, Average loss: 123.2869\n",
      "Epoch: 14/15, Average loss: 121.7311\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 396.2289\n",
      "Epoch: 1/15, Average loss: 274.0455\n",
      "Epoch: 2/15, Average loss: 225.6387\n",
      "Epoch: 3/15, Average loss: 197.5839\n",
      "Epoch: 4/15, Average loss: 177.1822\n",
      "Epoch: 5/15, Average loss: 162.4309\n",
      "Epoch: 6/15, Average loss: 152.1034\n",
      "Epoch: 7/15, Average loss: 144.3967\n",
      "Epoch: 8/15, Average loss: 138.8854\n",
      "Epoch: 9/15, Average loss: 134.5667\n",
      "Epoch: 10/15, Average loss: 131.3657\n",
      "Epoch: 11/15, Average loss: 128.9836\n",
      "Epoch: 12/15, Average loss: 126.8194\n",
      "Epoch: 13/15, Average loss: 125.1062\n",
      "Epoch: 14/15, Average loss: 123.7093\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 401.6955\n",
      "Epoch: 1/15, Average loss: 277.8787\n",
      "Epoch: 2/15, Average loss: 231.1103\n",
      "Epoch: 3/15, Average loss: 205.0764\n",
      "Epoch: 4/15, Average loss: 184.1718\n",
      "Epoch: 5/15, Average loss: 169.0195\n",
      "Epoch: 6/15, Average loss: 158.4487\n",
      "Epoch: 7/15, Average loss: 151.1160\n",
      "Epoch: 8/15, Average loss: 145.3018\n",
      "Epoch: 9/15, Average loss: 140.4248\n",
      "Epoch: 10/15, Average loss: 137.0231\n",
      "Epoch: 11/15, Average loss: 134.2711\n",
      "Epoch: 12/15, Average loss: 132.0752\n",
      "Epoch: 13/15, Average loss: 130.4365\n",
      "Epoch: 14/15, Average loss: 129.0076\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 406.0553\n",
      "Epoch: 1/15, Average loss: 286.5170\n",
      "Epoch: 2/15, Average loss: 245.0898\n",
      "Epoch: 3/15, Average loss: 228.9989\n",
      "Epoch: 4/15, Average loss: 220.7183\n",
      "Epoch: 5/15, Average loss: 215.8021\n",
      "Epoch: 6/15, Average loss: 212.7106\n",
      "Epoch: 7/15, Average loss: 210.7006\n",
      "Epoch: 8/15, Average loss: 209.3399\n",
      "Epoch: 9/15, Average loss: 208.4070\n",
      "Epoch: 10/15, Average loss: 207.7568\n",
      "Epoch: 11/15, Average loss: 207.2948\n",
      "Epoch: 12/15, Average loss: 206.9638\n",
      "Epoch: 13/15, Average loss: 206.7283\n",
      "Epoch: 14/15, Average loss: 206.5594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 406.0930\n",
      "Epoch: 1/15, Average loss: 286.7583\n",
      "Epoch: 2/15, Average loss: 245.0405\n",
      "Epoch: 3/15, Average loss: 228.9681\n",
      "Epoch: 4/15, Average loss: 220.7014\n",
      "Epoch: 5/15, Average loss: 215.8057\n",
      "Epoch: 6/15, Average loss: 212.7268\n",
      "Epoch: 7/15, Average loss: 210.7016\n",
      "Epoch: 8/15, Average loss: 209.3477\n",
      "Epoch: 9/15, Average loss: 208.4078\n",
      "Epoch: 10/15, Average loss: 207.7578\n",
      "Epoch: 11/15, Average loss: 207.2985\n",
      "Epoch: 12/15, Average loss: 206.9670\n",
      "Epoch: 13/15, Average loss: 206.7290\n",
      "Epoch: 14/15, Average loss: 206.5562\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b5160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 406.1312\n",
      "Epoch: 1/15, Average loss: 286.6929\n",
      "Epoch: 2/15, Average loss: 245.0724\n",
      "Epoch: 3/15, Average loss: 228.9505\n",
      "Epoch: 4/15, Average loss: 220.6820\n",
      "Epoch: 5/15, Average loss: 215.7933\n",
      "Epoch: 6/15, Average loss: 212.7011\n",
      "Epoch: 7/15, Average loss: 210.6946\n",
      "Epoch: 8/15, Average loss: 209.3343\n",
      "Epoch: 9/15, Average loss: 208.4053\n",
      "Epoch: 10/15, Average loss: 207.7541\n",
      "Epoch: 11/15, Average loss: 207.2952\n",
      "Epoch: 12/15, Average loss: 206.9622\n",
      "Epoch: 13/15, Average loss: 206.7289\n",
      "Epoch: 14/15, Average loss: 206.5564\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 406.1138\n",
      "Epoch: 1/15, Average loss: 287.0436\n",
      "Epoch: 2/15, Average loss: 245.4392\n",
      "Epoch: 3/15, Average loss: 229.2813\n",
      "Epoch: 4/15, Average loss: 220.8981\n",
      "Epoch: 5/15, Average loss: 215.9010\n",
      "Epoch: 6/15, Average loss: 212.7603\n",
      "Epoch: 7/15, Average loss: 210.7333\n",
      "Epoch: 8/15, Average loss: 209.3567\n",
      "Epoch: 9/15, Average loss: 208.4134\n",
      "Epoch: 10/15, Average loss: 207.7604\n",
      "Epoch: 11/15, Average loss: 207.2899\n",
      "Epoch: 12/15, Average loss: 206.9614\n",
      "Epoch: 13/15, Average loss: 206.7287\n",
      "Epoch: 14/15, Average loss: 206.5513\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 395.7954\n",
      "Epoch: 1/15, Average loss: 273.6304\n",
      "Epoch: 2/15, Average loss: 224.9896\n",
      "Epoch: 3/15, Average loss: 197.5184\n",
      "Epoch: 4/15, Average loss: 177.4550\n",
      "Epoch: 5/15, Average loss: 163.1144\n",
      "Epoch: 6/15, Average loss: 153.2959\n",
      "Epoch: 7/15, Average loss: 146.2907\n",
      "Epoch: 8/15, Average loss: 141.3657\n",
      "Epoch: 9/15, Average loss: 137.5538\n",
      "Epoch: 10/15, Average loss: 134.8549\n",
      "Epoch: 11/15, Average loss: 132.4067\n",
      "Epoch: 12/15, Average loss: 130.7438\n",
      "Epoch: 13/15, Average loss: 129.3027\n",
      "Epoch: 14/15, Average loss: 127.9742\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 404.6317\n",
      "Epoch: 1/15, Average loss: 286.6195\n",
      "Epoch: 2/15, Average loss: 245.0425\n",
      "Epoch: 3/15, Average loss: 228.9936\n",
      "Epoch: 4/15, Average loss: 220.7149\n",
      "Epoch: 5/15, Average loss: 215.8072\n",
      "Epoch: 6/15, Average loss: 212.7196\n",
      "Epoch: 7/15, Average loss: 210.7017\n",
      "Epoch: 8/15, Average loss: 209.3442\n",
      "Epoch: 9/15, Average loss: 208.4157\n",
      "Epoch: 10/15, Average loss: 207.7630\n",
      "Epoch: 11/15, Average loss: 207.2950\n",
      "Epoch: 12/15, Average loss: 206.9689\n",
      "Epoch: 13/15, Average loss: 206.7273\n",
      "Epoch: 14/15, Average loss: 206.5598\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7fc8e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 398.0999\n",
      "Epoch: 1/15, Average loss: 274.7463\n",
      "Epoch: 2/15, Average loss: 225.8005\n",
      "Epoch: 3/15, Average loss: 198.8731\n",
      "Epoch: 4/15, Average loss: 178.7910\n",
      "Epoch: 5/15, Average loss: 164.1575\n",
      "Epoch: 6/15, Average loss: 154.1215\n",
      "Epoch: 7/15, Average loss: 147.2208\n",
      "Epoch: 8/15, Average loss: 142.1926\n",
      "Epoch: 9/15, Average loss: 138.4858\n",
      "Epoch: 10/15, Average loss: 135.6056\n",
      "Epoch: 11/15, Average loss: 133.4099\n",
      "Epoch: 12/15, Average loss: 131.6860\n",
      "Epoch: 13/15, Average loss: 130.0864\n",
      "Epoch: 14/15, Average loss: 128.7931\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b384a81",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57784a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_pureHFM\n",
    "val_loader = val_loader_pureHFM\n",
    "input_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ada8b8f",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c005f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM\n",
    "val_loader = val_loader_expandedHFM\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a2b00",
   "metadata": {},
   "source": [
    "# Prior HFM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee7b08",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a97c660",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ebb4a",
   "metadata": {},
   "source": [
    "### latent_dim = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1505e8f",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d20a552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 381.2235\n",
      "Epoch: 1/15, Average loss: 267.2092\n",
      "Epoch: 2/15, Average loss: 217.5806\n",
      "Epoch: 3/15, Average loss: 188.0966\n",
      "Epoch: 4/15, Average loss: 167.0413\n",
      "Epoch: 5/15, Average loss: 152.4087\n",
      "Epoch: 6/15, Average loss: 142.3722\n",
      "Epoch: 7/15, Average loss: 135.2797\n",
      "Epoch: 8/15, Average loss: 129.9808\n",
      "Epoch: 9/15, Average loss: 125.8942\n",
      "Epoch: 10/15, Average loss: 122.6558\n",
      "Epoch: 11/15, Average loss: 119.9359\n",
      "Epoch: 12/15, Average loss: 117.7380\n",
      "Epoch: 13/15, Average loss: 115.8091\n",
      "Epoch: 14/15, Average loss: 114.0555\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee27ae24",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d9ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 384.0407\n",
      "Epoch: 1/15, Average loss: 267.2638\n",
      "Epoch: 2/15, Average loss: 217.0715\n",
      "Epoch: 3/15, Average loss: 187.1329\n",
      "Epoch: 4/15, Average loss: 165.7336\n",
      "Epoch: 5/15, Average loss: 151.0280\n",
      "Epoch: 6/15, Average loss: 140.8218\n",
      "Epoch: 7/15, Average loss: 133.6518\n",
      "Epoch: 8/15, Average loss: 128.1828\n",
      "Epoch: 9/15, Average loss: 124.1656\n",
      "Epoch: 10/15, Average loss: 120.9789\n",
      "Epoch: 11/15, Average loss: 118.3214\n",
      "Epoch: 12/15, Average loss: 116.1554\n",
      "Epoch: 13/15, Average loss: 114.2890\n",
      "Epoch: 14/15, Average loss: 112.6553\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd61d5b",
   "metadata": {},
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce8fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 388.0177\n",
      "Epoch: 1/15, Average loss: 268.3313\n",
      "Epoch: 2/15, Average loss: 218.4126\n",
      "Epoch: 3/15, Average loss: 188.6445\n",
      "Epoch: 4/15, Average loss: 167.0999\n",
      "Epoch: 5/15, Average loss: 152.2866\n",
      "Epoch: 6/15, Average loss: 141.9315\n",
      "Epoch: 7/15, Average loss: 134.4886\n",
      "Epoch: 8/15, Average loss: 129.2678\n",
      "Epoch: 9/15, Average loss: 125.2033\n",
      "Epoch: 10/15, Average loss: 122.0196\n",
      "Epoch: 11/15, Average loss: 119.5765\n",
      "Epoch: 12/15, Average loss: 117.3921\n",
      "Epoch: 13/15, Average loss: 115.6194\n",
      "Epoch: 14/15, Average loss: 114.1646\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1efe33",
   "metadata": {},
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "173f34aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 393.6342\n",
      "Epoch: 1/15, Average loss: 269.6998\n",
      "Epoch: 2/15, Average loss: 219.6953\n",
      "Epoch: 3/15, Average loss: 190.3475\n",
      "Epoch: 4/15, Average loss: 169.2802\n",
      "Epoch: 5/15, Average loss: 154.4133\n",
      "Epoch: 6/15, Average loss: 144.3866\n",
      "Epoch: 7/15, Average loss: 137.1287\n",
      "Epoch: 8/15, Average loss: 132.0503\n",
      "Epoch: 9/15, Average loss: 128.1275\n",
      "Epoch: 10/15, Average loss: 125.1531\n",
      "Epoch: 11/15, Average loss: 122.7621\n",
      "Epoch: 12/15, Average loss: 121.0150\n",
      "Epoch: 13/15, Average loss: 119.3108\n",
      "Epoch: 14/15, Average loss: 118.1421\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bd709",
   "metadata": {},
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9162f318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 395.5249\n",
      "Epoch: 1/15, Average loss: 272.2222\n",
      "Epoch: 2/15, Average loss: 222.2275\n",
      "Epoch: 3/15, Average loss: 193.0322\n",
      "Epoch: 4/15, Average loss: 171.9084\n",
      "Epoch: 5/15, Average loss: 156.9654\n",
      "Epoch: 6/15, Average loss: 146.6863\n",
      "Epoch: 7/15, Average loss: 139.2100\n",
      "Epoch: 8/15, Average loss: 134.0216\n",
      "Epoch: 9/15, Average loss: 129.9386\n",
      "Epoch: 10/15, Average loss: 126.7252\n",
      "Epoch: 11/15, Average loss: 124.2133\n",
      "Epoch: 12/15, Average loss: 122.3124\n",
      "Epoch: 13/15, Average loss: 120.6604\n",
      "Epoch: 14/15, Average loss: 119.1750\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a280e4",
   "metadata": {},
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc004937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 401.8110\n",
      "Epoch: 1/15, Average loss: 276.7550\n",
      "Epoch: 2/15, Average loss: 227.4809\n",
      "Epoch: 3/15, Average loss: 199.2246\n",
      "Epoch: 4/15, Average loss: 177.7921\n",
      "Epoch: 5/15, Average loss: 162.2182\n",
      "Epoch: 6/15, Average loss: 151.3351\n",
      "Epoch: 7/15, Average loss: 143.7801\n",
      "Epoch: 8/15, Average loss: 138.3514\n",
      "Epoch: 9/15, Average loss: 134.4793\n",
      "Epoch: 10/15, Average loss: 131.3795\n",
      "Epoch: 11/15, Average loss: 129.0269\n",
      "Epoch: 12/15, Average loss: 127.2262\n",
      "Epoch: 13/15, Average loss: 125.5772\n",
      "Epoch: 14/15, Average loss: 124.3121\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2367c38",
   "metadata": {},
   "source": [
    "### latent_dim = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e3b15",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f139778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 387.0440\n",
      "Epoch: 1/15, Average loss: 272.8479\n",
      "Epoch: 2/15, Average loss: 225.5189\n",
      "Epoch: 3/15, Average loss: 199.2283\n",
      "Epoch: 4/15, Average loss: 180.2615\n",
      "Epoch: 5/15, Average loss: 166.6176\n",
      "Epoch: 6/15, Average loss: 156.7113\n",
      "Epoch: 7/15, Average loss: 149.4788\n",
      "Epoch: 8/15, Average loss: 144.0979\n",
      "Epoch: 9/15, Average loss: 140.0381\n",
      "Epoch: 10/15, Average loss: 136.9199\n",
      "Epoch: 11/15, Average loss: 134.4511\n",
      "Epoch: 12/15, Average loss: 132.6279\n",
      "Epoch: 13/15, Average loss: 131.0840\n",
      "Epoch: 14/15, Average loss: 130.0151\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b1a6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e517a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 387.5959\n",
      "Epoch: 1/15, Average loss: 272.1826\n",
      "Epoch: 2/15, Average loss: 224.7308\n",
      "Epoch: 3/15, Average loss: 197.6505\n",
      "Epoch: 4/15, Average loss: 177.7356\n",
      "Epoch: 5/15, Average loss: 163.1281\n",
      "Epoch: 6/15, Average loss: 152.7592\n",
      "Epoch: 7/15, Average loss: 145.3947\n",
      "Epoch: 8/15, Average loss: 139.8405\n",
      "Epoch: 9/15, Average loss: 135.6575\n",
      "Epoch: 10/15, Average loss: 132.5980\n",
      "Epoch: 11/15, Average loss: 130.0530\n",
      "Epoch: 12/15, Average loss: 128.1629\n",
      "Epoch: 13/15, Average loss: 126.4687\n",
      "Epoch: 14/15, Average loss: 125.1941\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36161cf",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d765a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 389.2973\n",
      "Epoch: 1/15, Average loss: 272.2578\n",
      "Epoch: 2/15, Average loss: 224.7092\n",
      "Epoch: 3/15, Average loss: 197.7860\n",
      "Epoch: 4/15, Average loss: 178.3064\n",
      "Epoch: 5/15, Average loss: 163.9842\n",
      "Epoch: 6/15, Average loss: 153.8460\n",
      "Epoch: 7/15, Average loss: 146.4737\n",
      "Epoch: 8/15, Average loss: 141.0582\n",
      "Epoch: 9/15, Average loss: 137.0902\n",
      "Epoch: 10/15, Average loss: 133.8126\n",
      "Epoch: 11/15, Average loss: 131.3879\n",
      "Epoch: 12/15, Average loss: 129.5232\n",
      "Epoch: 13/15, Average loss: 127.8610\n",
      "Epoch: 14/15, Average loss: 126.6031\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac66cf",
   "metadata": {},
   "source": [
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1252379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 396.3191\n",
      "Epoch: 1/15, Average loss: 272.8376\n",
      "Epoch: 2/15, Average loss: 224.6855\n",
      "Epoch: 3/15, Average loss: 197.7511\n",
      "Epoch: 4/15, Average loss: 178.2524\n",
      "Epoch: 5/15, Average loss: 164.3228\n",
      "Epoch: 6/15, Average loss: 154.5431\n",
      "Epoch: 7/15, Average loss: 147.5874\n",
      "Epoch: 8/15, Average loss: 142.6433\n",
      "Epoch: 9/15, Average loss: 138.8007\n",
      "Epoch: 10/15, Average loss: 136.0297\n",
      "Epoch: 11/15, Average loss: 133.7757\n",
      "Epoch: 12/15, Average loss: 132.2615\n",
      "Epoch: 13/15, Average loss: 130.5843\n",
      "Epoch: 14/15, Average loss: 129.4097\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a97cbc",
   "metadata": {},
   "source": [
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316bfc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 394.5280\n",
      "Epoch: 1/15, Average loss: 274.2076\n",
      "Epoch: 2/15, Average loss: 226.2545\n",
      "Epoch: 3/15, Average loss: 199.6100\n",
      "Epoch: 4/15, Average loss: 180.4173\n",
      "Epoch: 5/15, Average loss: 166.7032\n",
      "Epoch: 6/15, Average loss: 157.1782\n",
      "Epoch: 7/15, Average loss: 150.2692\n",
      "Epoch: 8/15, Average loss: 145.1395\n",
      "Epoch: 9/15, Average loss: 141.3832\n",
      "Epoch: 10/15, Average loss: 138.5214\n",
      "Epoch: 11/15, Average loss: 136.2802\n",
      "Epoch: 12/15, Average loss: 134.5237\n",
      "Epoch: 13/15, Average loss: 133.1168\n",
      "Epoch: 14/15, Average loss: 132.0541\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc3eea2",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 6, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09d9b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 404.0643\n",
      "Epoch: 1/15, Average loss: 279.7820\n",
      "Epoch: 2/15, Average loss: 231.1276\n",
      "Epoch: 3/15, Average loss: 204.6358\n",
      "Epoch: 4/15, Average loss: 185.7424\n",
      "Epoch: 5/15, Average loss: 171.7875\n",
      "Epoch: 6/15, Average loss: 161.4567\n",
      "Epoch: 7/15, Average loss: 154.0192\n",
      "Epoch: 8/15, Average loss: 148.8494\n",
      "Epoch: 9/15, Average loss: 144.7229\n",
      "Epoch: 10/15, Average loss: 141.8280\n",
      "Epoch: 11/15, Average loss: 139.6754\n",
      "Epoch: 12/15, Average loss: 137.8384\n",
      "Epoch: 13/15, Average loss: 136.3801\n",
      "Epoch: 14/15, Average loss: 134.8126\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld6_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=6, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld6_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0a6dc",
   "metadata": {},
   "source": [
    "### latent_dim = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a64c07",
   "metadata": {},
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b15fddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 383.1762\n",
      "Epoch: 1/15, Average loss: 268.6504\n",
      "Epoch: 2/15, Average loss: 219.5911\n",
      "Epoch: 3/15, Average loss: 190.6701\n",
      "Epoch: 4/15, Average loss: 170.3356\n",
      "Epoch: 5/15, Average loss: 156.1361\n",
      "Epoch: 6/15, Average loss: 146.3509\n",
      "Epoch: 7/15, Average loss: 139.3945\n",
      "Epoch: 8/15, Average loss: 134.1125\n",
      "Epoch: 9/15, Average loss: 130.0846\n",
      "Epoch: 10/15, Average loss: 126.7365\n",
      "Epoch: 11/15, Average loss: 123.9843\n",
      "Epoch: 12/15, Average loss: 121.7459\n",
      "Epoch: 13/15, Average loss: 119.8368\n",
      "Epoch: 14/15, Average loss: 118.3179\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f284440d",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9e6544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 385.1324\n",
      "Epoch: 1/15, Average loss: 268.4826\n",
      "Epoch: 2/15, Average loss: 219.0720\n",
      "Epoch: 3/15, Average loss: 189.9261\n",
      "Epoch: 4/15, Average loss: 169.2491\n",
      "Epoch: 5/15, Average loss: 154.7276\n",
      "Epoch: 6/15, Average loss: 144.5990\n",
      "Epoch: 7/15, Average loss: 137.3649\n",
      "Epoch: 8/15, Average loss: 131.8118\n",
      "Epoch: 9/15, Average loss: 127.4682\n",
      "Epoch: 10/15, Average loss: 123.9615\n",
      "Epoch: 11/15, Average loss: 121.0202\n",
      "Epoch: 12/15, Average loss: 118.4307\n",
      "Epoch: 13/15, Average loss: 116.2330\n",
      "Epoch: 14/15, Average loss: 114.2826\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab9cb1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fca54d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 387.0321\n",
      "Epoch: 1/15, Average loss: 268.9205\n",
      "Epoch: 2/15, Average loss: 219.2299\n",
      "Epoch: 3/15, Average loss: 189.8106\n",
      "Epoch: 4/15, Average loss: 168.9077\n",
      "Epoch: 5/15, Average loss: 154.3619\n",
      "Epoch: 6/15, Average loss: 144.4570\n",
      "Epoch: 7/15, Average loss: 137.2431\n",
      "Epoch: 8/15, Average loss: 131.8989\n",
      "Epoch: 9/15, Average loss: 127.8105\n",
      "Epoch: 10/15, Average loss: 124.7920\n",
      "Epoch: 11/15, Average loss: 122.3061\n",
      "Epoch: 12/15, Average loss: 120.3614\n",
      "Epoch: 13/15, Average loss: 118.5575\n",
      "Epoch: 14/15, Average loss: 117.1697\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5402b5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b552bcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 392.7803\n",
      "Epoch: 1/15, Average loss: 270.6597\n",
      "Epoch: 2/15, Average loss: 221.2276\n",
      "Epoch: 3/15, Average loss: 192.6594\n",
      "Epoch: 4/15, Average loss: 172.0431\n",
      "Epoch: 5/15, Average loss: 157.3659\n",
      "Epoch: 6/15, Average loss: 147.2441\n",
      "Epoch: 7/15, Average loss: 140.0963\n",
      "Epoch: 8/15, Average loss: 134.9187\n",
      "Epoch: 9/15, Average loss: 131.0110\n",
      "Epoch: 10/15, Average loss: 127.9503\n",
      "Epoch: 11/15, Average loss: 125.4441\n",
      "Epoch: 12/15, Average loss: 123.5264\n",
      "Epoch: 13/15, Average loss: 121.8168\n",
      "Epoch: 14/15, Average loss: 120.3490\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf31f0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, hidden layers = 5, decrease_rate = 0.55, g_HFM per KL = log2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da2b257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 396.9351\n",
      "Epoch: 1/15, Average loss: 272.2454\n",
      "Epoch: 2/15, Average loss: 222.7689\n",
      "Epoch: 3/15, Average loss: 194.1453\n",
      "Epoch: 4/15, Average loss: 173.3943\n",
      "Epoch: 5/15, Average loss: 158.8272\n",
      "Epoch: 6/15, Average loss: 148.9317\n",
      "Epoch: 7/15, Average loss: 141.8160\n",
      "Epoch: 8/15, Average loss: 136.6592\n",
      "Epoch: 9/15, Average loss: 132.9064\n",
      "Epoch: 10/15, Average loss: 129.9749\n",
      "Epoch: 11/15, Average loss: 127.5991\n",
      "Epoch: 12/15, Average loss: 125.6838\n",
      "Epoch: 13/15, Average loss: 124.1039\n",
      "Epoch: 14/15, Average loss: 122.7595\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aabc3a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f7ef060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 398.9619\n",
      "Epoch: 1/15, Average loss: 275.8184\n",
      "Epoch: 2/15, Average loss: 226.5427\n",
      "Epoch: 3/15, Average loss: 199.0038\n",
      "Epoch: 4/15, Average loss: 179.0022\n",
      "Epoch: 5/15, Average loss: 164.5804\n",
      "Epoch: 6/15, Average loss: 154.2868\n",
      "Epoch: 7/15, Average loss: 147.1191\n",
      "Epoch: 8/15, Average loss: 141.9896\n",
      "Epoch: 9/15, Average loss: 137.8861\n",
      "Epoch: 10/15, Average loss: 134.6356\n",
      "Epoch: 11/15, Average loss: 132.0193\n",
      "Epoch: 12/15, Average loss: 129.9379\n",
      "Epoch: 13/15, Average loss: 128.3240\n",
      "Epoch: 14/15, Average loss: 127.1255\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ad21b",
   "metadata": {},
   "source": [
    "### latent_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de2273",
   "metadata": {},
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5cdd62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 385.5374\n",
      "Epoch: 1/15, Average loss: 270.4076\n",
      "Epoch: 2/15, Average loss: 221.8838\n",
      "Epoch: 3/15, Average loss: 193.9957\n",
      "Epoch: 4/15, Average loss: 174.2846\n",
      "Epoch: 5/15, Average loss: 160.5452\n",
      "Epoch: 6/15, Average loss: 151.0899\n",
      "Epoch: 7/15, Average loss: 144.1675\n",
      "Epoch: 8/15, Average loss: 139.0345\n",
      "Epoch: 9/15, Average loss: 135.0356\n",
      "Epoch: 10/15, Average loss: 131.8010\n",
      "Epoch: 11/15, Average loss: 129.2508\n",
      "Epoch: 12/15, Average loss: 127.2344\n",
      "Epoch: 13/15, Average loss: 125.4366\n",
      "Epoch: 14/15, Average loss: 124.0299\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e307691",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26cd7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 385.5536\n",
      "Epoch: 1/15, Average loss: 270.0968\n",
      "Epoch: 2/15, Average loss: 221.6856\n",
      "Epoch: 3/15, Average loss: 193.5973\n",
      "Epoch: 4/15, Average loss: 173.3367\n",
      "Epoch: 5/15, Average loss: 158.6900\n",
      "Epoch: 6/15, Average loss: 148.3585\n",
      "Epoch: 7/15, Average loss: 140.6620\n",
      "Epoch: 8/15, Average loss: 134.9014\n",
      "Epoch: 9/15, Average loss: 130.4423\n",
      "Epoch: 10/15, Average loss: 126.8215\n",
      "Epoch: 11/15, Average loss: 123.9568\n",
      "Epoch: 12/15, Average loss: 121.7570\n",
      "Epoch: 13/15, Average loss: 119.8965\n",
      "Epoch: 14/15, Average loss: 118.3773\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd416b",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f398416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 386.0043\n",
      "Epoch: 1/15, Average loss: 270.1266\n",
      "Epoch: 2/15, Average loss: 221.6020\n",
      "Epoch: 3/15, Average loss: 193.5423\n",
      "Epoch: 4/15, Average loss: 173.0970\n",
      "Epoch: 5/15, Average loss: 158.8722\n",
      "Epoch: 6/15, Average loss: 148.5503\n",
      "Epoch: 7/15, Average loss: 141.1077\n",
      "Epoch: 8/15, Average loss: 135.6479\n",
      "Epoch: 9/15, Average loss: 131.4073\n",
      "Epoch: 10/15, Average loss: 128.0857\n",
      "Epoch: 11/15, Average loss: 125.3988\n",
      "Epoch: 12/15, Average loss: 123.2765\n",
      "Epoch: 13/15, Average loss: 121.5404\n",
      "Epoch: 14/15, Average loss: 120.0439\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74113c73",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1398a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 394.4963\n",
      "Epoch: 1/15, Average loss: 271.5268\n",
      "Epoch: 2/15, Average loss: 222.4439\n",
      "Epoch: 3/15, Average loss: 194.1568\n",
      "Epoch: 4/15, Average loss: 173.8201\n",
      "Epoch: 5/15, Average loss: 159.5451\n",
      "Epoch: 6/15, Average loss: 149.7136\n",
      "Epoch: 7/15, Average loss: 142.6117\n",
      "Epoch: 8/15, Average loss: 137.7010\n",
      "Epoch: 9/15, Average loss: 133.6084\n",
      "Epoch: 10/15, Average loss: 130.5410\n",
      "Epoch: 11/15, Average loss: 127.8862\n",
      "Epoch: 12/15, Average loss: 126.0378\n",
      "Epoch: 13/15, Average loss: 124.2359\n",
      "Epoch: 14/15, Average loss: 122.9636\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f525fb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, hidden layers = 5, decrease_rate = 0.55, g_HFM per KL = log2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7020f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 393.1247\n",
      "Epoch: 1/15, Average loss: 271.6345\n",
      "Epoch: 2/15, Average loss: 223.4829\n",
      "Epoch: 3/15, Average loss: 196.0521\n",
      "Epoch: 4/15, Average loss: 176.3467\n",
      "Epoch: 5/15, Average loss: 162.2854\n",
      "Epoch: 6/15, Average loss: 152.6328\n",
      "Epoch: 7/15, Average loss: 145.7995\n",
      "Epoch: 8/15, Average loss: 140.8579\n",
      "Epoch: 9/15, Average loss: 136.9776\n",
      "Epoch: 10/15, Average loss: 134.0712\n",
      "Epoch: 11/15, Average loss: 131.6691\n",
      "Epoch: 12/15, Average loss: 129.8249\n",
      "Epoch: 13/15, Average loss: 128.1605\n",
      "Epoch: 14/15, Average loss: 126.7619\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01306666",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca40810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 405.5456\n",
      "Epoch: 1/15, Average loss: 281.1150\n",
      "Epoch: 2/15, Average loss: 233.4960\n",
      "Epoch: 3/15, Average loss: 205.8958\n",
      "Epoch: 4/15, Average loss: 185.1832\n",
      "Epoch: 5/15, Average loss: 169.6680\n",
      "Epoch: 6/15, Average loss: 159.0213\n",
      "Epoch: 7/15, Average loss: 151.6857\n",
      "Epoch: 8/15, Average loss: 146.3823\n",
      "Epoch: 9/15, Average loss: 142.5077\n",
      "Epoch: 10/15, Average loss: 140.3031\n",
      "Epoch: 11/15, Average loss: 137.4579\n",
      "Epoch: 12/15, Average loss: 135.6326\n",
      "Epoch: 13/15, Average loss: 134.2292\n",
      "Epoch: 14/15, Average loss: 133.4637\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479d5643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8MAAAGNCAYAAAA1uKMDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS+ZJREFUeJzt3QeQXdV9x/GntpJ2Ja121XtviyQkUUw1GDAg2YApLrjiEHCZ2E5CjIPjGDvjDBkHtyQkccYJ2CEYOwYb22CHJkDGAgSSUO/SqqzaaqVVX7WXOXdGjND9/Zdz9Mq+t+f7mdFg/3V09777zi3/ve/+XodsNpvNAAAAAAAQkY5tvQIAAAAAABQbzTAAAAAAIDo0wwAAAACA6NAMAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6NAMAwAAAACiQzMMAAAAAIgOzTAAROaFF17I3HHHHZm6urpMTU1NpkuXLpk+ffpkzj///Myf/dmfZZ599tlMNptt69UsS7fddlumQ4cOmYceeqitV6VdYvsCAPKpc16XBgAoWY2NjZmPfexjmaeffjr5/0OGDMlcfPHFmerq6kxzc3NmyZIlmQceeCD5M3369Mz8+fPbepUBAAAKhmYYACKwZ8+ezCWXXJJZuXJlZuLEiZl//dd/zbznPe9JjXMN8fe+973Mo48+2ibrCQAAUCw0wwAQgS984QtJIzx69OjMH//4x+Tj0crkyZMz//mf/5n5zGc+U/R1BAAAKCaeGQaAdm7t2rWZRx55JPnf7q6v1Qifyj0/fLrLL788eV7TPXM8Z86czHXXXZfp169fpmPHjm97hvPQoUOZ73znO5kLLrgg07t370y3bt0yEyZMyNx9992ZXbt2vW2Z9957b7LM1prv1157LRnjPtZ97Nixt+ru2Wa3DgMGDEiee3ava9y4cZmPf/zjmZdeekku6/nnn8988IMfzAwdOjTTtWvXZP3PO++8ZD1OXbejR49mHn744eRj5e5Oeq9evTLdu3dPXscXv/jFTENDQ+ZMvPHGG8kyhw8fnvz82trazDXXXJN56qmngpflPtr+ta99LTNlypRMVVVVsrzBgwcnH33/+te/nryGU7nt5X4pMm3atEzfvn2T8W47fPjDH87MmzdP/oxvfOMbybZ3/3Wv+U//9E+Tn+G2xclfnJy0YsWKzEc/+tHMwIEDk/f87LPPzvzsZz+Tyx05cmSy3A0bNmR++ctfJp9acNu4Z8+eyTw7k+1xJtt369atmS996UuZ8ePHJ+tcWVmZGTZsWObKK6/M3H///We0DgCAMpIFALRr3//+910aVrampiZ7/PjxM17OZZddlizn85//fLZjx47Zurq67Ec+8pHs1VdfnX3kkUeSMVu2bMlOmTIlGVdbW5u96qqrsjfeeGN2xIgRSW3kyJHZDRs2vLXMrVu3ZisqKrJVVVXZ3bt3y5/7yU9+Mvm33/zmN9+qPfTQQ9kOHTokf971rndlP/zhD2evv/767IwZM7KdOnXKfulLX0ot5wtf+EKyHPdn2rRpybrPnDkzO3r06KQ2e/bst8Zu2rQpqVVXV2cvuOCC7Ac/+MHsrFmzsoMHD07q/fr1y65evTr1Mz71qU8lf//ggw/K98Ftt5M//5Zbbslecsklyes//fW9kwMHDmQnT5781rpcd911yeu5/PLLswMHDkzqp2/PMWPGJD9r+vTpyba66aabkvfQje3cuXP2F7/4Rern3Hvvvcnff/rTn06WO3z48OyHPvSh7Hve855kO7u/u//++7Nz587N9uzZMzthwoRkPS688MK3tvWjjz6aWu7J+fAXf/EXyX/PPffc7K233po9//zz3/p3//RP/1TQ7evm3sn3072uG264IZlHl156aTJ33XsPAGjfaIYBoJ37xCc+kVzwX3nllTkt52Qz7P488MADqb8/ceJE9uKLL07+/vbbb8/u3bv3rb87evRo9q677kr+zjVSp/rYxz6W1L/73e+mlrlz585s165ds126dEmal5NGjRqV/Js5c+ak/s327duz8+fPf1vNNVZufJ8+fbLPP/986t+8+uqr2Y0bN771/926P/HEE9mWlpa3jTty5Ej2nnvuSZblmmPfZu33v/990rj37ds3++KLL77t7xYtWpQdOnRo8u9eeOGFrI8f//jHyXjXzLt1OpX7hYdbzunr/stf/jLb1NSUWparu2bYbZuDBw/KZtj9+exnP5u8jyf9+te/TuquCXbN7be+9a1kDpz+S5ixY8eazbDbJg8//PDb/s41z67u1mnx4sUF276uOXa1O++8823r7bht+uyzz6bWGwDQvtAMA0A75xomd9Hv7tgpCxcuTJqM0/+c3miebIavuOIKuZzf/e53b92VO7VpOrVJO3k389Qm57XXXktq48aNSzUl9913X/J37q7hqSorK73v3Ll1cXdP3XIee+yxbD64O4ruLuSpDX9rzZq7e+3q6u6r8/Of/zz5+5tvvtnr53/72982f4FwJtz2dct78sknZTPs7pweOnQo9e+mTp2a/L27o3v6e+e2u7vD6v6+vr5eNsMf+MAH5Pq47eD+/o477ijY9nWfcHC1xx9/3NwuAID2jQAtAIjcpk2bMj/+8Y9TdffspnuW83S33HKLXM6TTz6Z/Pfmm2/OdO6cPr24Z4vf/e53J4nVLsTLPXPquGd2L7zwwszcuXMz//d//5e59tprk/qJEycy//7v/578b/f9x6c/0+yeXf7kJz+ZPPPpvgrKLd96jnTnzp3Jc7I33nhjJsSbb76Zee655zLr16/PHDhwIFknxz277P73mjVrkp/9Tl9p5Z57ds/ZumecFbetHbddfLht5nz7299OviP6/e9/f/J87Dtxz/2698k93+ueOT75DPbSpUuT/7qQtVmzZqX+nUsed8/Uns49o71o0aLMzJkzk2eAT+XmgHs2uKmpKfm57jne033qU5+S6+nqjz32WPIev5Mz3b5uDrlU9b/+679Ovlf76quvzvTo0eMdfx4AoP2gGQaAds41gY5rCBXXSLlm4KSrrroqaQAtrsFR1q1bl/z3b//2b5M/rTl9XVwolWuG/+Vf/uWtZvi3v/1tpr6+Pmk2L7rooreNd02MW+///u//Tv644CXXIF5xxRWZT3ziE29rvNwyHBd+dXrDZnGNr1uOC3dqzd69e99xWa6RdtvXBYu5UKfWWO+Rau6+8pWvZP7xH/8xaRzd63KNqQvPuuGGG5Km8PRfDnzzm9/M/P3f/30qWMvn9ahG1jnZPFp/794X5/Dhw/LvR40a1Wp98+bNmUJtX/f+PvPMM5n/+Z//SX6B06lTp0xdXV3yCyD3Cx83lwAA7RvNMAC0czNmzEgaxvnz5yd3M607qL7cHTjl5F1T10yMGTOm1WWcddZZb/v/rvn4q7/6q8zvfve7pLlxzdADDzwg7wo7kyZNSu5iPv3000lCtLvj5xKu3f/+u7/7uyTl2KVKn6l77rknaYRdkvQ//MM/JI22+6VCRUVF8veuOXfN+6m/RLCc3C6ucXRNV7649frsZz+b+c1vfpP5wx/+kHn55ZczDz74YPLHre/s2bOTlGnn8ccfTxKh3Tq4Xzi4Ru9kKrRrpL/61a9m7rvvPvP1vNOcyXVOWQq5fd06u8Rw99rd3XK3/dyff/u3f0v+uF8ouDngmmQAQPtEMwwA7Zy7g3rXXXdldu/enXzFjPv/heC+ksZxdyZdYxvCfaT2c5/7XPJVQe6u7x133JHctXMf/b311lvNf+M+0nvyY73uruZ3v/vd5A6o+6om95Fo1wyevGu5atWqpLnyuTv885//PPmv+2qgqVOnpv5+9erVwdvF/dz/+q//ymvj6O7Su69Lcn8c9xVJ7pcA7r/uI9RuW5z6etyd4TvvvDOn15NP7hcf7iuYTue+cslxX/1U6O3r7ga7P1/+8peT+eF+oeK+Isr9kuEnP/lJ5tOf/nTQ8gAA5YPvGQaAdm7s2LHJd8k6f/mXf5k8K1oI7rlR53//93+97uidzjWw7rlU19C47yl2y7j99tvNO9Gnc99T6+5+uu82PnjwYNL8Oueee25yV9d9RPZXv/qV17Lcc67OiBEjUn/nnmt2z6n6cndgXUO9b9++zO9///tMIbk7wp///OeT/71w4UKv17Njx47kFw9twX1iQXFN6KnP+hZr+7qG2n3HsGuGT9+GAID2h2YYACLgPnLsmmJ3B9B9xPfFF1+U49wdOZ/nNBV3R9g1Yy7MyN1NU8+/urvTLhTrZHDTqVzD6poQ17j9x3/8R3KH72RjdyrX6Lo7wGr57qPSe/bsST7aevKuoruD/Dd/8zfJ/3Z3RV966aXUv3N3Uk993e5j2M4///M/v22c+2i2+2hyqG9961vJf912cXccT+ca/1dffTX52LcP9/Fd9zpOfkT4JPc88MmG8NTG9+Trcdv1yJEjb9XdL0bcM8eF+gWJz+t49NFH31b7xS9+kYRnufft5B3vQmxf13C7cLXTuab6ZHCX+uUBAKD94GPSABCBmpqa5HlI12y6cCx3x801i9OmTUvupLrwIdcoL168OGkcpkyZktxRDeGaV3fn9X3ve1+STu2aGvcRWPcxZdeAuYAtt/zjx49nbrvtNpk47YK03J1hxy1HhXW5ZbmPfbuPtbr1dMFRXbp0SRr5V155JRnjmt9+/fq99W9c4rRrZF0jftlllyWhXC5Qy3202iUru3Vzz9iebKDvvffe5DlmFwTmPmLsnnF2d1Bds33ppZcmdyN9k58d9/zpD37wg2S9r7/++uQXE+7nV1dXJ029S612y3ehWC7V+J24X2a45blfILjX0r9//6SJc6/fLWfIkCGZu++++63xf/7nf540f+5j8qNHj85ccMEFSePsllNZWZn5kz/5k7e2ezG598V9DN79csO9j2vXrk2aVuf++++XH1HP1/Z1z1G7XwS499LtB24fcb+scfuJ++WASzt3H9cHALRfNMMAEAnXMD377LNJM/zII48kF/3u7qK70+pSf11olbtzejJJ90yebXWNhWvIHnrooeR5W/e1O+5OsXv21/2du6vqmhX1NT2Oa54HDhyY2bZtmwzOclxQkmtqXSO3YMGC5CO+rkF2y7/pppuSu8mnJwG7j7+6UCR399r9W7eO7iue3C8C3Ot2TdGpjZdbjlu+e+bWNVKuSXNNpPsYtnse2qdhVY2+Wy93t9k13u59cNvYvV7X0Lrm3zcAyv0ywX183AVnLVu2LFlX1/i5Xzy4xte9j+4rl05yr9FtK/dMtmvoXVK3+7muEXWvyW2btuCaYfdJhe9973uZX//618kvYtwvG1wjH/pse+j2dY2z2y7ulxouXM59IsHNU/f8sPulkbvLfDKADADQPnVwXzbc1isBAIDjmvX3vve9yV295cuXe38VEsqLu+PvvvLKBWhZX9UFAECh8cwwAKAkuI9Pu48nnwz6ohEGAACFxMekAQBtyn0vrvu49uuvv558dNk9B+yeYQUAACgk7gwDANqUe97VPWPs0pzddwO751lVuBYAAEA+8cwwAAAAACA63BkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESns+/ADh06FHZNgFNks9lMqevUqVNbrwIicvz48Uyp4zyBYiqH8wT7BIqpnPeJcttXrG2tXkfI2EJS69GhgOuQ63zMx7qdOHHiHcdwZxgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAETHO0ALAEollKFQSiXkAgCA9qhQ59N8BFpZSuEaoFDhaNkih66VwrY8HXeGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdAjQAtohAhHifh0o3jwo9r4GFFo+AofKbd9uL6+vvQkJxQo5bofOg1xDQpl3+X3N+b5W484wAAAAACA6NMMAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6pEkDkSQt5mPZhUxbbi8JpiHrTHp12yvme2D9rEKlVxcyFbsc9028XYxpuOW2vu1RoRKii51SHLJPtJd51yHwHFYu24I7wwAAAACA6NAMAwAAAACiQzMMAAAAAIgOzTAAAAAAIDoEaAElplQCmEKW3alTp1RtwIABcux1110n65WVlanar371Kzl248aNqdrx48czpYBQrNJUqPfFWm7Hjh1zDiHKNcylc+fO3ut24sQJOfbo0aOyrva3UgxGQWGPSWoulfI8KJV1K3YwFHJTLkFQ7zS/1LVaRUWFHNurV69UrUePHkHniebm5lRt//79cuyxY8fa7HqKO8MAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6NMMAAAAAgOhElyZtJZOpujVWpW6qRMXQn2cl01kpn6WQZqjWLTQxFeWz/ax1U8myAwcOlGNnzZol60OHDk3Vmpqa5NjHH388Vdu7d68cqxJ1SyV5Gm0v5DiojvMqndNablVVlRzbrVs373qXLl3kWLUeVmpn9+7dvcceOHBA1vft2+e9X5XCsStmoWnlIcnkat5Z1yxqfoTsP9Y+aF1/qYTbI0eOZHzlY97y7QL+SuE4UQrrEDL3na5du6Zqo0aNkmPHjRuXqq1fv16OnThxovcx3lrn+fPnp2qHDh3yTpMu1vvBnWEAAAAAQHRohgEAAAAA0aEZBgAAAABEh2YYAAAAABCdsgrQCgl7sB42t4JKVLBJ37595Vj1QLd68NtaB6dHjx7e66aWbYWdqLHWOlhBEmrZe/bs8V5GSFhHKYQVtKWQ15+PII5cg9tUUIMVrlJTUyPHDhkyRNYHDRqUqvXq1cv7dVRUVHiPVcEqre0rahuFBJ0RolK65wlVt0J91Bzr06ePHKvqw4YNk2NHjx4t62q8FWCijtvWWBVgsmzZMjl269atsr5x40bvEDvr/Ag/uR4/rOOaFYqlrk969uzpvQzruF1dXe31s6yQN2uO7dixQ47dvn17qtbc3CzHHj58OFVj3mZKNry13ISE7Fr7ZW1traxfe+21qdqtt97qvT03bdokxzY2NqZqjzzyiBz7vve9T9YbGhq899eQfiLfuDMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6NAMAwAAAACiQzMMAAAAAIhO53JKXuvSpYscayWvqXRmK+FWpXkOHDhQjlVJiVYi4oABA7wTda006aamplTt4MGDcqxKXrMSRa1lLFmyJFWbN2+eHFtfX++d1KvWrb0kBZ6pUkgZVqnRVppnSFK4lQaqktut/dhKPLfWOdftbi031/cpZLuhuGme6rxiJeeq88TUqVPl2DFjxqRq48aNCzpPqH1l3759cqyqhySejx8/3vt84Lz22mup2qpVq7wTgK3zUsxCjgch36JhXTtZqc/q2mfUqFHeieeTJ0+WYwcPHuz9DQVWknNIEvqLL77oPVb9PGuOFup4Hsv1UMj2CzmGlQr1bQTW61DHeGtf++QnPynrM2fOTNX69+8vx6pr+fXr18uxixcv9v5mmccee0zWL7jgglRt27Zt3u+p9c05+cadYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdNo8TdpKWFN1Kw3USmFWabZWQrRKBLWSP1VKW2VlpRxr1VX6tJWO169fv1StoqLC+zVbadtWKpxa5127dsmxmzdvzilNGrkJTa9UyZ3W+5JrYrOVEmrtE2qeqiT11pJGfVn7T0tLi/cy8pEoqpZBwnRuQs8TKsl5+vTpcuxVV12VqtXV1Xkn9VoJ69YxWs0FlVRqLduao6pufdOCSgu29m9r3VQitUqYjkkx93MrTdp6z0eOHJmqXXTRRXLslClTUrXq6mo5trm5OVU7cOBA0D6hvonDeh3qeN7Y2Oi9btZ8LlQSejkmJ7fV3FfLKOR2UnMh5BrA+oaCG264IVX74Ac/6J3M7NTW1npfy6j9bZmRsL5hw4acvvXG6rlmzJghxy5YsCBVW7t2rRyb7/eaO8MAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6nUv1IfaQ5VoP4/ft29c7FEvVR4wY4R0AZAUqHD582Du8ygq0UstWIV7OmDFjvB/cP3ToUMZXQ0NDziFLxQ48KFchIRqh+8TBgwe9g65yfW+soCwrREjN8y1bthQk3CsUoVblE7RoBXyMHTvWO5TkxhtvlGPHjx/vHbKjgkqs0KiQZVjnCRVcaG0LdV6ygseGDx8u6x//+Me93ycVrhJ7gFahqPfACgy0gq5UgJwKynKqqqq8AtOcxYsXp2r79++XY4cOHSrr5513nncw6qRJk1K1V155RY5dt25dppjhiTFfD+Vj+xWbuj6xgulUQK4KynLuuuuuVG306NE5XztZ54n169enahs3bpRjVdic1dNYQXjqtfTu3VuO3bRpU6q2Zs2aTDFwZxgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEJ2c06RzTb8LSY+z0i6tJE6V6GaNbWlpSdV2794tx6qEWyv11kph3r59e6q2b98+71TFmTNnyrEqGViljDrNzc2yvnLlSq/1DU31jSUpMVeF3E6FSmtUy1X7X2vp1Wr+hyTO5iOFOx/Usks5JbOcqbk0atQoOfZDH/qQrF9zzTXe6bTqHLRz5045dtWqVanarl27vM8/ViKodV5S5zYrfVQl+FrnxlmzZsn62Wef7bUtrQRTdZ6J6TxRqOOEWob13tbW1sr6oEGDvOeSeh+ff/55733iyJEjQddO/fr1897nVYK8+vetbaNCiWWe53quLpVzp1oP6xsz1LHxK1/5ihw7YcIE7/OB1SOobwlZuHChHPvyyy97n1MOiW+cOXbsWFAvoHoH6xsK1DEm5P3PZZ/izjAAAAAAIDo0wwAAAACA6NAMAwAAAACiQzMMAAAAAIhOzgFaIYr1IPRJhw8f9qo5mzdv9g7FWrduXaq2adMmObaxsdH7wfSePXvKsZMmTfJ66N56oN8KIXr11Vdl/ZVXXknVDhw4IMceP348VYs5GCL09RcqHEK9L1bgkLW+IWEWKnzECiGqqKjwDoFQQT/WulnUWCtUrlCs9VXbM/b9JyQASIXhXHvttXLsddddJ+tDhgzxnh8qAGjevHly7OLFi73DR9TctwK0rGNxSKiJCi0aNmyYHDtx4kRZnzFjRqo2evRo7xDI2JVCMFDv3r1lvbq62nufqK+vT9U2bNjgfT3UpUuXoMBUFbJjhWKpeW69ZvV+hASEFhLnibanrs+t4LbPfOYz3tdD6hpHHfdbC9DasWNHqvbkk0/KsYsWLfLuU06I+W/1UFVVVd7LWLZsmRxrhVEWA3eGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADR6VzMlMRc0++shFwroVOlnlkJ0SpNTSU+O9u3b/dObLaS11Sq77hx4+TYm2++2SsB1UpPXLJkiRz7xBNPyHpDQ4P3tlfvKSmHpatQ71fnzp29UzutZODdu3d7p+wWKrXVWrdSTQ2PhZVAXldXl6rdcsstcuzgwYO9j9ErV66UY5966qlUbfbs2d7LtdJprfOEGt/S0pLz2JDEbpUWbJ0Tamtr5dgxY8Z4rUN7PH+Uwr5vzTurrpJhrdehjv3W/qrmR9++fb33bWsuWecaa7/yTa+2Eq0thZq77W2fKIV9xfpZVl2dP6666io5dvz48anatm3bvPuXNWvWeH/rjZXO/Oabb3pfZx0RfUMoq39RxwfVY1jz3DovqbG5pL9zZxgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESnc1s/7B8y1no42goJ2bNnT06BVlZo1L59+7xDvFQQhfOud70rVfvc5z4nx5599tneP++NN95I1X74wx/KsVawlnqYPiS0JZawB4sKYLC2iaqHBMuEBk4U6r1R+48VjGKFkqgQOmvehWyLUgiwCVFu61sMPXr0kPWZM2emahMmTAg6fyxevDhV+81vfiPHvvDCC6najh075Fh1jA45Dlh167xk1X1DTQ4cOCDHVldXey/D2rdD1q29yccxN+SYoOb50aNH5Vhr7u7cudM7IGfixIne1ydqjlmha9Z+PHDgQK/zj/W6rfdDzd1iH4tjv3Zq62uW1qiwrFmzZnmH7O7fv987FGvVqlVBYYa7du3yDuyygoGVLl26eAdaTZ482ft4ZJ0nrONGMXBnGAAAAAAQHZphAAAAAEB0aIYBAAAAANGhGQYAAAAARIdmGAAAAAAQnYKkSRc7Vc5KSmxubvZOp+3evbt3kqBKQuvdu7cce84558j6Zz/72VRt6tSpcqxajzfffFOO/clPfpKqLVy40Ds1uq0T3dqrkIToUkmUVOtsvQ61/6jUz9aW0djY6L1v58raxlbqrZV+qJAGnf/3pn///nLslClTvI/FK1askPVnnnkmVfvDH/4gxzY0NHh/Q0HIfpxrWrD186x5W1FR4Z0aPWjQIFnv1q2b9zlFbbdSOc6VK2v7qflhXfeo1Ftn/vz53sfzESNGeM0N65rMSoq36uqcUKjzhLX/hGx7S67fDtEe959CvSaVgGwd76xvgLn55pu9f566Pl+3bp33dY/6Jhzrm2ysZVjp1WpbVIjzgXXetd4j61gyatQor/OBlYpdrLnCnWEAAAAAQHRohgEAAAAA0aEZBgAAAABEh2YYAAAAABAdmmEAAAAAQHTKKk06JKnPSrbs3Fm/ZJW2aI3t2rVrqjZ9+nTv1Ghn2rRp3omIixYtStV++MMfyrHz5s1L1Q4cOBCUnJtrIiKKJ3T755p0bP17lfzZp0+foHVWaYvWXAxJui62XFNCkfFOsh0zZoz3/Fq6dKn3MdNKxlTJ0fk4XlrzI2Seq5RQK9VX7Zt1dXXeid3W+XHz5s1B2x75p+aYdW2xdetWWX/ttde8U3bHjh3rNRet/cdKzrVS2gcPHux9fFDrYSVrh3yLRsj+yvWUP7X98rFN1Nx9//vf7z2/rPTp2bNny7HLli1L1TZu3Oj9mq0kaOu8pMZb8662ttb7Nc+aNStVW716dSaE2r+t1Gj1OkL2tVzmCneGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdMoqQMt6ODokoMAKjVJ1FZRlBYp88YtflGOnTp0q6yrQYu7cuXLsj370o1RtwYIFcuzu3bu9gsTOJJCsmIEHsSj29lNzoaKiIufwnpqamlStV69eQUElKmAiZFtY68Z8LH8qvMSprKz0Dgtas2aNrO/bt8/7mKnmWMeOHQs279TPs4Idu3fv7h1ip85hN910kxw7YsQIWVfb+c033/QOgWS/zE3I9rPO9VZQjwoztIKnVPiOFdymjv179+71HmuFj1500UVybM+ePb33bXUNaG03q57rnOYclhsruG3ixImp2jXXXBO0Tzz66KOpWlNTkxxbX1/vdXy2+ozQ+aWu4ayfp7bF9ddfL8eee+65qdr69evl2MbGRu/zqxWgpfZ56/ya71BS7gwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKJTVmnSVnqYVVfJclYam0roHD16tBx79913p2ozZsyQY61k0+eeey5V++EPfyjHLl261DuBUf08K5UxhLXdSDlsW/lIn7TmqEootJL9VKKoSvpt7ec1NDS0i/mV75RD2N8CoOaoNdZ6X9Q8VSm01nwMSZwNOVdZ5yUrpX3cuHFeybvOBRdckKq9613vkmOt7blixYpU7ec//7kcu2PHDlmHn1yPg6HfHKESYFVCrpUim49v/rC+5WDIkCE5JfVa81mtc6lc9/CtHX6sY6NKjr7wwgvl2KeeekrWV61a5f0NBeqcUlVV5T1HrW8BsK77VQJ2//795diZM2emameddZYc29zc7LUdWkuZVu+JlTytzoPW/pqPb8M5FXeGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdMoqQMsKDLBCfVRdBZJYoQxf/epX5VgVlmWtg/Uw/ve//33vB9MPHz6c00PlVmhLyAPohDXEF8Ck3nNrnqvABxUM4Rw5ckTWd+/enclFSNiJFc6Sj7A533Uopfe61O3cuVPW1XHQCiqpq6uT9c2bN6dqNTU13uFvKrzEmkvWvLMCtGpra73DTi666KJUbezYsXKsCgg7ePCgHLtt2zZZ/8EPfpCqPf3000GheWhb1jWAmrvW/LCO576sc4oViqX2IWufV8fXlpaWnK+dSiVYq1wVajtZ1/fnnntuqjZo0CA5duvWrbKuQp+saxa1Hl26dJFjR4wY4T1H9+zZ471PWIGI1dXVqdqCBQvk2Llz53qFajkHDhyQ9aamJu/zgdpu+QiJ9cGdYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdMoqTTo0eVUlk6nUaOeLX/xiqnbhhRd6J38+++yzcux9990n6/X19V6p0VbKoZUCqRLWrORpkmyLp5TThK2Uw5AUZpXMaCU7WomIKo0wJEkwJF3QSkC1kk1zVQrvcznbvn2793F00qRJcuy0adNkvUePHt4poWqO5mPf7tu3r6z37t07VRs4cKB38rSVjq4Sov/4xz/Ksc8//7ysP/fcc6na3r17c/7mArQ99X5Zx8Zck8Kt5VrnD5UyXVlZKceq+W8tV62Htf+QGp2bQiUEW/NAHRutebtu3TpZ37dvn9e3aFjXSdbYXr16pWqHDh2SY7t16ybr6pxgXavNmTMnVVu+fLkcq77hZteuXUH7cci3kqjjTsicyGX+cGcYAAAAABAdmmEAAAAAQHRohgEAAAAA0aEZBgAAAABEp10EaFkBQCos6wtf+IIce9NNN3k/rP7KK6+kal/72tfk2PXr18u6CssKCcWyWGFZuS4X7YO1r/jub1ZARU1NjfdyVQiRNXdDghYsLS0tmWJS+1VImBL7ZVpjY6OsP/zww6nanXfeGRRSpYK1rOOoCk+03i+1DGveqlAga9lW8MuOHTtStRUrVsixTz31VKr20ksveYdtOc3NzTmdf1C61HG32CFoVniVqoecJ6xgIbXcQgU9WcuO5dgfEjqYj2DMzZs3p2ojRoyQY0OO0dYc7d+/v1dQo1NVVeUdoGUd+1Wo1aZNm+TYpqYmryDK1q7VlJDjg/We5nqMySWolDvDAAAAAIDo0AwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDolGyatEoFC0mNdu64445U7cMf/rB3otuqVavk2HvuuSdVW716dVC6Xa6pm7mkpp3JsmNJOSyUQr5fhVoPldBp7YOqbiUfqhRap3Pnzt4pobluT2u5+ZDrupXKXCkl+/fvl/VHHnnEK1XZuf7662V9ypQpqVrPnj3lWJVEa71f6thvpY9ar6+hocH7XPPGG2+kai+//LIcq9JDDx48KMda+3FI8ifnFFiseWDV1TdxWNdZIWnS6vxj7dsco3NTqO1nHatUUv6ECROCrg1UmrQ1R3v37p2q9erVy/vnqfTr1urq/GElUquE6CPG/lOohPBiJ9P74M4wAAAAACA6NMMAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6nUs1VU4l+9XW1sqxt956q6x/9KMfTdWqq6vl2G3btnmlRjtvvvmmd0poPlLT1DJCktvyleKI4sjHe9upUydZV6mBVkJ0SNKiSjNUqYWtpdaqdbZeh/p51nazlhEiZJ8IGUsqqR/r+Lpz585U7de//rX3cdtK+bz44ovl2L59+3qvm0pNt/aJ7du3y7r6RgP1mq2fp5J3rXNKIRM+OaeUJuv4o77twjqOFuq9teZjSBquuo5USb9WyrR1rgrB3M9tW1lzVI210qQXLFjglb7v3H777bL+5S9/2eubCKz5aKWYb926NVVbuXKlHLtlyxZZV8nR1rzL9ZtsSmX+5/vaiTvDAAAAAIDo0AwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOm0eoKUCDqyQg/PPP1+Ovfnmm2W9pqbGKyjL+cY3vpGqPfPMM3Ks9ZB+yEPeqm4FsaixIaECIeuQj1AnAiP8FWpbhSzXms8qLMsKFFm3bp1X+I+zfv1672AUK3BIHTeswJViz0f2ieJRYSD79u2TY5cvXy7r6rj7yiuvyLEqxNF6b9V+ZYWXWHNXLcPaX9W8K2QoFspfyHHJmksh1ych62CFNfoGCFn764ABA7zPKQQcFleu29u6XlizZk2q9uCDD8qxkydPlvW1a9emaqtXr5Zjr7nmGu9gRxWe2NTUJMe2tLTkHLLbnt//bA6vmTvDAAAAAIDo0AwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDodG7rpLCuXbvKsYMGDUrVPvCBD8ixw4YNk/WtW7emao8++qgcq5LeDh8+nHNiWchYKz0x11S4QqbKlWNiXbnKx7ZWcywkDXfnzp1y7COPPJKqzZ49W4619iuV9G6tW0hKbsh2K1R6KKmkhaHe23zMGSv1Wc3/Qh4DQ+YNydEopJBvlAhdRq4J63v27JFjKysrvZdbUVGRyRXXQ8Wj5p113N69e3eqtmTJkpy/daBTp05y7JNPPpmq7d+/X45lzpzZt+Hk+zqLO8MAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6RQ3QCtGjR49UbejQoXJsU1OTrM+fPz9Ve+KJJ7yXYT3YXqgALR6kj08pBCtZ66DmoxVQofaf5uZm7+Vayy72PpGPkBiUv1I5FpfKeqB8qGNVOYZoWkFXDQ0Nqdq8efPk2JEjR6Zq9fX1cmxLS0vwOqI8zrNqLu3duzdoGWrdQq6dUNq4MwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKJDMwwAAAAAiE6HrGfsWaFS3rp16ybrvXv3TtUuvfRSOfbYsWOyvmDBglRty5YtcuzRo0ffYU1RTOWQxtepU6e2XoWyVMrvbSmnRltJ3qWklLcf2p9SPpacxD5xZtvHqnfv3j1VGzVqlBw7ceLEVG3JkiVy7Pr161O1I0eOZMptPpbCOryTjh07eq97ofaffGynUl639qJDHraxlUx/Ku4MAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6LR5gJa1XOsB+5CHo3kIvXyVw3tHgFbbvechx6NihnIUEgFaQPmdJ9gnymMb5+P8UwrzsRTWIZ+hadbrKeX9qtjXHCE/L9d1yxZwfuW6jax181ln7gwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKLjnSYNAAAAAEB7wZ1hAAAAAEB0aIYBAAAAANGhGQYAAAAARIdmGAAAAAAQHZphAAAAAEB0aIYBAAAAANGhGQYAAAAARIdmGAAAAAAQnc6+Azt06FDYNQFOkc1mM6WOfQLFVA77RMeO/H4VxXPixIlMqeM8gWIqh/ME+wRKbZ/gygUAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAETHO0ALAPIRktGpUyfv8VbwgQrOscaWQ6AIAOCdhZwnAMAHd4YBAAAAANGhGQYAAAAARIdmGAAAAAAQHZphAAAAAEB0CNAC4K1jR/37sy5duqRqvXv3lmPr6upkfeTIkV7LdbZt25aqzZ8/X47duXNnqnb06FE5NiSIhdAWADGFHxaCdRy11qEUjrv52D6l8DpQmvOjHPeJcsedYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdNp1mrRKXrPS2FRKbqdOneRYlXBrje3WrZusq/GHDh2SYw8fPpyqHTt2TI49ceKEd9IcyblojdonrHTnfv36pWqXXXaZHHvDDTd4L2PNmjVy7KJFi1K1HTt2yLFHjhxJ1Zqbm+XYfKRMA7EK2U+KmVhc7tS2Ck2cLVWh6xuyLQqlvWz7mOVj3lnL6Ny5s3ePoMZa88tahrpuCekRLIUaW4rXU9wZBgAAAABEh2YYAAAAABAdmmEAAAAAQHRohgEAAAAA0elc7oE+rT1UrupWAFDv3r1TtbPOOkuOPe+881K1qVOneocCOcePH0/VlixZIsfOnj07VVu+fLkcu3PnTu9gLhUsZD0IbwUFlOKD8Ahjvbeq3qNHD+994rbbbpNjp02bJut79+5N1Xbv3u29bipozjoOdO3a1Xu/bK0O5EOhjqOFDPTh2F887WVbh8zHkECeUn6fQs6v7eV9LgfW+2L1CCGhWFVVVanamDFj5NhLLrkkVTv//PPl2Hnz5sn6nDlzUrWlS5fKsfv370/VKioqcp7P1vWX73KterH2Ce4MAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6NAMAwAAAACi0yHrGdVVyFRK359nrYNKebMSY6urq+XYcePGpWq33HKLHKuS3iZOnCjHduvWTdaPHTuWqu3Zs0eOnT9/fqr205/+VI59/fXXU7WtW7fKsVbKtErOtZIdC5X0Vg6pisXeJ4qd0l5ZWZmqnXPOOXLs17/+9VRt+vTpcuyBAwdkfe7cuanaU089JccuXLgwVWtqapJj1X7V0tLivV9a87/Yc7Qc9glrLsVIvV/WuapQx6OjR4/KsSoFNTS9t1DHv5B5Xg6Jw4XaTqVwTWbVrbHq+Gql94a85yHzoNjH0Xy8TyHrXA7niVKYu6Fp0upa3kqTrq2tTdVGjhzpfc60ru+tdVPfRPP444/LsSpl+vnnn5djVb9kfcOH9Y0buc7HfMxnn2Vw5QIAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKJTuDSPAjxIHxLgYNWtQKvx48enatOmTZNj+/Tp4x3ec/DgQVk/fPiw98P4/fr1S9WuvPJK77Cgbdu2BYXdWA/Co/ypfcIKZVChcLfffrsce/bZZ3uHmixfvlzWH3vssVTtjTfekGPV/maFBanQFitQISSIJeTYVQ6hJuVIbddiB9ZYoVgh62Edi1U9ZLnWOUW9Pmv/sY4PKqDSGqvW2TrP7N+/33tsLHKd06H/PmS8em+qqqrkWDU/rGOutQ4h+4SqWz9PvQ4rULGQ162lGuBYDvIR8mYdi625oNTX16dqw4YN8w7vVaG5rZ1rfvWrX3lfZ6meZNKkSXLs3r17U7Vdu3ZlclWKc5c7wwAAAACA6NAMAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6LR5mnQ+WMlkKkmzV69ecqxKU7MSLDdv3pyqrVy5Uo5dtmyZrB85ciRVGzJkiBw7efLkVG3gwIFy7JQpU7zXQb2OUk16Q36ofcKad+9///tTtauvvto7JXTFihVy7M9+9jNZf/XVV71T2tX+Y83bkPlspe+qJMl8pBajvFRUVHjPAzWXrLTlkBRmK+1UzXNrPqu0X+ubFkaPHi3rEyZMSNVGjRolxx44cCBVe/nll+XY2bNne6WaQrPmR8hYVbfSxqurqwtybLSSc9W6hewTLS0tcqyqh3y7gLUfq9R1axtZ21h9+0gs12nWXAr5JgHrOOi7XGvZ1lxS307T3Nwsx65du9b72smycePGVK2hocF7W/Ts2dN7H9yyZYv3NVk54c4wAAAAACA6NMMAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6ZZUmHZqe171791Rt2LBh3imh+/btk2PXrVuXqs2ZM8c75c1at4MHD8qxffv2TdXGjBkjx44bN847DXTHjh2yrhIUrWRtlCYrVVGlyJ5//vly7Ec+8pFUraamRo5ds2ZNqvb444/LsS+++KKs79q1yzuhUM3RkATTkMTI0GWjePLxvqgE2JB5YKUwq28uqK2t9T7GO4MGDUrVhg8f7r2MwYMHy7Hq/DFy5Eg51krDVccSa79S50wrQfull15K1WLf/0KSc0OSma26em8tKp3WWq66zrLmgbUO6trJGqtSpq1zikrJtVLMrX1e1fv16yfHNjY2eicAq3UOTbpub9T8D9knQlLXrW/dUHPR2l+tseqcYM3nkOs667it9rcRI0bIsfv370/Vtm3bJsdac7dcUs+5MwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKLT5gFaIUE2oaE3x44dS9V69Oghx1ZWVnoHaC1dujRV27BhgxxrhWKpdduzZ4930IJ6mN+prq72CmFp7YH+lpYWWUf5UKEmVsDaJz7xCe+QHStQRIXePPnkk3Ls9u3bcw4JUfu89ZpDgjKsY4yqEyrXPhw9etT72KjOE1ag1YwZM1K1c845R46tq6uTdRWAZYXYWeusqIAjFW7U2nmwqakpVdu6dascu2TJEu/QSXX+KZcQlkIJOd6pgBwrpErNZ+u4O3ToUO9lWHNULaN3795yrLVfqWscVbPWzZrPy5YtS9V27twpx06cOFHW1eu29om1a9emak8//bT3+9Hc3JyJQUgolnUMy8cy1Fy46KKLvM8p1r521llnpWpjx46VY3v27Cnrmzdv9qpZPdBwY19ToXJWMFe5484wAAAAACA6NMMAAAAAgOjQDAMAAAAAokMzDAAAAACIDs0wAAAAACA6bZ4mHcJKhLPSYlVKoZWs3KtXr1TtwIED3mmXIWl1VjqmlSatUux27Nghx1ZVVeWcJm0lBqP0WPNOzQNn5syZ3omIyuLFi2X9iSeeSNUaGhq8k9RDUm+tRENrPoekH1oJ0Sq500qKJ2W6/M8f1v6jUjcvv/xyOfbSSy/1TqG10n7VeUIlfFrpzvX19XKsmrsqAdVK2bXOQVZyrvpGBCsNN5ZvMwi5ZlBjreOaSla2kmzVtwtY81GlozujR4/2vubo06eP9+vo37+/d9pvSIrw/v375dgBAwZ4n3+mT5/u/fOsVN/nnnvOe+7/7Gc/8/pZ5S7kNanjtpU6r95H6zqkW7dusq7mmJrP1v5m7WtqLk2ZMkWOtebjyJEjU7VJkybJscOGDfNKtHbeeOONzOnmz5+fCXnvyuWbALgzDAAAAACIDs0wAAAAACA6NMMAAAAAgOjQDAMAAAAAotMuArQ6derkHXZiPSiuwhOs8BGlZ8+eQeusHoS3gsBUkJcVttW3b1+vWmsBFSqwC6XJmjNWWMN73/ter6A5Kwzn97//vRy7YsUK74AKKyxI1a39aujQod6hLT169PAOorDm/qpVq1K1TZs2eYcTWaFa6vhQLoETpco65lp1Nf9HjBghx5599tmp2nnnnecdamKFEy5cuFDWVVjJypUr5djt27enavv27ZNjjxw54j3WCtZS2/PEiRNyrJrT1jxn/vtd41jHu4EDB6ZqEyZMkGNrampk/d3vfrdX8I4VLGQdX9V7awUfqtA1a+5awVMqpM3aB9U11bRp0+TYNWvWeJ/DrNDJuXPnpmpz5syRY63Qr5ip4491zaHOv1ZwmzU/1HHeGltbW+sdzKX2FWv/sYIdVeCWFdymrp369evnvdyHHnooqA8LOfa3Je4MAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6NAMAwAAAACi0+Zp0lbCZ0hyrpXSplLTVGq0xUqKU8l+hw8flmOtdM1Dhw55vz61bGu5ffr08U5EVAm5ztq1a73WAW3PmvvWe15XV+c9l15//fVU7Q9/+IP3fLZSQq2EaJWCaiWYjh8/PlUbMmSIHKvWo7KyUo5taGjwTl5/7rnn5NgtW7bklLKLwpxTrBRzdcwcNWqU9341ePBg7zT22bNny7HWXFIJ0erbBaxjtJWuGpLEGiJkPltjQ64J2hvrGkAltVrHfnWNY6XFWknh6niuUpyd3bt3ey9XzbGNGzd6n3+seW5tNyshXVHnGutbANQxw9mxY4f361D7vNrfrfVoj+eOkNdkvTeKmh/WfLbWwXpvfK9ltm3b5v2tA6HHRrVPWGnZ6tsTKo3rIXUetL6Rx1o3dR60jg9tOae5MwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKLT5gFaIawAFCuop6qqyvsBbRV6s3r1ajl2586dqVpjY6McG/KguBWMoh6EP++88+TYvn37ej0w74wZM8Z7uzU3N8ux7THEoVSpEAgrjOrSSy+V9V69enkHxf3xj3/0mvvWnFEhEq3Nu7Fjx6Zqw4cPl2NVaJF6bdYctY4l1jLUtrcC6FSISmhYB86cChtq7T2vqalJ1SZOnOg9R62ft3nz5lStqalJjrXCTnr06OEVbmSxQk3UvMvHXGQ+57atrPdL1a1jse+/b62ujq9WCKAKFLWCq1QI0cqVK+XYTZs2ee/HnTt39t5/rOshda22YMECOdY6nqv9+7XXXpNj1TUj5wn/OarmoxWklutYa56HBBSuWbNGjj148KD3dd3o0aNlfenSpana/PnzM76GGOGjKoR48uTJQb2OmufW2LbEnWEAAAAAQHRohgEAAAAA0aEZBgAAAABEh2YYAAAAABAdmmEAAAAAQHRKNk1apbFZaaAqydZK+dyzZ493yqGV/qbS0awkaCtt7vjx496vT6X9Wol3KinRSna0Uvq6devmPRbFo9IP+/fvL8eOGzfOexkq9dbZsGGDd+rtyJEjU7W6ujo51krqVcmFKunX2rdVirO1bw4aNEiOtbbn4cOHvZdRWVnpnQBs7cc4c6HJuWqfsM4p6hhtJWOqJNtRo0bJsVZ66Pr16zO+1PwPmXehibWcE/JPXRdY7411Xlfz2bqWsdLz1Xjr+KquT6zrHnX9FZKKbX17iJWyq86D1s9T13ULFy6UY61U+IqKCu/3SR03Yk6NtoScI0O2n7Vcaxkq6ds6Bm7cuNF7n1DHaOt8YPUvzz77rHcS+rZt27y+JcHaf4YNG5ZRfvrTn8q6uma0zkttOf+5MwwAAAAAiA7NMAAAAAAgOjTDAAAAAIDo0AwDAAAAAKJDMwwAAAAAiE5R06RV8pqVxqbSYjt31qtrJdyqZbe0tMixDQ0NqdquXbvkWJUOuH///qB0SLVu1lhVV0ml1jZSCYfWNrbSKNH21Pui0sOtxE0r0VClCzoHDx5M1fr06SPHTpgwwTuh0FqGSne0kq7VvqkSHK3959xzz5Vjre2pEtatfdBKhUf+hSQaW0mVKm183bp13sdX6ziq5p11rhoyZIj3vFPJ5lY6rZWYaqV5FhNp1Jmc5q6VYq7mh5WgbM0P9U0C1rFRsdatubnZ+7qntrZW1qdNm+Z9PFfLrq+vl2O3bNniff6xXp86PljXnCH7BCnTuVHbNXSbqn0l5NtirHmu9s0lS5Z4L9eau2pfs9Z59erVcuyVV17pfRxQ36JhpXAX8n06U3Q9AAAAAIDo0AwDAAAAAKJDMwwAAAAAiA7NMAAAAAAgOm0eoGWFj6gQGisIqqqqyjtwyAq62rNnj1eAkFW3HhK3AipCtoUKUbEeYrdCxhTr9YU88I78s0I01Hy2wpqseaeCP6xAHrVfWSEJ/fr181rf1sJcVOCDNVYF3qlgOyucyDqWWGENqm6NZf8pHnW8s0KqrH2lsbExVVu0aJF3sJC1v6qAtcGDB8uxw4YN8w6bGzVqlBy7Y8eOVG3v3r0lG6Bl7T8xB2sVapuEhMdZYYTWXApZX3X+sfbLgQMHynpdXV2qVl1dLccuXrzYOzBS1a3wKyvIKOQ84fvvkXsgr++/b426PgkJA7WuC9T116ZNm7zHWnXrGlCNbRTnQOs1q16pteu9nj17ei9DHR+KFSrHnWEAAAAAQHRohgEAAAAA0aEZBgAAAABEh2YYAAAAABAdmmEAAAAAQHSKmiat0sasBOXu3bt7p0b36tVL1tV4Kx1QpZhZ6W/Hjh3LKYXW2hYqNdpKIB05cqT3z7NSo61URZW4Tcph21NzxkoXtN7b4cOHe6d2Tpo0yTuxWaUzW4m11jo3NTV51ax9UKVGW+mjVgqkOg4427dvT9V27dolx6rXbSU7wo+VKKnmnTWfrQRYdZyvr6+XY62kakUl/lvJuRMnTvQ+t1mpvmrZ1ryzkj9RmkJSitW+Ys07a59Qx1crcVZdZ1lp/Wr/sY7F55xzjqyr47y1T6jjtnVeUq85ZPvkI00a/tQ8DzmuWXPUem8V6/rE6kkUNT+s6ywrNV1d41v9kjondDa+hWb37t2p2tatWzMh1LqFHLuKde3EGREAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESnqAFa6uFo68FtVbceCO/Xr5+s9+3b1zukSj3wbo1V62Y9dG8FhFVWVqZqo0ePlmOvuOIK77AgFSBgBV9s2LDBO+CIEIjS1NzcLOurV6+W9alTp3oFtDmXX355qtbQ0OAdQmSF1alwPGtfUfuwtYxhw4bJsSpQyTrurF+/XtYXLlyYqi1fvtx7/yFAqzDUPJgyZUpQONrGjRu9A+jUPFfHcmvuTps2TY611lkF+Fj7tgo+tOZdSIiKFSIUEnTG+SM3IcFM6v2yArSs65OQa7UBAwZ4h9ip6yxrn1DnKuscZB231fnRCttS13DWNaAVSqqWwbG/eKz5HNJPhASsWdf96j235oGaSz169AgK0FLXPlu2bPG+PjnrrLMyvq655hpZX7x4say/+OKLOZ1TioU7wwAAAACA6NAMAwAAAACiQzMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6BQ1TVqlH6r0Yysl1EqhtVIOKyoqUrUJEybIsbt3707VRowYkfGlkjytdbASGK+99lo59sorr0zVunbtKsfu27cvVXvmmWfk2GXLlsm6SgEmDbTtqTRcK/X29ddfl/VzzjknVZs4caIcqxIGx4wZ4z3/rcRNlWZozTFrnlupkb77RH19vRz72muvede3bt0qx5ZiUmJ7pY6vkydPlmOt9FCVgtmnTx/vpFHrWwAuvvhir/2vtW9EWLduXaq2YsUK73OYSr+2WMmoVkJ0iHwsIwYhadxWOq16z60k9ZDjqJX6rK5l+vfv7516O336dDnWmrvq+qSpqUmOVSnT1jciqHPYnj17grabek9Crp1IY/cXsk3U+2IlrFtp44r1TRzqPGGtrzqHzZgxQ4697rrrZF19o8GiRYu896v+xv6q5vl3vvMdOdY6L4Vsi7ac59wZBgAAAABEh2YYAAAAABAdmmEAAAAAQHRohgEAAAAA0WnzAC0rbCbkQWprGeqh8J49e8qxV111Vao2dOhQOVaFFlnrYAVXjBs3zjtcpbq62isUyHnqqadStV/84hdy7M6dO2VdvRaCHYrH2qYq4MaaB/PmzZP1mpqaVO3GG2+UY1UQUd++feXYgQMHege8WAFaat5Zr0+FBW3evFmOXbNmTaq2ZMkSOXbp0qWyrsZb62a9bhRHr169ZN0KRFQBQNYcVcfi4cOHe4erWIGKjY2Nsv7CCy+kam+++aZ3AJB1XgoJ1kLx5ON8qt5zKyzowIEDsl5XV+d9PaTC5qxAHnWesAJQrZDDV199NVWbM2eOHLtx40bvub9r1y7v98M6xnM9VDwhoXxqjlnX5tZyVaCoNQ/Ucd4K2T3vvPNStcsvv1yOvfTSS2VdXYtYgYgqIKyXcc783e9+532NZAWK5hoqVyzcGQYAAAAARIdmGAAAAAAQHZphAAAAAEB0aIYBAAAAANGhGQYAAAAARKfN06StxDOVjmalDnbt2lXWt2/fnqpNmTJFjlWJoCpN10qFs1IZrTTP7t27eyc+quTcuXPnyrE/+tGPUrWGhgY5tqWlRdZLMekNmpWMaSX7qXRAa36oRMNp06bJsSpl2krOtda5qanJK+HTSndevHixHLtp0yavWmsJ6ypdmNTo4rGOSSoRVKWHO5WVlbKukm+t5Gk1tqqqyvv4umLFCjn22WeflfWnn37ae45a6ai+27NjR/27cc4HpSkk6dhKR7fec5Wwbl1nqSRalbpurZt1LWMlRKu6tU+ofTDkG0yKvU+wrxUmTbpbt27e5wNrX5k0aVKqNnLkSO9jsXV9P2rUqFTtkksukWN79+7tvc4HDx703l87GvP8scce875+s3q5cpnT3BkGAAAAAESHZhgAAAAAEB2aYQAAAABAdGiGAQAAAADRoRkGAAAAAESnZNOkVTqzlZZppco98cQT3onUs2bNStUGDx7snZyrEtpaS5nes2dPqrZ8+XI59pe//GWqNnv2bDlWJQOr9OvW0nDLJf0tNiHvi0pjt/YhlbruvPLKK6laTU2NHNuvXz/vNOmQZGDrdaj9p7m5WY5V899KRGSfKE3WMV7NDyuF1joOqjltHfvVPLdS+VevXp2qLV26VI5du3atrKs0dSvtVK2HNW+t9NCQbc8+UT5putZxdNy4cbK+bNkyr9Rba95t2bJFjq2vr0/VVq5cKccuXLhQ1hsbG72vDa3jvO/2DJ3j+VgGzlynTp1kXSU59+jRQ461EqLnzZuXqk2fPl2OVd8w0KdPH+9vKFDfNtPaN3+o3mHDhg3er2Pz5s1yrOrPrOuskJT2UsSdYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEJ0OWc+nm0PCGoJWIGC5VuiHFdSjHqYfMGCAHDt+/PhU7ZxzzpFjhw4d6v06rACtFStWpGpvvPGGHLt169ZUbffu3d6BEfkIBSp2iEo5PHRfqH2ikNQ+Yb0ONW+sscU+Pqhgh3KYM7koh9cXEswUImTeWeEjAwcOlPVu3bqlal27dvUeqwJ9rKCRQ4cOBQX9qFASa6zat/MxZ0p53lnntlJSqGOjFQaq6tZ8rqys9A4otMITzz77bK9rFqepqcn7GskKvFP7hBXEGrLt1Twv9jm+ve+v+dyuahlWgJY6bqtQLWusdTy3zneqR7ACu4YMGZKqDR8+PGjd1L7y29/+Vo5Vr3ufEbCnXrMVlGXVS4HPPsGdYQAAAABAdGiGAQAAAADRoRkGAAAAAESHZhgAAAAAEB2aYQAAAABAdNo8TbrY8vE6QhJTc00ztBIzyyExMBfl8Prayz4RwkprVNsidPtYiaDlOj/yLeY0aYuaY9Y6WEmcary1rVX98OHDBTtuF/MYUw7zK9Y06ZDkXJUWa6VGW4nUai4cPXrUe/9RadTWckP2NWtblMrczcf1Xq5KZVuU+rfT5GMZoXO3UOuW6zmsQ8C3xZTD/DodadIAAAAAAAg0wwAAAACA6NAMAwAAAACiQzMMAAAAAIiOTk9ox/Lx8Pfx48fzsi5AuWHuo5SEBHy0tLR4LyMfP6+UQ/fKMQQlZur9so7FKljLmnfWMlRYljX2yJEj3j9PscbmIxyt2IFW7Fdtq5DvYa5zupBzI9frsg4lcl5qS9wZBgAAAABEh2YYAAAAABAdmmEAAAAAQHRohgEAAAAA0aEZBgAAAABEJ7o0aQBA+2Wl0Fr1YibOWj+LFFoUMln20KFDQftEx44dvRKmnWPHjmWKKdf0d+BM5PqtAflIWM/HfFb7NrgzDAAAAACIEM0wAAAAACA6NMMAAAAAgOjQDAMAAAAAokOAFgAgWsUM2SHQB20xx1paWoKWoQJ8VDCX9fOseV7MsDoLIXYotFznUj7mYsi+Bu4MAwAAAAAiRDMMAAAAAIgOzTAAAAAAIDo0wwAAAACA6NAMAwAAAACi0yFLhB4AAAAAIDLcGQYAAAAARIdmGAAAAAAQHZphAAAAAEB0aIYBAAAAANGhGQYAAAAARIdmGAAAAAAQHZphAAAAAEB0aIYBAAAAANGhGQYAAAAAZGLz/13gUUDV2vWXAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d56bd83",
   "metadata": {},
   "source": [
    "## train over fashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fab93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_FashionMNIST\n",
    "val_loader = val_loader_FashionMNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0e04e",
   "metadata": {},
   "source": [
    "### latent_dim = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7328867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 390.5580\n",
      "Epoch: 1/15, Average loss: 328.8307\n",
      "Epoch: 2/15, Average loss: 301.4959\n",
      "Epoch: 3/15, Average loss: 282.8894\n",
      "Epoch: 4/15, Average loss: 270.0157\n",
      "Epoch: 5/15, Average loss: 261.8850\n",
      "Epoch: 6/15, Average loss: 256.6333\n",
      "Epoch: 7/15, Average loss: 252.8290\n",
      "Epoch: 8/15, Average loss: 250.1731\n",
      "Epoch: 9/15, Average loss: 248.1693\n",
      "Epoch: 10/15, Average loss: 246.2765\n",
      "Epoch: 11/15, Average loss: 244.8467\n",
      "Epoch: 12/15, Average loss: 243.6671\n",
      "Epoch: 13/15, Average loss: 242.5018\n",
      "Epoch: 14/15, Average loss: 241.4765\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17430dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 390.7023\n",
      "Epoch: 1/15, Average loss: 329.8947\n",
      "Epoch: 2/15, Average loss: 301.9531\n",
      "Epoch: 3/15, Average loss: 283.2363\n",
      "Epoch: 4/15, Average loss: 270.1758\n",
      "Epoch: 5/15, Average loss: 261.6072\n",
      "Epoch: 6/15, Average loss: 256.2617\n",
      "Epoch: 7/15, Average loss: 252.4125\n",
      "Epoch: 8/15, Average loss: 249.4742\n",
      "Epoch: 9/15, Average loss: 247.0539\n",
      "Epoch: 10/15, Average loss: 245.2985\n",
      "Epoch: 11/15, Average loss: 243.9185\n",
      "Epoch: 12/15, Average loss: 242.5683\n",
      "Epoch: 13/15, Average loss: 241.5314\n",
      "Epoch: 14/15, Average loss: 240.5676\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9e0e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 392.7611\n",
      "Epoch: 1/15, Average loss: 332.3344\n",
      "Epoch: 2/15, Average loss: 304.8491\n",
      "Epoch: 3/15, Average loss: 285.1961\n",
      "Epoch: 4/15, Average loss: 272.0609\n",
      "Epoch: 5/15, Average loss: 263.5342\n",
      "Epoch: 6/15, Average loss: 257.9186\n",
      "Epoch: 7/15, Average loss: 253.8324\n",
      "Epoch: 8/15, Average loss: 250.7719\n",
      "Epoch: 9/15, Average loss: 248.5465\n",
      "Epoch: 10/15, Average loss: 246.4548\n",
      "Epoch: 11/15, Average loss: 244.7143\n",
      "Epoch: 12/15, Average loss: 243.6010\n",
      "Epoch: 13/15, Average loss: 242.4122\n",
      "Epoch: 14/15, Average loss: 241.4052\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cfb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 396.8559\n",
      "Epoch: 1/15, Average loss: 334.7537\n",
      "Epoch: 2/15, Average loss: 309.1033\n",
      "Epoch: 3/15, Average loss: 288.9122\n",
      "Epoch: 4/15, Average loss: 274.7021\n",
      "Epoch: 5/15, Average loss: 265.8321\n",
      "Epoch: 6/15, Average loss: 259.9738\n",
      "Epoch: 7/15, Average loss: 256.1328\n",
      "Epoch: 8/15, Average loss: 252.9517\n",
      "Epoch: 9/15, Average loss: 250.8496\n",
      "Epoch: 10/15, Average loss: 249.0158\n",
      "Epoch: 11/15, Average loss: 247.4768\n",
      "Epoch: 12/15, Average loss: 246.1288\n",
      "Epoch: 13/15, Average loss: 245.3457\n",
      "Epoch: 14/15, Average loss: 244.0072\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ee9ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 407.5538\n",
      "Epoch: 1/15, Average loss: 340.1971\n",
      "Epoch: 2/15, Average loss: 314.3715\n",
      "Epoch: 3/15, Average loss: 298.1911\n",
      "Epoch: 4/15, Average loss: 284.1517\n",
      "Epoch: 5/15, Average loss: 272.4562\n",
      "Epoch: 6/15, Average loss: 265.0021\n",
      "Epoch: 7/15, Average loss: 259.9460\n",
      "Epoch: 8/15, Average loss: 256.7584\n",
      "Epoch: 9/15, Average loss: 253.5896\n",
      "Epoch: 10/15, Average loss: 251.3511\n",
      "Epoch: 11/15, Average loss: 249.8446\n",
      "Epoch: 12/15, Average loss: 248.3605\n",
      "Epoch: 13/15, Average loss: 247.4314\n",
      "Epoch: 14/15, Average loss: 246.2766\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b8885a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 403.7192\n",
      "Epoch: 1/15, Average loss: 344.3157\n",
      "Epoch: 2/15, Average loss: 318.1620\n",
      "Epoch: 3/15, Average loss: 302.7861\n",
      "Epoch: 4/15, Average loss: 290.4061\n",
      "Epoch: 5/15, Average loss: 278.3420\n",
      "Epoch: 6/15, Average loss: 270.3972\n",
      "Epoch: 7/15, Average loss: 265.0974\n",
      "Epoch: 8/15, Average loss: 261.1610\n",
      "Epoch: 9/15, Average loss: 258.2593\n",
      "Epoch: 10/15, Average loss: 256.0371\n",
      "Epoch: 11/15, Average loss: 253.8625\n",
      "Epoch: 12/15, Average loss: 252.1910\n",
      "Epoch: 13/15, Average loss: 251.3083\n",
      "Epoch: 14/15, Average loss: 249.6799\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee527f",
   "metadata": {},
   "source": [
    "\n",
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bf3c",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a60ff162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 393.8527\n",
      "Epoch: 1/15, Average loss: 333.0312\n",
      "Epoch: 2/15, Average loss: 307.6510\n",
      "Epoch: 3/15, Average loss: 290.6462\n",
      "Epoch: 4/15, Average loss: 277.5916\n",
      "Epoch: 5/15, Average loss: 268.8914\n",
      "Epoch: 6/15, Average loss: 263.5379\n",
      "Epoch: 7/15, Average loss: 259.7359\n",
      "Epoch: 8/15, Average loss: 256.8149\n",
      "Epoch: 9/15, Average loss: 254.4613\n",
      "Epoch: 10/15, Average loss: 252.5904\n",
      "Epoch: 11/15, Average loss: 250.8536\n",
      "Epoch: 12/15, Average loss: 249.6486\n",
      "Epoch: 13/15, Average loss: 248.5295\n",
      "Epoch: 14/15, Average loss: 247.4250\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d532d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf09cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 394.6638\n",
      "Epoch: 1/15, Average loss: 333.7562\n",
      "Epoch: 2/15, Average loss: 307.5900\n",
      "Epoch: 3/15, Average loss: 289.5058\n",
      "Epoch: 4/15, Average loss: 276.9208\n",
      "Epoch: 5/15, Average loss: 268.5197\n",
      "Epoch: 6/15, Average loss: 262.9799\n",
      "Epoch: 7/15, Average loss: 259.2604\n",
      "Epoch: 8/15, Average loss: 256.2368\n",
      "Epoch: 9/15, Average loss: 253.8237\n",
      "Epoch: 10/15, Average loss: 251.8655\n",
      "Epoch: 11/15, Average loss: 250.1550\n",
      "Epoch: 12/15, Average loss: 248.6765\n",
      "Epoch: 13/15, Average loss: 247.2953\n",
      "Epoch: 14/15, Average loss: 246.0844\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d3169",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "078fdb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 397.8299\n",
      "Epoch: 1/15, Average loss: 335.6916\n",
      "Epoch: 2/15, Average loss: 310.4764\n",
      "Epoch: 3/15, Average loss: 292.3859\n",
      "Epoch: 4/15, Average loss: 278.5663\n",
      "Epoch: 5/15, Average loss: 269.3754\n",
      "Epoch: 6/15, Average loss: 263.2122\n",
      "Epoch: 7/15, Average loss: 258.7293\n",
      "Epoch: 8/15, Average loss: 255.4832\n",
      "Epoch: 9/15, Average loss: 253.1279\n",
      "Epoch: 10/15, Average loss: 250.9017\n",
      "Epoch: 11/15, Average loss: 249.3471\n",
      "Epoch: 12/15, Average loss: 247.9899\n",
      "Epoch: 13/15, Average loss: 246.8600\n",
      "Epoch: 14/15, Average loss: 245.4575\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab729425",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e47c8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 398.3482\n",
      "Epoch: 1/15, Average loss: 336.9857\n",
      "Epoch: 2/15, Average loss: 312.6492\n",
      "Epoch: 3/15, Average loss: 295.5657\n",
      "Epoch: 4/15, Average loss: 281.1785\n",
      "Epoch: 5/15, Average loss: 271.4434\n",
      "Epoch: 6/15, Average loss: 265.2860\n",
      "Epoch: 7/15, Average loss: 261.1684\n",
      "Epoch: 8/15, Average loss: 258.2306\n",
      "Epoch: 9/15, Average loss: 256.0380\n",
      "Epoch: 10/15, Average loss: 253.8234\n",
      "Epoch: 11/15, Average loss: 252.1474\n",
      "Epoch: 12/15, Average loss: 250.8369\n",
      "Epoch: 13/15, Average loss: 249.6603\n",
      "Epoch: 14/15, Average loss: 248.8128\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b53882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d1160",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af2e2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 400.3358\n",
      "Epoch: 1/15, Average loss: 340.4148\n",
      "Epoch: 2/15, Average loss: 317.3384\n",
      "Epoch: 3/15, Average loss: 302.8063\n",
      "Epoch: 4/15, Average loss: 290.0570\n",
      "Epoch: 5/15, Average loss: 277.6104\n",
      "Epoch: 6/15, Average loss: 269.5549\n",
      "Epoch: 7/15, Average loss: 264.5253\n",
      "Epoch: 8/15, Average loss: 260.6654\n",
      "Epoch: 9/15, Average loss: 258.6063\n",
      "Epoch: 10/15, Average loss: 256.3294\n",
      "Epoch: 11/15, Average loss: 254.8966\n",
      "Epoch: 12/15, Average loss: 253.4899\n",
      "Epoch: 13/15, Average loss: 252.2174\n",
      "Epoch: 14/15, Average loss: 251.0709\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2d680",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52b93dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 402.9198\n",
      "Epoch: 1/15, Average loss: 345.1993\n",
      "Epoch: 2/15, Average loss: 321.5186\n",
      "Epoch: 3/15, Average loss: 306.7467\n",
      "Epoch: 4/15, Average loss: 296.3839\n",
      "Epoch: 5/15, Average loss: 283.5194\n",
      "Epoch: 6/15, Average loss: 274.3747\n",
      "Epoch: 7/15, Average loss: 267.8208\n",
      "Epoch: 8/15, Average loss: 263.7321\n",
      "Epoch: 9/15, Average loss: 260.8180\n",
      "Epoch: 10/15, Average loss: 258.8114\n",
      "Epoch: 11/15, Average loss: 256.8465\n",
      "Epoch: 12/15, Average loss: 254.9873\n",
      "Epoch: 13/15, Average loss: 254.0814\n",
      "Epoch: 14/15, Average loss: 252.5321\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a490788",
   "metadata": {},
   "source": [
    "### latent_dim = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bc2f",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54de308c",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75e70b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 389.0890\n",
      "Epoch: 1/20, Average loss: 325.6962\n",
      "Epoch: 2/20, Average loss: 296.0037\n",
      "Epoch: 3/20, Average loss: 276.8709\n",
      "Epoch: 4/20, Average loss: 264.3897\n",
      "Epoch: 5/20, Average loss: 256.0267\n",
      "Epoch: 6/20, Average loss: 250.3627\n",
      "Epoch: 7/20, Average loss: 246.3227\n",
      "Epoch: 8/20, Average loss: 243.3773\n",
      "Epoch: 9/20, Average loss: 240.8766\n",
      "Epoch: 10/20, Average loss: 239.0853\n",
      "Epoch: 11/20, Average loss: 237.3914\n",
      "Epoch: 12/20, Average loss: 236.1984\n",
      "Epoch: 13/20, Average loss: 235.3642\n",
      "Epoch: 14/20, Average loss: 234.4350\n",
      "Epoch: 15/20, Average loss: 233.6106\n",
      "Epoch: 16/20, Average loss: 232.8913\n",
      "Epoch: 17/20, Average loss: 232.4347\n",
      "Epoch: 18/20, Average loss: 232.0001\n",
      "Epoch: 19/20, Average loss: 231.4370\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33c17e",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3a7a863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 392.7184\n",
      "Epoch: 1/20, Average loss: 330.1752\n",
      "Epoch: 2/20, Average loss: 300.8050\n",
      "Epoch: 3/20, Average loss: 281.0163\n",
      "Epoch: 4/20, Average loss: 267.9777\n",
      "Epoch: 5/20, Average loss: 259.1248\n",
      "Epoch: 6/20, Average loss: 252.9723\n",
      "Epoch: 7/20, Average loss: 248.8830\n",
      "Epoch: 8/20, Average loss: 245.6547\n",
      "Epoch: 9/20, Average loss: 243.3275\n",
      "Epoch: 10/20, Average loss: 241.1693\n",
      "Epoch: 11/20, Average loss: 239.6135\n",
      "Epoch: 12/20, Average loss: 238.4788\n",
      "Epoch: 13/20, Average loss: 237.2878\n",
      "Epoch: 14/20, Average loss: 235.9741\n",
      "Epoch: 15/20, Average loss: 235.5782\n",
      "Epoch: 16/20, Average loss: 234.4863\n",
      "Epoch: 17/20, Average loss: 234.0487\n",
      "Epoch: 18/20, Average loss: 233.5277\n",
      "Epoch: 19/20, Average loss: 232.8822\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33256f52",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6428009e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 392.8488\n",
      "Epoch: 1/20, Average loss: 331.2440\n",
      "Epoch: 2/20, Average loss: 303.7108\n",
      "Epoch: 3/20, Average loss: 283.6276\n",
      "Epoch: 4/20, Average loss: 270.1510\n",
      "Epoch: 5/20, Average loss: 261.5015\n",
      "Epoch: 6/20, Average loss: 255.8282\n",
      "Epoch: 7/20, Average loss: 251.6353\n",
      "Epoch: 8/20, Average loss: 248.6680\n",
      "Epoch: 9/20, Average loss: 246.2480\n",
      "Epoch: 10/20, Average loss: 244.4681\n",
      "Epoch: 11/20, Average loss: 242.5636\n",
      "Epoch: 12/20, Average loss: 241.5544\n",
      "Epoch: 13/20, Average loss: 240.5480\n",
      "Epoch: 14/20, Average loss: 239.3995\n",
      "Epoch: 15/20, Average loss: 238.5651\n",
      "Epoch: 16/20, Average loss: 237.6737\n",
      "Epoch: 17/20, Average loss: 237.2265\n",
      "Epoch: 18/20, Average loss: 236.5799\n",
      "Epoch: 19/20, Average loss: 235.9901\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff333e97",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, hidden layers = 5, decrease_rate = 0.55, g_HFM per KL = log2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f7a1169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 397.4748\n",
      "Epoch: 1/20, Average loss: 333.8013\n",
      "Epoch: 2/20, Average loss: 308.7578\n",
      "Epoch: 3/20, Average loss: 288.8149\n",
      "Epoch: 4/20, Average loss: 274.2027\n",
      "Epoch: 5/20, Average loss: 265.1079\n",
      "Epoch: 6/20, Average loss: 259.0098\n",
      "Epoch: 7/20, Average loss: 254.6710\n",
      "Epoch: 8/20, Average loss: 251.5748\n",
      "Epoch: 9/20, Average loss: 249.3519\n",
      "Epoch: 10/20, Average loss: 247.4829\n",
      "Epoch: 11/20, Average loss: 245.3746\n",
      "Epoch: 12/20, Average loss: 244.7032\n",
      "Epoch: 13/20, Average loss: 242.9167\n",
      "Epoch: 14/20, Average loss: 241.8273\n",
      "Epoch: 15/20, Average loss: 241.1356\n",
      "Epoch: 16/20, Average loss: 240.2478\n",
      "Epoch: 17/20, Average loss: 239.6516\n",
      "Epoch: 18/20, Average loss: 238.8100\n",
      "Epoch: 19/20, Average loss: 239.0816\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e4f54",
   "metadata": {},
   "source": [
    "#### features = 20, g_model = log(2), epochs = 20, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=20, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld20_glog2_ep20_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09a48b",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664bb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_pureHFM\n",
    "val_loader = val_loader_pureHFM\n",
    "input_dim = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd45d2",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483b0f3",
   "metadata": {},
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b1299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.7243\n",
      "Epoch: 1/15, Average loss: 236.6423\n",
      "Epoch: 2/15, Average loss: 218.7505\n",
      "Epoch: 3/15, Average loss: 208.8073\n",
      "Epoch: 4/15, Average loss: 202.3695\n",
      "Epoch: 5/15, Average loss: 197.5880\n",
      "Epoch: 6/15, Average loss: 194.2041\n",
      "Epoch: 7/15, Average loss: 191.8665\n",
      "Epoch: 8/15, Average loss: 190.1675\n",
      "Epoch: 9/15, Average loss: 188.8633\n",
      "Epoch: 10/15, Average loss: 187.8459\n",
      "Epoch: 11/15, Average loss: 187.0252\n",
      "Epoch: 12/15, Average loss: 186.3370\n",
      "Epoch: 13/15, Average loss: 185.7570\n",
      "Epoch: 14/15, Average loss: 185.2364\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b6301",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09623d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.4261\n",
      "Epoch: 1/15, Average loss: 236.2003\n",
      "Epoch: 2/15, Average loss: 218.4437\n",
      "Epoch: 3/15, Average loss: 208.6953\n",
      "Epoch: 4/15, Average loss: 202.2707\n",
      "Epoch: 5/15, Average loss: 197.5945\n",
      "Epoch: 6/15, Average loss: 194.1586\n",
      "Epoch: 7/15, Average loss: 191.8542\n",
      "Epoch: 8/15, Average loss: 190.2589\n",
      "Epoch: 9/15, Average loss: 188.9879\n",
      "Epoch: 10/15, Average loss: 187.9349\n",
      "Epoch: 11/15, Average loss: 187.0725\n",
      "Epoch: 12/15, Average loss: 186.4054\n",
      "Epoch: 13/15, Average loss: 185.8500\n",
      "Epoch: 14/15, Average loss: 185.2787\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd401e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02ce9a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 287.3755\n",
      "Epoch: 1/15, Average loss: 237.0502\n",
      "Epoch: 2/15, Average loss: 219.5545\n",
      "Epoch: 3/15, Average loss: 209.5671\n",
      "Epoch: 4/15, Average loss: 202.7791\n",
      "Epoch: 5/15, Average loss: 197.9154\n",
      "Epoch: 6/15, Average loss: 194.5951\n",
      "Epoch: 7/15, Average loss: 192.2125\n",
      "Epoch: 8/15, Average loss: 190.5190\n",
      "Epoch: 9/15, Average loss: 189.2600\n",
      "Epoch: 10/15, Average loss: 188.1938\n",
      "Epoch: 11/15, Average loss: 187.2816\n",
      "Epoch: 12/15, Average loss: 186.8389\n",
      "Epoch: 13/15, Average loss: 186.0661\n",
      "Epoch: 14/15, Average loss: 185.5707\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8dfeb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0609fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.7288\n",
      "Epoch: 1/15, Average loss: 236.6928\n",
      "Epoch: 2/15, Average loss: 219.0489\n",
      "Epoch: 3/15, Average loss: 209.1980\n",
      "Epoch: 4/15, Average loss: 202.8524\n",
      "Epoch: 5/15, Average loss: 198.0753\n",
      "Epoch: 6/15, Average loss: 194.6767\n",
      "Epoch: 7/15, Average loss: 192.2970\n",
      "Epoch: 8/15, Average loss: 190.5979\n",
      "Epoch: 9/15, Average loss: 189.3843\n",
      "Epoch: 10/15, Average loss: 188.2781\n",
      "Epoch: 11/15, Average loss: 187.4554\n",
      "Epoch: 12/15, Average loss: 186.7519\n",
      "Epoch: 13/15, Average loss: 186.1111\n",
      "Epoch: 14/15, Average loss: 185.6430\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9903df",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, hidden layers = 5, decrease_rate = 0.55, g_HFM per KL = log2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbf42b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 287.7852\n",
      "Epoch: 1/15, Average loss: 237.5617\n",
      "Epoch: 2/15, Average loss: 220.1914\n",
      "Epoch: 3/15, Average loss: 210.3694\n",
      "Epoch: 4/15, Average loss: 203.5353\n",
      "Epoch: 5/15, Average loss: 198.6652\n",
      "Epoch: 6/15, Average loss: 195.2864\n",
      "Epoch: 7/15, Average loss: 193.0813\n",
      "Epoch: 8/15, Average loss: 191.3384\n",
      "Epoch: 9/15, Average loss: 189.9766\n",
      "Epoch: 10/15, Average loss: 188.9391\n",
      "Epoch: 11/15, Average loss: 188.0414\n",
      "Epoch: 12/15, Average loss: 187.3425\n",
      "Epoch: 13/15, Average loss: 186.8371\n",
      "Epoch: 14/15, Average loss: 186.1999\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc22dfd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e01de756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 292.6029\n",
      "Epoch: 1/15, Average loss: 238.5189\n",
      "Epoch: 2/15, Average loss: 220.9748\n",
      "Epoch: 3/15, Average loss: 210.9588\n",
      "Epoch: 4/15, Average loss: 204.0923\n",
      "Epoch: 5/15, Average loss: 199.1431\n",
      "Epoch: 6/15, Average loss: 195.4405\n",
      "Epoch: 7/15, Average loss: 192.9559\n",
      "Epoch: 8/15, Average loss: 191.2045\n",
      "Epoch: 9/15, Average loss: 189.9498\n",
      "Epoch: 10/15, Average loss: 189.1115\n",
      "Epoch: 11/15, Average loss: 187.8321\n",
      "Epoch: 12/15, Average loss: 187.3549\n",
      "Epoch: 13/15, Average loss: 186.6793\n",
      "Epoch: 14/15, Average loss: 186.0641\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1644b64",
   "metadata": {},
   "source": [
    "### latent_dim = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a39e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.8464\n",
      "Epoch: 1/15, Average loss: 236.1935\n",
      "Epoch: 2/15, Average loss: 218.5763\n",
      "Epoch: 3/15, Average loss: 208.8317\n",
      "Epoch: 4/15, Average loss: 202.2423\n",
      "Epoch: 5/15, Average loss: 197.4519\n",
      "Epoch: 6/15, Average loss: 194.0614\n",
      "Epoch: 7/15, Average loss: 191.6466\n",
      "Epoch: 8/15, Average loss: 189.8962\n",
      "Epoch: 9/15, Average loss: 188.5487\n",
      "Epoch: 10/15, Average loss: 187.4814\n",
      "Epoch: 11/15, Average loss: 186.5976\n",
      "Epoch: 12/15, Average loss: 185.8504\n",
      "Epoch: 13/15, Average loss: 185.2475\n",
      "Epoch: 14/15, Average loss: 184.7187\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf447d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 283.7363\n",
      "Epoch: 1/15, Average loss: 235.5655\n",
      "Epoch: 2/15, Average loss: 217.9087\n",
      "Epoch: 3/15, Average loss: 208.3028\n",
      "Epoch: 4/15, Average loss: 201.9814\n",
      "Epoch: 5/15, Average loss: 197.2498\n",
      "Epoch: 6/15, Average loss: 193.8478\n",
      "Epoch: 7/15, Average loss: 191.4785\n",
      "Epoch: 8/15, Average loss: 189.7849\n",
      "Epoch: 9/15, Average loss: 188.5196\n",
      "Epoch: 10/15, Average loss: 187.5343\n",
      "Epoch: 11/15, Average loss: 186.6833\n",
      "Epoch: 12/15, Average loss: 185.9613\n",
      "Epoch: 13/15, Average loss: 185.3660\n",
      "Epoch: 14/15, Average loss: 184.8252\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 285.2223\n",
      "Epoch: 1/15, Average loss: 236.3018\n",
      "Epoch: 2/15, Average loss: 218.7822\n",
      "Epoch: 3/15, Average loss: 208.9726\n",
      "Epoch: 4/15, Average loss: 202.4610\n",
      "Epoch: 5/15, Average loss: 197.6650\n",
      "Epoch: 6/15, Average loss: 194.2955\n",
      "Epoch: 7/15, Average loss: 192.0910\n",
      "Epoch: 8/15, Average loss: 190.4163\n",
      "Epoch: 9/15, Average loss: 189.1646\n",
      "Epoch: 10/15, Average loss: 188.1475\n",
      "Epoch: 11/15, Average loss: 187.3214\n",
      "Epoch: 12/15, Average loss: 186.5558\n",
      "Epoch: 13/15, Average loss: 185.9756\n",
      "Epoch: 14/15, Average loss: 185.5028\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ccd4bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 287.0142\n",
      "Epoch: 1/15, Average loss: 236.6771\n",
      "Epoch: 2/15, Average loss: 219.3834\n",
      "Epoch: 3/15, Average loss: 209.4740\n",
      "Epoch: 4/15, Average loss: 202.7421\n",
      "Epoch: 5/15, Average loss: 197.9045\n",
      "Epoch: 6/15, Average loss: 194.7804\n",
      "Epoch: 7/15, Average loss: 192.4768\n",
      "Epoch: 8/15, Average loss: 190.7469\n",
      "Epoch: 9/15, Average loss: 189.4877\n",
      "Epoch: 10/15, Average loss: 188.5356\n",
      "Epoch: 11/15, Average loss: 187.6153\n",
      "Epoch: 12/15, Average loss: 186.8701\n",
      "Epoch: 13/15, Average loss: 186.3621\n",
      "Epoch: 14/15, Average loss: 185.8067\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd09879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 292.8941\n",
      "Epoch: 1/15, Average loss: 237.9449\n",
      "Epoch: 2/15, Average loss: 220.1433\n",
      "Epoch: 3/15, Average loss: 210.1393\n",
      "Epoch: 4/15, Average loss: 203.2156\n",
      "Epoch: 5/15, Average loss: 198.3725\n",
      "Epoch: 6/15, Average loss: 194.8614\n",
      "Epoch: 7/15, Average loss: 192.6915\n",
      "Epoch: 8/15, Average loss: 190.9345\n",
      "Epoch: 9/15, Average loss: 189.7620\n",
      "Epoch: 10/15, Average loss: 188.5940\n",
      "Epoch: 11/15, Average loss: 187.7056\n",
      "Epoch: 12/15, Average loss: 186.9935\n",
      "Epoch: 13/15, Average loss: 186.8883\n",
      "Epoch: 14/15, Average loss: 186.0274\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 290.9737\n",
      "Epoch: 1/15, Average loss: 238.1177\n",
      "Epoch: 2/15, Average loss: 220.4897\n",
      "Epoch: 3/15, Average loss: 210.6305\n",
      "Epoch: 4/15, Average loss: 203.6614\n",
      "Epoch: 5/15, Average loss: 198.8301\n",
      "Epoch: 6/15, Average loss: 195.4852\n",
      "Epoch: 7/15, Average loss: 193.0148\n",
      "Epoch: 8/15, Average loss: 191.4951\n",
      "Epoch: 9/15, Average loss: 189.9841\n",
      "Epoch: 10/15, Average loss: 188.9502\n",
      "Epoch: 11/15, Average loss: 188.2171\n",
      "Epoch: 12/15, Average loss: 187.2930\n",
      "Epoch: 13/15, Average loss: 186.7010\n",
      "Epoch: 14/15, Average loss: 186.2205\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld10_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=10, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld10_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e59333",
   "metadata": {},
   "source": [
    "### latent_dim = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a253641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.5432\n",
      "Epoch: 1/15, Average loss: 236.0743\n",
      "Epoch: 1/15, Average loss: 236.0743\n",
      "Epoch: 2/15, Average loss: 218.6306\n",
      "Epoch: 2/15, Average loss: 218.6306\n",
      "Epoch: 3/15, Average loss: 208.9190\n",
      "Epoch: 3/15, Average loss: 208.9190\n",
      "Epoch: 4/15, Average loss: 202.0166\n",
      "Epoch: 4/15, Average loss: 202.0166\n",
      "Epoch: 5/15, Average loss: 197.0756\n",
      "Epoch: 5/15, Average loss: 197.0756\n",
      "Epoch: 6/15, Average loss: 193.6763\n",
      "Epoch: 6/15, Average loss: 193.6763\n",
      "Epoch: 7/15, Average loss: 191.3216\n",
      "Epoch: 7/15, Average loss: 191.3216\n",
      "Epoch: 8/15, Average loss: 189.5958\n",
      "Epoch: 8/15, Average loss: 189.5958\n",
      "Epoch: 9/15, Average loss: 188.2371\n",
      "Epoch: 9/15, Average loss: 188.2371\n",
      "Epoch: 10/15, Average loss: 187.1204\n",
      "Epoch: 10/15, Average loss: 187.1204\n",
      "Epoch: 11/15, Average loss: 186.2441\n",
      "Epoch: 11/15, Average loss: 186.2441\n",
      "Epoch: 12/15, Average loss: 185.4375\n",
      "Epoch: 12/15, Average loss: 185.4375\n",
      "Epoch: 13/15, Average loss: 184.8307\n",
      "Epoch: 13/15, Average loss: 184.8307\n",
      "Epoch: 14/15, Average loss: 184.1982\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 184.1982\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e228ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.1642\n",
      "Epoch: 1/15, Average loss: 235.6598\n",
      "Epoch: 1/15, Average loss: 235.6598\n",
      "Epoch: 2/15, Average loss: 217.7534\n",
      "Epoch: 2/15, Average loss: 217.7534\n",
      "Epoch: 3/15, Average loss: 208.2301\n",
      "Epoch: 3/15, Average loss: 208.2301\n",
      "Epoch: 4/15, Average loss: 202.0851\n",
      "Epoch: 4/15, Average loss: 202.0851\n",
      "Epoch: 5/15, Average loss: 197.3137\n",
      "Epoch: 5/15, Average loss: 197.3137\n",
      "Epoch: 6/15, Average loss: 193.7529\n",
      "Epoch: 6/15, Average loss: 193.7529\n",
      "Epoch: 7/15, Average loss: 191.3909\n",
      "Epoch: 7/15, Average loss: 191.3909\n",
      "Epoch: 8/15, Average loss: 189.6400\n",
      "Epoch: 8/15, Average loss: 189.6400\n",
      "Epoch: 9/15, Average loss: 188.3069\n",
      "Epoch: 9/15, Average loss: 188.3069\n",
      "Epoch: 10/15, Average loss: 187.2378\n",
      "Epoch: 10/15, Average loss: 187.2378\n",
      "Epoch: 11/15, Average loss: 186.3681\n",
      "Epoch: 11/15, Average loss: 186.3681\n",
      "Epoch: 12/15, Average loss: 185.7713\n",
      "Epoch: 12/15, Average loss: 185.7713\n",
      "Epoch: 13/15, Average loss: 185.1249\n",
      "Epoch: 13/15, Average loss: 185.1249\n",
      "Epoch: 14/15, Average loss: 184.5224\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 184.5224\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae0f577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.3945\n",
      "Epoch: 1/15, Average loss: 236.0290\n",
      "Epoch: 1/15, Average loss: 236.0290\n",
      "Epoch: 2/15, Average loss: 218.3815\n",
      "Epoch: 2/15, Average loss: 218.3815\n",
      "Epoch: 3/15, Average loss: 208.5937\n",
      "Epoch: 3/15, Average loss: 208.5937\n",
      "Epoch: 4/15, Average loss: 202.1195\n",
      "Epoch: 4/15, Average loss: 202.1195\n",
      "Epoch: 5/15, Average loss: 197.3906\n",
      "Epoch: 5/15, Average loss: 197.3906\n",
      "Epoch: 6/15, Average loss: 194.0364\n",
      "Epoch: 6/15, Average loss: 194.0364\n",
      "Epoch: 7/15, Average loss: 191.7625\n",
      "Epoch: 7/15, Average loss: 191.7625\n",
      "Epoch: 8/15, Average loss: 190.0751\n",
      "Epoch: 8/15, Average loss: 190.0751\n",
      "Epoch: 9/15, Average loss: 188.7873\n",
      "Epoch: 9/15, Average loss: 188.7873\n",
      "Epoch: 10/15, Average loss: 187.8294\n",
      "Epoch: 10/15, Average loss: 187.8294\n",
      "Epoch: 11/15, Average loss: 186.9817\n",
      "Epoch: 11/15, Average loss: 186.9817\n",
      "Epoch: 12/15, Average loss: 186.2350\n",
      "Epoch: 12/15, Average loss: 186.2350\n",
      "Epoch: 13/15, Average loss: 185.6789\n",
      "Epoch: 13/15, Average loss: 185.6789\n",
      "Epoch: 14/15, Average loss: 185.2830\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 185.2830\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1734f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6daab63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.3178\n",
      "Epoch: 1/15, Average loss: 236.2829\n",
      "Epoch: 1/15, Average loss: 236.2829\n",
      "Epoch: 2/15, Average loss: 218.3552\n",
      "Epoch: 2/15, Average loss: 218.3552\n",
      "Epoch: 3/15, Average loss: 208.6827\n",
      "Epoch: 3/15, Average loss: 208.6827\n",
      "Epoch: 4/15, Average loss: 202.2258\n",
      "Epoch: 4/15, Average loss: 202.2258\n",
      "Epoch: 5/15, Average loss: 197.5253\n",
      "Epoch: 5/15, Average loss: 197.5253\n",
      "Epoch: 6/15, Average loss: 194.2341\n",
      "Epoch: 6/15, Average loss: 194.2341\n",
      "Epoch: 7/15, Average loss: 191.9013\n",
      "Epoch: 7/15, Average loss: 191.9013\n",
      "Epoch: 8/15, Average loss: 190.1920\n",
      "Epoch: 8/15, Average loss: 190.1920\n",
      "Epoch: 9/15, Average loss: 188.9356\n",
      "Epoch: 9/15, Average loss: 188.9356\n",
      "Epoch: 10/15, Average loss: 188.1370\n",
      "Epoch: 10/15, Average loss: 188.1370\n",
      "Epoch: 11/15, Average loss: 187.1928\n",
      "Epoch: 11/15, Average loss: 187.1928\n",
      "Epoch: 12/15, Average loss: 186.4941\n",
      "Epoch: 12/15, Average loss: 186.4941\n",
      "Epoch: 13/15, Average loss: 185.9425\n",
      "Epoch: 13/15, Average loss: 185.9425\n",
      "Epoch: 14/15, Average loss: 185.5083\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 185.5083\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95482bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 289.7564\n",
      "Epoch: 1/15, Average loss: 237.0259\n",
      "Epoch: 1/15, Average loss: 237.0259\n",
      "Epoch: 2/15, Average loss: 219.7418\n",
      "Epoch: 2/15, Average loss: 219.7418\n",
      "Epoch: 3/15, Average loss: 210.0837\n",
      "Epoch: 3/15, Average loss: 210.0837\n",
      "Epoch: 4/15, Average loss: 202.9634\n",
      "Epoch: 4/15, Average loss: 202.9634\n",
      "Epoch: 5/15, Average loss: 198.2069\n",
      "Epoch: 5/15, Average loss: 198.2069\n",
      "Epoch: 6/15, Average loss: 194.9044\n",
      "Epoch: 6/15, Average loss: 194.9044\n",
      "Epoch: 7/15, Average loss: 192.4980\n",
      "Epoch: 7/15, Average loss: 192.4980\n",
      "Epoch: 8/15, Average loss: 190.9749\n",
      "Epoch: 8/15, Average loss: 190.9749\n",
      "Epoch: 9/15, Average loss: 189.7428\n",
      "Epoch: 9/15, Average loss: 189.7428\n",
      "Epoch: 10/15, Average loss: 188.6268\n",
      "Epoch: 10/15, Average loss: 188.6268\n",
      "Epoch: 11/15, Average loss: 187.7101\n",
      "Epoch: 11/15, Average loss: 187.7101\n",
      "Epoch: 12/15, Average loss: 187.0131\n",
      "Epoch: 12/15, Average loss: 187.0131\n",
      "Epoch: 13/15, Average loss: 186.5094\n",
      "Epoch: 13/15, Average loss: 186.5094\n",
      "Epoch: 14/15, Average loss: 186.0085\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 186.0085\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a10243d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 286.2520\n",
      "Epoch: 1/15, Average loss: 236.8761\n",
      "Epoch: 1/15, Average loss: 236.8761\n",
      "Epoch: 2/15, Average loss: 219.5001\n",
      "Epoch: 2/15, Average loss: 219.5001\n",
      "Epoch: 3/15, Average loss: 209.7490\n",
      "Epoch: 3/15, Average loss: 209.7490\n",
      "Epoch: 4/15, Average loss: 203.1729\n",
      "Epoch: 4/15, Average loss: 203.1729\n",
      "Epoch: 5/15, Average loss: 198.4472\n",
      "Epoch: 5/15, Average loss: 198.4472\n",
      "Epoch: 6/15, Average loss: 194.9621\n",
      "Epoch: 6/15, Average loss: 194.9621\n",
      "Epoch: 7/15, Average loss: 192.6821\n",
      "Epoch: 7/15, Average loss: 192.6821\n",
      "Epoch: 8/15, Average loss: 191.2539\n",
      "Epoch: 8/15, Average loss: 191.2539\n",
      "Epoch: 9/15, Average loss: 189.7987\n",
      "Epoch: 9/15, Average loss: 189.7987\n",
      "Epoch: 10/15, Average loss: 188.6366\n",
      "Epoch: 10/15, Average loss: 188.6366\n",
      "Epoch: 11/15, Average loss: 187.8787\n",
      "Epoch: 11/15, Average loss: 187.8787\n",
      "Epoch: 12/15, Average loss: 187.1668\n",
      "Epoch: 12/15, Average loss: 187.1668\n",
      "Epoch: 13/15, Average loss: 186.4914\n",
      "Epoch: 13/15, Average loss: 186.4914\n",
      "Epoch: 14/15, Average loss: 186.0733\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 14/15, Average loss: 186.0733\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/pureHFM/ld12_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/pureHFM/ld12_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be859b91",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849abbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM_32_1024\n",
    "val_loader = val_loader_expandedHFM_32_1024\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81128433",
   "metadata": {},
   "source": [
    "### latent_dim = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b4f7f",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 1, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ced4ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 369.4557\n",
      "Epoch: 1/20, Average loss: 253.0922\n",
      "Epoch: 2/20, Average loss: 202.8551\n",
      "Epoch: 3/20, Average loss: 172.7191\n",
      "Epoch: 4/20, Average loss: 152.2356\n",
      "Epoch: 5/20, Average loss: 137.6990\n",
      "Epoch: 6/20, Average loss: 126.6692\n",
      "Epoch: 7/20, Average loss: 118.0560\n",
      "Epoch: 8/20, Average loss: 111.1311\n",
      "Epoch: 9/20, Average loss: 105.9597\n",
      "Epoch: 10/20, Average loss: 101.8351\n",
      "Epoch: 11/20, Average loss: 98.4729\n",
      "Epoch: 12/20, Average loss: 95.4016\n",
      "Epoch: 13/20, Average loss: 92.9106\n",
      "Epoch: 14/20, Average loss: 90.9402\n",
      "Epoch: 15/20, Average loss: 89.1270\n",
      "Epoch: 16/20, Average loss: 87.6784\n",
      "Epoch: 17/20, Average loss: 86.5530\n",
      "Epoch: 18/20, Average loss: 85.3882\n",
      "Epoch: 19/20, Average loss: 84.4254\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_1hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e5b3",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 2, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b165db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 393.1789\n",
      "Epoch: 1/20, Average loss: 300.7643\n",
      "Epoch: 2/20, Average loss: 241.3813\n",
      "Epoch: 3/20, Average loss: 207.7646\n",
      "Epoch: 4/20, Average loss: 179.9620\n",
      "Epoch: 5/20, Average loss: 160.1453\n",
      "Epoch: 6/20, Average loss: 144.9132\n",
      "Epoch: 7/20, Average loss: 133.3975\n",
      "Epoch: 8/20, Average loss: 125.1795\n",
      "Epoch: 9/20, Average loss: 118.3757\n",
      "Epoch: 10/20, Average loss: 112.1573\n",
      "Epoch: 11/20, Average loss: 107.3208\n",
      "Epoch: 12/20, Average loss: 103.3839\n",
      "Epoch: 13/20, Average loss: 99.9120\n",
      "Epoch: 14/20, Average loss: 96.7396\n",
      "Epoch: 15/20, Average loss: 94.4459\n",
      "Epoch: 16/20, Average loss: 92.6706\n",
      "Epoch: 17/20, Average loss: 90.7141\n",
      "Epoch: 18/20, Average loss: 88.7004\n",
      "Epoch: 19/20, Average loss: 87.0102\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1eed",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 3, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea231cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 399.2147\n",
      "Epoch: 1/20, Average loss: 324.6712\n",
      "Epoch: 2/20, Average loss: 275.4050\n",
      "Epoch: 3/20, Average loss: 239.8818\n",
      "Epoch: 4/20, Average loss: 216.3649\n",
      "Epoch: 5/20, Average loss: 198.0273\n",
      "Epoch: 6/20, Average loss: 182.2649\n",
      "Epoch: 7/20, Average loss: 170.8574\n",
      "Epoch: 8/20, Average loss: 162.9704\n",
      "Epoch: 9/20, Average loss: 154.4810\n",
      "Epoch: 10/20, Average loss: 146.7258\n",
      "Epoch: 11/20, Average loss: 139.4719\n",
      "Epoch: 12/20, Average loss: 134.2167\n",
      "Epoch: 13/20, Average loss: 129.6427\n",
      "Epoch: 14/20, Average loss: 125.4191\n",
      "Epoch: 15/20, Average loss: 121.2396\n",
      "Epoch: 16/20, Average loss: 118.2277\n",
      "Epoch: 17/20, Average loss: 115.6689\n",
      "Epoch: 18/20, Average loss: 112.8884\n",
      "Epoch: 19/20, Average loss: 111.4160\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_3hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde52c1d",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 4, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47314339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 405.6318\n",
      "Epoch: 1/20, Average loss: 331.2717\n",
      "Epoch: 2/20, Average loss: 293.0868\n",
      "Epoch: 3/20, Average loss: 260.4425\n",
      "Epoch: 4/20, Average loss: 235.3888\n",
      "Epoch: 5/20, Average loss: 217.5635\n",
      "Epoch: 6/20, Average loss: 207.8059\n",
      "Epoch: 7/20, Average loss: 200.4518\n",
      "Epoch: 8/20, Average loss: 194.3792\n",
      "Epoch: 9/20, Average loss: 189.3980\n",
      "Epoch: 10/20, Average loss: 184.9584\n",
      "Epoch: 11/20, Average loss: 179.7132\n",
      "Epoch: 12/20, Average loss: 174.9633\n",
      "Epoch: 13/20, Average loss: 170.7941\n",
      "Epoch: 14/20, Average loss: 167.3850\n",
      "Epoch: 15/20, Average loss: 164.2246\n",
      "Epoch: 16/20, Average loss: 161.3431\n",
      "Epoch: 17/20, Average loss: 159.5106\n",
      "Epoch: 18/20, Average loss: 156.5435\n",
      "Epoch: 19/20, Average loss: 154.0239\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26814b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.6061\n",
      "Epoch: 1/20, Average loss: 368.8077\n",
      "Epoch: 2/20, Average loss: 357.9416\n",
      "Epoch: 3/20, Average loss: 353.8225\n",
      "Epoch: 4/20, Average loss: 351.9900\n",
      "Epoch: 5/20, Average loss: 351.0825\n",
      "Epoch: 6/20, Average loss: 350.5930\n",
      "Epoch: 7/20, Average loss: 350.3083\n",
      "Epoch: 8/20, Average loss: 350.1508\n",
      "Epoch: 9/20, Average loss: 350.0325\n",
      "Epoch: 10/20, Average loss: 349.9596\n",
      "Epoch: 11/20, Average loss: 349.9249\n",
      "Epoch: 12/20, Average loss: 349.8968\n",
      "Epoch: 13/20, Average loss: 349.8726\n",
      "Epoch: 14/20, Average loss: 349.8558\n",
      "Epoch: 15/20, Average loss: 349.8398\n",
      "Epoch: 16/20, Average loss: 349.8369\n",
      "Epoch: 17/20, Average loss: 349.8276\n",
      "Epoch: 18/20, Average loss: 349.8294\n",
      "Epoch: 19/20, Average loss: 349.8175\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 5 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr05_gKLlog2_5hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5be372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.4123\n",
      "Epoch: 1/20, Average loss: 369.0100\n",
      "Epoch: 2/20, Average loss: 357.9196\n",
      "Epoch: 3/20, Average loss: 353.9314\n",
      "Epoch: 4/20, Average loss: 351.9628\n",
      "Epoch: 5/20, Average loss: 351.0653\n",
      "Epoch: 6/20, Average loss: 350.5855\n",
      "Epoch: 7/20, Average loss: 350.3135\n",
      "Epoch: 8/20, Average loss: 350.1441\n",
      "Epoch: 9/20, Average loss: 350.0360\n",
      "Epoch: 10/20, Average loss: 349.9780\n",
      "Epoch: 11/20, Average loss: 349.9259\n",
      "Epoch: 12/20, Average loss: 349.9004\n",
      "Epoch: 13/20, Average loss: 349.8688\n",
      "Epoch: 14/20, Average loss: 349.8592\n",
      "Epoch: 15/20, Average loss: 349.8426\n",
      "Epoch: 16/20, Average loss: 349.8388\n",
      "Epoch: 17/20, Average loss: 349.8324\n",
      "Epoch: 18/20, Average loss: 349.8224\n",
      "Epoch: 19/20, Average loss: 349.8243\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 6 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr06_gKLlog2_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=20, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM_32_1024/ld32_glog2_ep20_lmb01_LN_dr06_gKLlog2_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d56da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
