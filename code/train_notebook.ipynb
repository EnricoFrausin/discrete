{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d041b96",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import VAE_priorCategorical, VAE_priorHFM\n",
    "import metadata as md\n",
    "from train import train\n",
    "from datasets import Dataset_HFM, Dataset_pureHFM, MNISTDigit2Dataset\n",
    "from utilities import sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58f0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(md.seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(md.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1647",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe9c74",
   "metadata": {},
   "source": [
    "## train over ExtendedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db29123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eef56",
   "metadata": {},
   "source": [
    "## train over 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3bba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=32, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=32, shuffle=True)\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09921586",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f7171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0bae",
   "metadata": {},
   "source": [
    "## train over FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2094def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682e2",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2822c7",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db343",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9adabe",
   "metadata": {},
   "source": [
    "# Prior Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98cd69",
   "metadata": {},
   "source": [
    "## train over Extended_MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c091a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6ed9f",
   "metadata": {},
   "source": [
    "\n",
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 354.4975\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53143931",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 349.8097\n",
      "Epoch: 1/15, Average loss: 252.5678\n",
      "Epoch: 2/15, Average loss: 220.1362\n",
      "Epoch: 3/15, Average loss: 203.2145\n",
      "Epoch: 4/15, Average loss: 193.3282\n",
      "Epoch: 5/15, Average loss: 186.8372\n",
      "Epoch: 6/15, Average loss: 182.4854\n",
      "Epoch: 7/15, Average loss: 179.3084\n",
      "Epoch: 8/15, Average loss: 177.0011\n",
      "Epoch: 9/15, Average loss: 175.3268\n",
      "Epoch: 10/15, Average loss: 173.9250\n",
      "Epoch: 11/15, Average loss: 172.8551\n",
      "Epoch: 12/15, Average loss: 171.9738\n",
      "Epoch: 13/15, Average loss: 171.2925\n",
      "Epoch: 14/15, Average loss: 170.7406\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 350.6189\n",
      "Epoch: 1/15, Average loss: 250.9468\n",
      "Epoch: 2/15, Average loss: 217.6095\n",
      "Epoch: 3/15, Average loss: 200.6092\n",
      "Epoch: 4/15, Average loss: 190.5887\n",
      "Epoch: 5/15, Average loss: 183.7372\n",
      "Epoch: 6/15, Average loss: 178.4993\n",
      "Epoch: 7/15, Average loss: 174.6541\n",
      "Epoch: 8/15, Average loss: 171.8537\n",
      "Epoch: 9/15, Average loss: 169.7876\n",
      "Epoch: 10/15, Average loss: 167.9942\n",
      "Epoch: 11/15, Average loss: 166.7761\n",
      "Epoch: 12/15, Average loss: 165.5641\n",
      "Epoch: 13/15, Average loss: 164.6842\n",
      "Epoch: 14/15, Average loss: 163.9550\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 354.0489\n",
      "Epoch: 1/15, Average loss: 252.5556\n",
      "Epoch: 2/15, Average loss: 217.8604\n",
      "Epoch: 3/15, Average loss: 200.3722\n",
      "Epoch: 4/15, Average loss: 190.7886\n",
      "Epoch: 5/15, Average loss: 184.6503\n",
      "Epoch: 6/15, Average loss: 180.4723\n",
      "Epoch: 7/15, Average loss: 177.2928\n",
      "Epoch: 8/15, Average loss: 174.8085\n",
      "Epoch: 9/15, Average loss: 172.9309\n",
      "Epoch: 10/15, Average loss: 171.3527\n",
      "Epoch: 11/15, Average loss: 169.9989\n",
      "Epoch: 12/15, Average loss: 168.9262\n",
      "Epoch: 13/15, Average loss: 167.9097\n",
      "Epoch: 14/15, Average loss: 167.0546\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.5319\n",
      "Epoch: 1/15, Average loss: 257.6598\n",
      "Epoch: 2/15, Average loss: 222.6616\n",
      "Epoch: 3/15, Average loss: 205.1415\n",
      "Epoch: 4/15, Average loss: 195.8920\n",
      "Epoch: 5/15, Average loss: 190.0039\n",
      "Epoch: 6/15, Average loss: 185.8355\n",
      "Epoch: 7/15, Average loss: 182.7668\n",
      "Epoch: 8/15, Average loss: 180.3982\n",
      "Epoch: 9/15, Average loss: 178.4741\n",
      "Epoch: 10/15, Average loss: 176.9525\n",
      "Epoch: 11/15, Average loss: 175.7419\n",
      "Epoch: 12/15, Average loss: 174.6741\n",
      "Epoch: 13/15, Average loss: 173.7510\n",
      "Epoch: 14/15, Average loss: 172.8685\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.8176\n",
      "Epoch: 1/15, Average loss: 262.1270\n",
      "Epoch: 2/15, Average loss: 228.4061\n",
      "Epoch: 3/15, Average loss: 210.9515\n",
      "Epoch: 4/15, Average loss: 201.2823\n",
      "Epoch: 5/15, Average loss: 195.2321\n",
      "Epoch: 6/15, Average loss: 191.0820\n",
      "Epoch: 7/15, Average loss: 187.9966\n",
      "Epoch: 8/15, Average loss: 185.8052\n",
      "Epoch: 9/15, Average loss: 183.8282\n",
      "Epoch: 10/15, Average loss: 182.1945\n",
      "Epoch: 11/15, Average loss: 180.5830\n",
      "Epoch: 12/15, Average loss: 179.3044\n",
      "Epoch: 13/15, Average loss: 178.0446\n",
      "Epoch: 14/15, Average loss: 176.7816\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 363.8745\n",
      "Epoch: 1/15, Average loss: 265.8502\n",
      "Epoch: 2/15, Average loss: 234.5291\n",
      "Epoch: 3/15, Average loss: 215.7438\n",
      "Epoch: 4/15, Average loss: 205.0702\n",
      "Epoch: 5/15, Average loss: 198.5530\n",
      "Epoch: 6/15, Average loss: 194.0660\n",
      "Epoch: 7/15, Average loss: 190.3498\n",
      "Epoch: 8/15, Average loss: 187.6208\n",
      "Epoch: 9/15, Average loss: 185.2275\n",
      "Epoch: 10/15, Average loss: 183.3724\n",
      "Epoch: 11/15, Average loss: 181.8250\n",
      "Epoch: 12/15, Average loss: 180.4728\n",
      "Epoch: 13/15, Average loss: 179.2218\n",
      "Epoch: 14/15, Average loss: 178.1701\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 378.2936\n",
      "Epoch: 1/15, Average loss: 293.8599\n",
      "Epoch: 2/15, Average loss: 283.2834\n",
      "Epoch: 3/15, Average loss: 280.1153\n",
      "Epoch: 4/15, Average loss: 278.8265\n",
      "Epoch: 5/15, Average loss: 278.2186\n",
      "Epoch: 6/15, Average loss: 277.9094\n",
      "Epoch: 7/15, Average loss: 277.7441\n",
      "Epoch: 8/15, Average loss: 277.6542\n",
      "Epoch: 9/15, Average loss: 277.6033\n",
      "Epoch: 10/15, Average loss: 277.5740\n",
      "Epoch: 11/15, Average loss: 277.5544\n",
      "Epoch: 12/15, Average loss: 277.5417\n",
      "Epoch: 13/15, Average loss: 277.5356\n",
      "Epoch: 14/15, Average loss: 277.5299\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad702",
   "metadata": {},
   "source": [
    "## train over 2Digit_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16bd9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c19a7f",
   "metadata": {},
   "source": [
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b37487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 293.0254\n",
      "Epoch: 1/15, Average loss: 199.2283\n",
      "Epoch: 2/15, Average loss: 179.8768\n",
      "Epoch: 3/15, Average loss: 173.0299\n",
      "Epoch: 4/15, Average loss: 169.1212\n",
      "Epoch: 5/15, Average loss: 166.2844\n",
      "Epoch: 6/15, Average loss: 164.3290\n",
      "Epoch: 7/15, Average loss: 162.8076\n",
      "Epoch: 8/15, Average loss: 161.5842\n",
      "Epoch: 9/15, Average loss: 160.7148\n",
      "Epoch: 10/15, Average loss: 159.9534\n",
      "Epoch: 11/15, Average loss: 159.2769\n",
      "Epoch: 12/15, Average loss: 158.8208\n",
      "Epoch: 13/15, Average loss: 158.2286\n",
      "Epoch: 14/15, Average loss: 157.8266\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 293.5113\n",
      "Epoch: 1/15, Average loss: 198.2328\n",
      "Epoch: 2/15, Average loss: 178.4778\n",
      "Epoch: 3/15, Average loss: 170.6506\n",
      "Epoch: 4/15, Average loss: 165.6475\n",
      "Epoch: 5/15, Average loss: 162.1394\n",
      "Epoch: 6/15, Average loss: 159.5660\n",
      "Epoch: 7/15, Average loss: 157.7338\n",
      "Epoch: 8/15, Average loss: 156.2479\n",
      "Epoch: 9/15, Average loss: 155.0138\n",
      "Epoch: 10/15, Average loss: 153.9606\n",
      "Epoch: 11/15, Average loss: 153.0693\n",
      "Epoch: 12/15, Average loss: 152.4010\n",
      "Epoch: 13/15, Average loss: 151.7946\n",
      "Epoch: 14/15, Average loss: 151.3296\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 297.2092\n",
      "Epoch: 1/15, Average loss: 201.5075\n",
      "Epoch: 2/15, Average loss: 180.5769\n",
      "Epoch: 3/15, Average loss: 172.8325\n",
      "Epoch: 4/15, Average loss: 168.2767\n",
      "Epoch: 5/15, Average loss: 164.9350\n",
      "Epoch: 6/15, Average loss: 162.4663\n",
      "Epoch: 7/15, Average loss: 160.6125\n",
      "Epoch: 8/15, Average loss: 159.1465\n",
      "Epoch: 9/15, Average loss: 158.0652\n",
      "Epoch: 10/15, Average loss: 156.9120\n",
      "Epoch: 11/15, Average loss: 156.1592\n",
      "Epoch: 12/15, Average loss: 155.3253\n",
      "Epoch: 13/15, Average loss: 154.7250\n",
      "Epoch: 14/15, Average loss: 154.2013\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 299.0114\n",
      "Epoch: 1/15, Average loss: 204.6393\n",
      "Epoch: 2/15, Average loss: 183.8510\n",
      "Epoch: 3/15, Average loss: 176.4470\n",
      "Epoch: 4/15, Average loss: 172.9547\n",
      "Epoch: 5/15, Average loss: 170.6624\n",
      "Epoch: 6/15, Average loss: 169.1338\n",
      "Epoch: 7/15, Average loss: 167.6985\n",
      "Epoch: 8/15, Average loss: 166.4294\n",
      "Epoch: 9/15, Average loss: 165.4667\n",
      "Epoch: 10/15, Average loss: 164.3483\n",
      "Epoch: 11/15, Average loss: 163.4033\n",
      "Epoch: 12/15, Average loss: 162.6376\n",
      "Epoch: 13/15, Average loss: 161.7045\n",
      "Epoch: 14/15, Average loss: 161.0767\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 307.8540\n",
      "Epoch: 1/15, Average loss: 220.9278\n",
      "Epoch: 2/15, Average loss: 194.7669\n",
      "Epoch: 3/15, Average loss: 184.7988\n",
      "Epoch: 4/15, Average loss: 180.1304\n",
      "Epoch: 5/15, Average loss: 177.7514\n",
      "Epoch: 6/15, Average loss: 175.4698\n",
      "Epoch: 7/15, Average loss: 173.7945\n",
      "Epoch: 8/15, Average loss: 172.4466\n",
      "Epoch: 9/15, Average loss: 171.3992\n",
      "Epoch: 10/15, Average loss: 170.2958\n",
      "Epoch: 11/15, Average loss: 169.1930\n",
      "Epoch: 12/15, Average loss: 168.4157\n",
      "Epoch: 13/15, Average loss: 167.6548\n",
      "Epoch: 14/15, Average loss: 166.8264\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f5400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 317.1253\n",
      "Epoch: 1/15, Average loss: 254.0380\n",
      "Epoch: 2/15, Average loss: 250.4241\n",
      "Epoch: 3/15, Average loss: 249.7298\n",
      "Epoch: 4/15, Average loss: 249.5778\n",
      "Epoch: 5/15, Average loss: 249.5387\n",
      "Epoch: 6/15, Average loss: 249.5223\n",
      "Epoch: 7/15, Average loss: 249.5062\n",
      "Epoch: 8/15, Average loss: 249.5075\n",
      "Epoch: 9/15, Average loss: 249.5052\n",
      "Epoch: 10/15, Average loss: 249.4995\n",
      "Epoch: 11/15, Average loss: 249.4920\n",
      "Epoch: 12/15, Average loss: 249.4916\n",
      "Epoch: 13/15, Average loss: 249.4944\n",
      "Epoch: 14/15, Average loss: 249.4895\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a0c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 314.7471\n",
      "Epoch: 1/15, Average loss: 246.3329\n",
      "Epoch: 2/15, Average loss: 242.4432\n",
      "Epoch: 3/15, Average loss: 246.4289\n",
      "Epoch: 4/15, Average loss: 249.6111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m8\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:19\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     17\u001b[39m global_batch_idx += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m data = data.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss, KL, rec_loss = model(data, temp, _lambda, hard=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     21\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:962\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    960\u001b[39m     per_device_and_dtype_grads = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/profiler.py:787\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;66;03m# TODO: Too slow with __torch_function__ handling enabled\u001b[39;00m\n\u001b[32m    785\u001b[39m \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/76410\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDisableTorchFunctionSubclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    788\u001b[39m         torch.ops.profiler._record_function_exit._RecordFunction(record)\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d42e3",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84289a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 289.6640\n",
      "Epoch: 1/15, Average loss: 193.0851\n",
      "Epoch: 2/15, Average loss: 173.0547\n",
      "Epoch: 3/15, Average loss: 165.9744\n",
      "Epoch: 4/15, Average loss: 162.1465\n",
      "Epoch: 5/15, Average loss: 159.2105\n",
      "Epoch: 6/15, Average loss: 156.8267\n",
      "Epoch: 7/15, Average loss: 154.7964\n",
      "Epoch: 8/15, Average loss: 153.1571\n",
      "Epoch: 9/15, Average loss: 151.8358\n",
      "Epoch: 10/15, Average loss: 150.8027\n",
      "Epoch: 11/15, Average loss: 149.9243\n",
      "Epoch: 12/15, Average loss: 149.1409\n",
      "Epoch: 13/15, Average loss: 148.5724\n",
      "Epoch: 14/15, Average loss: 147.9692\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 291.2133\n",
      "Epoch: 1/15, Average loss: 192.8380\n",
      "Epoch: 2/15, Average loss: 172.2742\n",
      "Epoch: 3/15, Average loss: 165.7251\n",
      "Epoch: 4/15, Average loss: 162.3980\n",
      "Epoch: 5/15, Average loss: 159.8999\n",
      "Epoch: 6/15, Average loss: 157.8492\n",
      "Epoch: 7/15, Average loss: 155.8201\n",
      "Epoch: 8/15, Average loss: 153.9527\n",
      "Epoch: 9/15, Average loss: 152.0119\n",
      "Epoch: 10/15, Average loss: 150.3478\n",
      "Epoch: 11/15, Average loss: 148.7277\n",
      "Epoch: 12/15, Average loss: 147.1640\n",
      "Epoch: 13/15, Average loss: 145.8842\n",
      "Epoch: 14/15, Average loss: 144.8714\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 294.2355\n",
      "Epoch: 1/15, Average loss: 196.2685\n",
      "Epoch: 2/15, Average loss: 173.8741\n",
      "Epoch: 3/15, Average loss: 166.0807\n",
      "Epoch: 4/15, Average loss: 162.0633\n",
      "Epoch: 5/15, Average loss: 158.9627\n",
      "Epoch: 6/15, Average loss: 156.5541\n",
      "Epoch: 7/15, Average loss: 154.5101\n",
      "Epoch: 8/15, Average loss: 152.6384\n",
      "Epoch: 9/15, Average loss: 150.9864\n",
      "Epoch: 10/15, Average loss: 149.4495\n",
      "Epoch: 11/15, Average loss: 148.0731\n",
      "Epoch: 12/15, Average loss: 146.9253\n",
      "Epoch: 13/15, Average loss: 146.0283\n",
      "Epoch: 14/15, Average loss: 144.9650\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 298.5488\n",
      "Epoch: 1/15, Average loss: 202.9798\n",
      "Epoch: 2/15, Average loss: 181.3341\n",
      "Epoch: 3/15, Average loss: 173.6317\n",
      "Epoch: 4/15, Average loss: 169.8361\n",
      "Epoch: 5/15, Average loss: 167.2894\n",
      "Epoch: 6/15, Average loss: 165.4415\n",
      "Epoch: 7/15, Average loss: 163.9719\n",
      "Epoch: 8/15, Average loss: 162.8791\n",
      "Epoch: 9/15, Average loss: 161.7283\n",
      "Epoch: 10/15, Average loss: 160.6633\n",
      "Epoch: 11/15, Average loss: 159.9641\n",
      "Epoch: 12/15, Average loss: 159.1463\n",
      "Epoch: 13/15, Average loss: 158.3637\n",
      "Epoch: 14/15, Average loss: 157.6858\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.3491\n",
      "Epoch: 1/15, Average loss: 215.5182\n",
      "Epoch: 2/15, Average loss: 188.7768\n",
      "Epoch: 3/15, Average loss: 179.3800\n",
      "Epoch: 4/15, Average loss: 174.9331\n",
      "Epoch: 5/15, Average loss: 172.1443\n",
      "Epoch: 6/15, Average loss: 170.2548\n",
      "Epoch: 7/15, Average loss: 168.3283\n",
      "Epoch: 8/15, Average loss: 167.0273\n",
      "Epoch: 9/15, Average loss: 165.5205\n",
      "Epoch: 10/15, Average loss: 164.6139\n",
      "Epoch: 11/15, Average loss: 163.4669\n",
      "Epoch: 12/15, Average loss: 162.5855\n",
      "Epoch: 13/15, Average loss: 161.6188\n",
      "Epoch: 14/15, Average loss: 160.9562\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 307.4619\n",
      "Epoch: 1/15, Average loss: 224.2687\n",
      "Epoch: 2/15, Average loss: 198.2998\n",
      "Epoch: 3/15, Average loss: 185.7454\n",
      "Epoch: 4/15, Average loss: 180.7001\n",
      "Epoch: 5/15, Average loss: 177.3740\n",
      "Epoch: 6/15, Average loss: 175.3895\n",
      "Epoch: 7/15, Average loss: 173.9622\n",
      "Epoch: 8/15, Average loss: 172.5908\n",
      "Epoch: 9/15, Average loss: 171.5822\n",
      "Epoch: 10/15, Average loss: 170.8760\n",
      "Epoch: 11/15, Average loss: 169.7851\n",
      "Epoch: 12/15, Average loss: 169.0375\n",
      "Epoch: 13/15, Average loss: 168.5531\n",
      "Epoch: 14/15, Average loss: 167.5461\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c66f978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 318.0827\n",
      "Epoch: 1/15, Average loss: 254.0073\n",
      "Epoch: 2/15, Average loss: 250.4167\n",
      "Epoch: 3/15, Average loss: 249.7302\n",
      "Epoch: 4/15, Average loss: 249.5816\n",
      "Epoch: 5/15, Average loss: 249.5373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m10\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:20\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     18\u001b[39m data = data.to(device)\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m loss, KL, rec_loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhard\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m loss.backward()\n\u001b[32m     22\u001b[39m train_loss += loss.item() * \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/model.py:114\u001b[39m, in \u001b[36mVAE_priorCategorical.forward\u001b[39m\u001b[34m(self, data, temp, _lambda, hard)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    112\u001b[39m     data_flat = data\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m logits_z_flat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_flat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, latent_dim * categorical_dim)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Reshape per gumbel_softmax e calcolo KL\u001b[39;00m\n\u001b[32m    116\u001b[39m logits_z = logits_z_flat.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.latent_dim, \u001b[38;5;28mself\u001b[39m.categorical_dim) \u001b[38;5;66;03m# (batch_size, latent_dim, categorical_dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/model.py:99\u001b[39m, in \u001b[36mVAE_priorCategorical.encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# x ha shape (batch_size, input_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7130fe",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbdaa5",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 386.3299\n",
      "Epoch: 1/15, Average loss: 269.6630\n",
      "Epoch: 2/15, Average loss: 220.6156\n",
      "Epoch: 3/15, Average loss: 192.3674\n",
      "Epoch: 4/15, Average loss: 172.4641\n",
      "Epoch: 5/15, Average loss: 158.6117\n",
      "Epoch: 6/15, Average loss: 148.8546\n",
      "Epoch: 7/15, Average loss: 141.7337\n",
      "Epoch: 8/15, Average loss: 136.4395\n",
      "Epoch: 9/15, Average loss: 132.2159\n",
      "Epoch: 10/15, Average loss: 128.9444\n",
      "Epoch: 11/15, Average loss: 126.1562\n",
      "Epoch: 12/15, Average loss: 123.7926\n",
      "Epoch: 13/15, Average loss: 121.9730\n",
      "Epoch: 14/15, Average loss: 120.2335\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 386.8003\n",
      "Epoch: 1/15, Average loss: 270.2383\n",
      "Epoch: 2/15, Average loss: 221.5904\n",
      "Epoch: 3/15, Average loss: 193.3287\n",
      "Epoch: 4/15, Average loss: 173.1111\n",
      "Epoch: 5/15, Average loss: 158.6518\n",
      "Epoch: 6/15, Average loss: 148.4285\n",
      "Epoch: 7/15, Average loss: 140.9530\n",
      "Epoch: 8/15, Average loss: 135.4636\n",
      "Epoch: 9/15, Average loss: 131.2472\n",
      "Epoch: 10/15, Average loss: 127.9704\n",
      "Epoch: 11/15, Average loss: 125.2929\n",
      "Epoch: 12/15, Average loss: 123.1115\n",
      "Epoch: 13/15, Average loss: 121.3816\n",
      "Epoch: 14/15, Average loss: 119.8358\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 388.8020\n",
      "Epoch: 1/15, Average loss: 271.3259\n",
      "Epoch: 2/15, Average loss: 222.7423\n",
      "Epoch: 3/15, Average loss: 194.5868\n",
      "Epoch: 4/15, Average loss: 174.3165\n",
      "Epoch: 5/15, Average loss: 160.1383\n",
      "Epoch: 6/15, Average loss: 150.3916\n",
      "Epoch: 7/15, Average loss: 143.2736\n",
      "Epoch: 8/15, Average loss: 137.8577\n",
      "Epoch: 9/15, Average loss: 133.7821\n",
      "Epoch: 10/15, Average loss: 130.6661\n",
      "Epoch: 11/15, Average loss: 128.2295\n",
      "Epoch: 12/15, Average loss: 126.0127\n",
      "Epoch: 13/15, Average loss: 124.4271\n",
      "Epoch: 14/15, Average loss: 123.0456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST//ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 402.5911\n",
      "Epoch: 1/15, Average loss: 275.8961\n",
      "Epoch: 2/15, Average loss: 226.4554\n",
      "Epoch: 3/15, Average loss: 198.4908\n",
      "Epoch: 4/15, Average loss: 177.8793\n",
      "Epoch: 5/15, Average loss: 163.3509\n",
      "Epoch: 6/15, Average loss: 153.2155\n",
      "Epoch: 7/15, Average loss: 146.2363\n",
      "Epoch: 8/15, Average loss: 141.0705\n",
      "Epoch: 9/15, Average loss: 137.0449\n",
      "Epoch: 10/15, Average loss: 134.0465\n",
      "Epoch: 11/15, Average loss: 131.8412\n",
      "Epoch: 12/15, Average loss: 130.0399\n",
      "Epoch: 13/15, Average loss: 128.2418\n",
      "Epoch: 14/15, Average loss: 127.0624\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050da7b7",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f34279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 382.6704\n",
      "Epoch: 1/15, Average loss: 269.1358\n",
      "Epoch: 2/15, Average loss: 219.8319\n",
      "Epoch: 3/15, Average loss: 190.9535\n",
      "Epoch: 4/15, Average loss: 170.5004\n",
      "Epoch: 5/15, Average loss: 156.3013\n",
      "Epoch: 6/15, Average loss: 146.3802\n",
      "Epoch: 7/15, Average loss: 139.2002\n",
      "Epoch: 8/15, Average loss: 133.7200\n",
      "Epoch: 9/15, Average loss: 129.5677\n",
      "Epoch: 10/15, Average loss: 126.1323\n",
      "Epoch: 11/15, Average loss: 123.3453\n",
      "Epoch: 12/15, Average loss: 121.0505\n",
      "Epoch: 13/15, Average loss: 119.1494\n",
      "Epoch: 14/15, Average loss: 117.6010\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 384.0852\n",
      "Epoch: 1/15, Average loss: 267.8581\n",
      "Epoch: 2/15, Average loss: 218.3796\n",
      "Epoch: 3/15, Average loss: 189.0615\n",
      "Epoch: 4/15, Average loss: 168.0823\n",
      "Epoch: 5/15, Average loss: 153.4838\n",
      "Epoch: 6/15, Average loss: 143.4466\n",
      "Epoch: 7/15, Average loss: 136.2004\n",
      "Epoch: 8/15, Average loss: 130.9332\n",
      "Epoch: 9/15, Average loss: 126.8407\n",
      "Epoch: 10/15, Average loss: 123.6340\n",
      "Epoch: 11/15, Average loss: 121.0773\n",
      "Epoch: 12/15, Average loss: 118.7623\n",
      "Epoch: 13/15, Average loss: 116.8151\n",
      "Epoch: 14/15, Average loss: 115.2380\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 385.8713\n",
      "Epoch: 1/15, Average loss: 269.0690\n",
      "Epoch: 2/15, Average loss: 219.5711\n",
      "Epoch: 3/15, Average loss: 190.0897\n",
      "Epoch: 4/15, Average loss: 169.0395\n",
      "Epoch: 5/15, Average loss: 154.4246\n",
      "Epoch: 6/15, Average loss: 144.1625\n",
      "Epoch: 7/15, Average loss: 136.9041\n",
      "Epoch: 8/15, Average loss: 131.5134\n",
      "Epoch: 9/15, Average loss: 127.3652\n",
      "Epoch: 10/15, Average loss: 124.1533\n",
      "Epoch: 11/15, Average loss: 121.4891\n",
      "Epoch: 12/15, Average loss: 119.3849\n",
      "Epoch: 13/15, Average loss: 117.5831\n",
      "Epoch: 14/15, Average loss: 116.1856\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.6227\n",
      "Epoch: 1/15, Average loss: 270.5158\n",
      "Epoch: 2/15, Average loss: 221.6360\n",
      "Epoch: 3/15, Average loss: 192.9631\n",
      "Epoch: 4/15, Average loss: 171.9100\n",
      "Epoch: 5/15, Average loss: 157.1226\n",
      "Epoch: 6/15, Average loss: 146.9147\n",
      "Epoch: 7/15, Average loss: 139.7606\n",
      "Epoch: 8/15, Average loss: 134.5452\n",
      "Epoch: 9/15, Average loss: 130.5691\n",
      "Epoch: 10/15, Average loss: 127.6468\n",
      "Epoch: 11/15, Average loss: 125.3244\n",
      "Epoch: 12/15, Average loss: 123.3504\n",
      "Epoch: 13/15, Average loss: 121.7893\n",
      "Epoch: 14/15, Average loss: 120.3889\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.9649\n",
      "Epoch: 1/15, Average loss: 270.8257\n",
      "Epoch: 2/15, Average loss: 221.7338\n",
      "Epoch: 3/15, Average loss: 193.3308\n",
      "Epoch: 4/15, Average loss: 172.9608\n",
      "Epoch: 5/15, Average loss: 158.3673\n",
      "Epoch: 6/15, Average loss: 148.0558\n",
      "Epoch: 7/15, Average loss: 141.0201\n",
      "Epoch: 8/15, Average loss: 135.9111\n",
      "Epoch: 9/15, Average loss: 131.9691\n",
      "Epoch: 10/15, Average loss: 129.0454\n",
      "Epoch: 11/15, Average loss: 126.8269\n",
      "Epoch: 12/15, Average loss: 124.8382\n",
      "Epoch: 13/15, Average loss: 123.3273\n",
      "Epoch: 14/15, Average loss: 121.9604\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 402.9144\n",
      "Epoch: 1/15, Average loss: 278.1830\n",
      "Epoch: 2/15, Average loss: 229.3540\n",
      "Epoch: 3/15, Average loss: 201.5456\n",
      "Epoch: 4/15, Average loss: 181.0566\n",
      "Epoch: 5/15, Average loss: 165.8562\n",
      "Epoch: 6/15, Average loss: 155.0658\n",
      "Epoch: 7/15, Average loss: 147.2022\n",
      "Epoch: 8/15, Average loss: 141.7347\n",
      "Epoch: 9/15, Average loss: 137.6772\n",
      "Epoch: 10/15, Average loss: 134.4249\n",
      "Epoch: 11/15, Average loss: 132.2274\n",
      "Epoch: 12/15, Average loss: 130.1863\n",
      "Epoch: 13/15, Average loss: 128.6803\n",
      "Epoch: 14/15, Average loss: 127.5914\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 399.4419\n",
      "Epoch: 1/15, Average loss: 279.3267\n",
      "Epoch: 2/15, Average loss: 231.7823\n",
      "Epoch: 3/15, Average loss: 205.5778\n",
      "Epoch: 4/15, Average loss: 184.9356\n",
      "Epoch: 5/15, Average loss: 169.8275\n",
      "Epoch: 6/15, Average loss: 158.6164\n",
      "Epoch: 7/15, Average loss: 150.7300\n",
      "Epoch: 8/15, Average loss: 144.6163\n",
      "Epoch: 9/15, Average loss: 140.0995\n",
      "Epoch: 10/15, Average loss: 137.0610\n",
      "Epoch: 11/15, Average loss: 134.2746\n",
      "Epoch: 12/15, Average loss: 132.3369\n",
      "Epoch: 13/15, Average loss: 130.9848\n",
      "Epoch: 14/15, Average loss: 129.7188\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bc2f",
   "metadata": {},
   "source": [
    "## train over expandedHFM 16-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM\n",
    "val_loader = val_loader_expandedHFM\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b4f7f",
   "metadata": {},
   "source": [
    "### latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 369.4557\n",
      "Epoch: 1/20, Average loss: 253.0922\n",
      "Epoch: 2/20, Average loss: 202.8551\n",
      "Epoch: 3/20, Average loss: 172.7191\n",
      "Epoch: 4/20, Average loss: 152.2356\n",
      "Epoch: 5/20, Average loss: 137.6990\n",
      "Epoch: 6/20, Average loss: 126.6692\n",
      "Epoch: 7/20, Average loss: 118.0560\n",
      "Epoch: 8/20, Average loss: 111.1311\n",
      "Epoch: 9/20, Average loss: 105.9597\n",
      "Epoch: 10/20, Average loss: 101.8351\n",
      "Epoch: 11/20, Average loss: 98.4729\n",
      "Epoch: 12/20, Average loss: 95.4016\n",
      "Epoch: 13/20, Average loss: 92.9106\n",
      "Epoch: 14/20, Average loss: 90.9402\n",
      "Epoch: 15/20, Average loss: 89.1270\n",
      "Epoch: 16/20, Average loss: 87.6784\n",
      "Epoch: 17/20, Average loss: 86.5530\n",
      "Epoch: 18/20, Average loss: 85.3882\n",
      "Epoch: 19/20, Average loss: 84.4254\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e5b3",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 2, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b165db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 393.1789\n",
      "Epoch: 1/20, Average loss: 300.7643\n",
      "Epoch: 2/20, Average loss: 241.3813\n",
      "Epoch: 3/20, Average loss: 207.7646\n",
      "Epoch: 4/20, Average loss: 179.9620\n",
      "Epoch: 5/20, Average loss: 160.1453\n",
      "Epoch: 6/20, Average loss: 144.9132\n",
      "Epoch: 7/20, Average loss: 133.3975\n",
      "Epoch: 8/20, Average loss: 125.1795\n",
      "Epoch: 9/20, Average loss: 118.3757\n",
      "Epoch: 10/20, Average loss: 112.1573\n",
      "Epoch: 11/20, Average loss: 107.3208\n",
      "Epoch: 12/20, Average loss: 103.3839\n",
      "Epoch: 13/20, Average loss: 99.9120\n",
      "Epoch: 14/20, Average loss: 96.7396\n",
      "Epoch: 15/20, Average loss: 94.4459\n",
      "Epoch: 16/20, Average loss: 92.6706\n",
      "Epoch: 17/20, Average loss: 90.7141\n",
      "Epoch: 18/20, Average loss: 88.7004\n",
      "Epoch: 19/20, Average loss: 87.0102\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1eed",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 3, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 399.2147\n",
      "Epoch: 1/20, Average loss: 324.6712\n",
      "Epoch: 2/20, Average loss: 275.4050\n",
      "Epoch: 3/20, Average loss: 239.8818\n",
      "Epoch: 4/20, Average loss: 216.3649\n",
      "Epoch: 5/20, Average loss: 198.0273\n",
      "Epoch: 6/20, Average loss: 182.2649\n",
      "Epoch: 7/20, Average loss: 170.8574\n",
      "Epoch: 8/20, Average loss: 162.9704\n",
      "Epoch: 9/20, Average loss: 154.4810\n",
      "Epoch: 10/20, Average loss: 146.7258\n",
      "Epoch: 11/20, Average loss: 139.4719\n",
      "Epoch: 12/20, Average loss: 134.2167\n",
      "Epoch: 13/20, Average loss: 129.6427\n",
      "Epoch: 14/20, Average loss: 125.4191\n",
      "Epoch: 15/20, Average loss: 121.2396\n",
      "Epoch: 16/20, Average loss: 118.2277\n",
      "Epoch: 17/20, Average loss: 115.6689\n",
      "Epoch: 18/20, Average loss: 112.8884\n",
      "Epoch: 19/20, Average loss: 111.4160\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde52c1d",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 4, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47314339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 405.6318\n",
      "Epoch: 1/20, Average loss: 331.2717\n",
      "Epoch: 2/20, Average loss: 293.0868\n",
      "Epoch: 3/20, Average loss: 260.4425\n",
      "Epoch: 4/20, Average loss: 235.3888\n",
      "Epoch: 5/20, Average loss: 217.5635\n",
      "Epoch: 6/20, Average loss: 207.8059\n",
      "Epoch: 7/20, Average loss: 200.4518\n",
      "Epoch: 8/20, Average loss: 194.3792\n",
      "Epoch: 9/20, Average loss: 189.3980\n",
      "Epoch: 10/20, Average loss: 184.9584\n",
      "Epoch: 11/20, Average loss: 179.7132\n",
      "Epoch: 12/20, Average loss: 174.9633\n",
      "Epoch: 13/20, Average loss: 170.7941\n",
      "Epoch: 14/20, Average loss: 167.3850\n",
      "Epoch: 15/20, Average loss: 164.2246\n",
      "Epoch: 16/20, Average loss: 161.3431\n",
      "Epoch: 17/20, Average loss: 159.5106\n",
      "Epoch: 18/20, Average loss: 156.5435\n",
      "Epoch: 19/20, Average loss: 154.0239\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26814b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.6061\n",
      "Epoch: 1/20, Average loss: 368.8077\n",
      "Epoch: 2/20, Average loss: 357.9416\n",
      "Epoch: 3/20, Average loss: 353.8225\n",
      "Epoch: 4/20, Average loss: 351.9900\n",
      "Epoch: 5/20, Average loss: 351.0825\n",
      "Epoch: 6/20, Average loss: 350.5930\n",
      "Epoch: 7/20, Average loss: 350.3083\n",
      "Epoch: 8/20, Average loss: 350.1508\n",
      "Epoch: 9/20, Average loss: 350.0325\n",
      "Epoch: 10/20, Average loss: 349.9596\n",
      "Epoch: 11/20, Average loss: 349.9249\n",
      "Epoch: 12/20, Average loss: 349.8968\n",
      "Epoch: 13/20, Average loss: 349.8726\n",
      "Epoch: 14/20, Average loss: 349.8558\n",
      "Epoch: 15/20, Average loss: 349.8398\n",
      "Epoch: 16/20, Average loss: 349.8369\n",
      "Epoch: 17/20, Average loss: 349.8276\n",
      "Epoch: 18/20, Average loss: 349.8294\n",
      "Epoch: 19/20, Average loss: 349.8175\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 5 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.4123\n",
      "Epoch: 1/20, Average loss: 369.0100\n",
      "Epoch: 2/20, Average loss: 357.9196\n",
      "Epoch: 3/20, Average loss: 353.9314\n",
      "Epoch: 4/20, Average loss: 351.9628\n",
      "Epoch: 5/20, Average loss: 351.0653\n",
      "Epoch: 6/20, Average loss: 350.5855\n",
      "Epoch: 7/20, Average loss: 350.3135\n",
      "Epoch: 8/20, Average loss: 350.1441\n",
      "Epoch: 9/20, Average loss: 350.0360\n",
      "Epoch: 10/20, Average loss: 349.9780\n",
      "Epoch: 11/20, Average loss: 349.9259\n",
      "Epoch: 12/20, Average loss: 349.9004\n",
      "Epoch: 13/20, Average loss: 349.8688\n",
      "Epoch: 14/20, Average loss: 349.8592\n",
      "Epoch: 15/20, Average loss: 349.8426\n",
      "Epoch: 16/20, Average loss: 349.8388\n",
      "Epoch: 17/20, Average loss: 349.8324\n",
      "Epoch: 18/20, Average loss: 349.8224\n",
      "Epoch: 19/20, Average loss: 349.8243\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 6 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48cd883",
   "metadata": {},
   "source": [
    "## CoarseGrainMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1961281",
   "metadata": {},
   "source": [
    "### 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b427f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db70be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 289.6801\n",
      "Epoch: 1/15, Average loss: 193.2628\n",
      "Epoch: 2/15, Average loss: 173.1857\n",
      "Epoch: 3/15, Average loss: 166.2264\n",
      "Epoch: 4/15, Average loss: 162.2819\n",
      "Epoch: 5/15, Average loss: 159.5451\n",
      "Epoch: 6/15, Average loss: 157.3774\n",
      "Epoch: 7/15, Average loss: 155.7244\n",
      "Epoch: 8/15, Average loss: 154.3177\n",
      "Epoch: 9/15, Average loss: 153.0372\n",
      "Epoch: 10/15, Average loss: 152.0837\n",
      "Epoch: 11/15, Average loss: 151.2058\n",
      "Epoch: 12/15, Average loss: 150.6141\n",
      "Epoch: 13/15, Average loss: 149.9940\n",
      "Epoch: 14/15, Average loss: 149.4595\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 290.8905\n",
      "Epoch: 1/15, Average loss: 192.6489\n",
      "Epoch: 2/15, Average loss: 171.9591\n",
      "Epoch: 3/15, Average loss: 165.0285\n",
      "Epoch: 4/15, Average loss: 161.3574\n",
      "Epoch: 5/15, Average loss: 158.6042\n",
      "Epoch: 6/15, Average loss: 156.4119\n",
      "Epoch: 7/15, Average loss: 154.2437\n",
      "Epoch: 8/15, Average loss: 152.3802\n",
      "Epoch: 9/15, Average loss: 150.4624\n",
      "Epoch: 10/15, Average loss: 148.9291\n",
      "Epoch: 11/15, Average loss: 147.4861\n",
      "Epoch: 12/15, Average loss: 146.2182\n",
      "Epoch: 13/15, Average loss: 145.1909\n",
      "Epoch: 14/15, Average loss: 144.2715\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 293.8857\n",
      "Epoch: 1/15, Average loss: 195.2444\n",
      "Epoch: 2/15, Average loss: 173.3541\n",
      "Epoch: 3/15, Average loss: 165.8520\n",
      "Epoch: 4/15, Average loss: 161.9161\n",
      "Epoch: 5/15, Average loss: 158.9183\n",
      "Epoch: 6/15, Average loss: 156.7009\n",
      "Epoch: 7/15, Average loss: 154.7471\n",
      "Epoch: 8/15, Average loss: 153.1507\n",
      "Epoch: 9/15, Average loss: 151.4836\n",
      "Epoch: 10/15, Average loss: 150.0993\n",
      "Epoch: 11/15, Average loss: 148.6495\n",
      "Epoch: 12/15, Average loss: 147.4614\n",
      "Epoch: 13/15, Average loss: 146.1274\n",
      "Epoch: 14/15, Average loss: 145.2253\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 298.1447\n",
      "Epoch: 1/15, Average loss: 202.0074\n",
      "Epoch: 2/15, Average loss: 180.3035\n",
      "Epoch: 3/15, Average loss: 172.4941\n",
      "Epoch: 4/15, Average loss: 168.4466\n",
      "Epoch: 5/15, Average loss: 165.8237\n",
      "Epoch: 6/15, Average loss: 163.8447\n",
      "Epoch: 7/15, Average loss: 162.5710\n",
      "Epoch: 8/15, Average loss: 161.3496\n",
      "Epoch: 9/15, Average loss: 160.2445\n",
      "Epoch: 10/15, Average loss: 159.3711\n",
      "Epoch: 11/15, Average loss: 158.4322\n",
      "Epoch: 12/15, Average loss: 157.7368\n",
      "Epoch: 13/15, Average loss: 156.9747\n",
      "Epoch: 14/15, Average loss: 156.2230\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.3239\n",
      "Epoch: 1/15, Average loss: 218.3334\n",
      "Epoch: 2/15, Average loss: 191.2042\n",
      "Epoch: 3/15, Average loss: 180.2265\n",
      "Epoch: 4/15, Average loss: 174.9965\n",
      "Epoch: 5/15, Average loss: 171.8572\n",
      "Epoch: 6/15, Average loss: 169.7922\n",
      "Epoch: 7/15, Average loss: 168.4368\n",
      "Epoch: 8/15, Average loss: 167.1125\n",
      "Epoch: 9/15, Average loss: 165.8994\n",
      "Epoch: 10/15, Average loss: 164.8349\n",
      "Epoch: 11/15, Average loss: 163.8959\n",
      "Epoch: 12/15, Average loss: 163.0649\n",
      "Epoch: 13/15, Average loss: 162.1713\n",
      "Epoch: 14/15, Average loss: 161.3726\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 317.2487\n",
      "Epoch: 1/15, Average loss: 254.0201\n",
      "Epoch: 2/15, Average loss: 250.4007\n",
      "Epoch: 3/15, Average loss: 249.7340\n",
      "Epoch: 4/15, Average loss: 249.5865\n",
      "Epoch: 5/15, Average loss: 249.5347\n",
      "Epoch: 6/15, Average loss: 249.5146\n",
      "Epoch: 7/15, Average loss: 249.5115\n",
      "Epoch: 8/15, Average loss: 249.5062\n",
      "Epoch: 9/15, Average loss: 249.5032\n",
      "Epoch: 10/15, Average loss: 249.5002\n",
      "Epoch: 11/15, Average loss: 249.4957\n",
      "Epoch: 12/15, Average loss: 249.4947\n",
      "Epoch: 13/15, Average loss: 249.4885\n",
      "Epoch: 14/15, Average loss: 249.4943\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6bd50",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6a0514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 133.2970\n",
      "Epoch: 1/15, Average loss: 122.0446\n",
      "Epoch: 2/15, Average loss: 118.3386\n",
      "Epoch: 3/15, Average loss: 116.1568\n",
      "Epoch: 4/15, Average loss: 114.5595\n",
      "Epoch: 5/15, Average loss: 113.3461\n",
      "Epoch: 6/15, Average loss: 112.3307\n",
      "Epoch: 7/15, Average loss: 111.6382\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f60be0",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd9771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea6b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 198.2102\n",
      "Epoch: 1/15, Average loss: 182.7440\n",
      "Epoch: 2/15, Average loss: 178.3392\n",
      "Epoch: 3/15, Average loss: 175.9371\n",
      "Epoch: 4/15, Average loss: 174.3433\n",
      "Epoch: 5/15, Average loss: 173.1451\n",
      "Epoch: 6/15, Average loss: 172.2648\n",
      "Epoch: 7/15, Average loss: 171.4805\n",
      "Epoch: 8/15, Average loss: 170.9616\n",
      "Epoch: 9/15, Average loss: 170.4049\n",
      "Epoch: 10/15, Average loss: 169.9646\n",
      "Epoch: 11/15, Average loss: 169.6006\n",
      "Epoch: 12/15, Average loss: 169.3137\n",
      "Epoch: 13/15, Average loss: 168.9561\n",
      "Epoch: 14/15, Average loss: 168.6767\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 191.4708\n",
      "Epoch: 1/15, Average loss: 175.8902\n",
      "Epoch: 2/15, Average loss: 171.7526\n",
      "Epoch: 3/15, Average loss: 169.3644\n",
      "Epoch: 4/15, Average loss: 167.7059\n",
      "Epoch: 5/15, Average loss: 166.3757\n",
      "Epoch: 6/15, Average loss: 165.3819\n",
      "Epoch: 7/15, Average loss: 164.5822\n",
      "Epoch: 8/15, Average loss: 163.8546\n",
      "Epoch: 9/15, Average loss: 163.2931\n",
      "Epoch: 10/15, Average loss: 162.7565\n",
      "Epoch: 11/15, Average loss: 162.2840\n",
      "Epoch: 12/15, Average loss: 161.8526\n",
      "Epoch: 13/15, Average loss: 161.5285\n",
      "Epoch: 14/15, Average loss: 161.1831\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 190.2715\n",
      "Epoch: 1/15, Average loss: 175.0285\n",
      "Epoch: 2/15, Average loss: 171.1064\n",
      "Epoch: 3/15, Average loss: 168.7353\n",
      "Epoch: 4/15, Average loss: 166.9982\n",
      "Epoch: 5/15, Average loss: 165.7492\n",
      "Epoch: 6/15, Average loss: 164.7856\n",
      "Epoch: 7/15, Average loss: 164.0175\n",
      "Epoch: 8/15, Average loss: 163.3324\n",
      "Epoch: 9/15, Average loss: 162.6994\n",
      "Epoch: 10/15, Average loss: 162.0603\n",
      "Epoch: 11/15, Average loss: 161.6777\n",
      "Epoch: 12/15, Average loss: 161.2303\n",
      "Epoch: 13/15, Average loss: 160.8520\n",
      "Epoch: 14/15, Average loss: 160.4095\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 205.7477\n",
      "Epoch: 1/15, Average loss: 187.9862\n",
      "Epoch: 2/15, Average loss: 182.9358\n",
      "Epoch: 3/15, Average loss: 179.8979\n",
      "Epoch: 4/15, Average loss: 177.6414\n",
      "Epoch: 5/15, Average loss: 176.0550\n",
      "Epoch: 6/15, Average loss: 174.5772\n",
      "Epoch: 7/15, Average loss: 173.3865\n",
      "Epoch: 8/15, Average loss: 172.4173\n",
      "Epoch: 9/15, Average loss: 171.5957\n",
      "Epoch: 10/15, Average loss: 170.7571\n",
      "Epoch: 11/15, Average loss: 170.1503\n",
      "Epoch: 12/15, Average loss: 169.4242\n",
      "Epoch: 13/15, Average loss: 168.8996\n",
      "Epoch: 14/15, Average loss: 168.4701\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 209.4241\n",
      "Epoch: 1/15, Average loss: 191.0073\n",
      "Epoch: 2/15, Average loss: 185.6736\n",
      "Epoch: 3/15, Average loss: 182.3992\n",
      "Epoch: 4/15, Average loss: 179.9405\n",
      "Epoch: 5/15, Average loss: 178.3684\n",
      "Epoch: 6/15, Average loss: 176.6909\n",
      "Epoch: 7/15, Average loss: 175.4929\n",
      "Epoch: 8/15, Average loss: 174.4900\n",
      "Epoch: 9/15, Average loss: 173.5293\n",
      "Epoch: 10/15, Average loss: 172.6143\n",
      "Epoch: 11/15, Average loss: 171.9010\n",
      "Epoch: 12/15, Average loss: 171.3949\n",
      "Epoch: 13/15, Average loss: 170.7261\n",
      "Epoch: 14/15, Average loss: 170.2107\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 218.3337\n",
      "Epoch: 1/15, Average loss: 202.8109\n",
      "Epoch: 2/15, Average loss: 197.2647\n",
      "Epoch: 3/15, Average loss: 193.8361\n",
      "Epoch: 4/15, Average loss: 191.9366\n",
      "Epoch: 5/15, Average loss: 189.8902\n",
      "Epoch: 6/15, Average loss: 188.1672\n",
      "Epoch: 7/15, Average loss: 186.7733\n",
      "Epoch: 8/15, Average loss: 185.6743\n",
      "Epoch: 9/15, Average loss: 184.4807\n",
      "Epoch: 10/15, Average loss: 183.5472\n",
      "Epoch: 11/15, Average loss: 182.6977\n",
      "Epoch: 12/15, Average loss: 181.9317\n",
      "Epoch: 13/15, Average loss: 181.2428\n",
      "Epoch: 14/15, Average loss: 180.6594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca481267",
   "metadata": {},
   "source": [
    "## FineGrainMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0052a",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11bc426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab444bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 350.1548\n",
      "Epoch: 1/15, Average loss: 253.1009\n",
      "Epoch: 2/15, Average loss: 220.7762\n",
      "Epoch: 3/15, Average loss: 203.9781\n",
      "Epoch: 4/15, Average loss: 193.5875\n",
      "Epoch: 5/15, Average loss: 186.7863\n",
      "Epoch: 6/15, Average loss: 182.2406\n",
      "Epoch: 7/15, Average loss: 179.1544\n",
      "Epoch: 8/15, Average loss: 176.9054\n",
      "Epoch: 9/15, Average loss: 175.2276\n",
      "Epoch: 10/15, Average loss: 174.0070\n",
      "Epoch: 11/15, Average loss: 172.9156\n",
      "Epoch: 12/15, Average loss: 171.9834\n",
      "Epoch: 13/15, Average loss: 171.3669\n",
      "Epoch: 14/15, Average loss: 170.7154\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 352.0988\n",
      "Epoch: 1/15, Average loss: 251.0358\n",
      "Epoch: 2/15, Average loss: 217.2974\n",
      "Epoch: 3/15, Average loss: 200.0545\n",
      "Epoch: 4/15, Average loss: 189.6971\n",
      "Epoch: 5/15, Average loss: 182.8278\n",
      "Epoch: 6/15, Average loss: 177.8364\n",
      "Epoch: 7/15, Average loss: 174.0655\n",
      "Epoch: 8/15, Average loss: 171.3912\n",
      "Epoch: 9/15, Average loss: 169.0858\n",
      "Epoch: 10/15, Average loss: 167.4931\n",
      "Epoch: 11/15, Average loss: 166.1182\n",
      "Epoch: 12/15, Average loss: 165.0023\n",
      "Epoch: 13/15, Average loss: 164.0743\n",
      "Epoch: 14/15, Average loss: 163.2456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 353.1490\n",
      "Epoch: 1/15, Average loss: 252.2755\n",
      "Epoch: 2/15, Average loss: 218.2468\n",
      "Epoch: 3/15, Average loss: 200.4980\n",
      "Epoch: 4/15, Average loss: 190.2130\n",
      "Epoch: 5/15, Average loss: 183.5315\n",
      "Epoch: 6/15, Average loss: 179.1006\n",
      "Epoch: 7/15, Average loss: 175.7140\n",
      "Epoch: 8/15, Average loss: 173.1074\n",
      "Epoch: 9/15, Average loss: 171.1868\n",
      "Epoch: 10/15, Average loss: 169.6781\n",
      "Epoch: 11/15, Average loss: 168.3934\n",
      "Epoch: 12/15, Average loss: 167.1742\n",
      "Epoch: 13/15, Average loss: 166.2797\n",
      "Epoch: 14/15, Average loss: 165.4711\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 359.6497\n",
      "Epoch: 1/15, Average loss: 256.7990\n",
      "Epoch: 2/15, Average loss: 222.4290\n",
      "Epoch: 3/15, Average loss: 205.5026\n",
      "Epoch: 4/15, Average loss: 196.8224\n",
      "Epoch: 5/15, Average loss: 191.4309\n",
      "Epoch: 6/15, Average loss: 187.7220\n",
      "Epoch: 7/15, Average loss: 184.9673\n",
      "Epoch: 8/15, Average loss: 182.7030\n",
      "Epoch: 9/15, Average loss: 180.6239\n",
      "Epoch: 10/15, Average loss: 178.9949\n",
      "Epoch: 11/15, Average loss: 177.4248\n",
      "Epoch: 12/15, Average loss: 176.2233\n",
      "Epoch: 13/15, Average loss: 175.0779\n",
      "Epoch: 14/15, Average loss: 174.1343\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 359.5036\n",
      "Epoch: 1/15, Average loss: 259.3319\n",
      "Epoch: 2/15, Average loss: 224.9798\n",
      "Epoch: 3/15, Average loss: 206.6746\n",
      "Epoch: 4/15, Average loss: 196.8557\n",
      "Epoch: 5/15, Average loss: 190.7042\n",
      "Epoch: 6/15, Average loss: 186.5188\n",
      "Epoch: 7/15, Average loss: 183.5274\n",
      "Epoch: 8/15, Average loss: 181.0153\n",
      "Epoch: 9/15, Average loss: 179.1720\n",
      "Epoch: 10/15, Average loss: 177.6520\n",
      "Epoch: 11/15, Average loss: 176.4386\n",
      "Epoch: 12/15, Average loss: 175.2991\n",
      "Epoch: 13/15, Average loss: 174.4466\n",
      "Epoch: 14/15, Average loss: 173.5305\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 373.2254\n",
      "Epoch: 1/15, Average loss: 274.5785\n",
      "Epoch: 2/15, Average loss: 244.4096\n",
      "Epoch: 3/15, Average loss: 225.6048\n",
      "Epoch: 4/15, Average loss: 214.5333\n",
      "Epoch: 5/15, Average loss: 207.8231\n",
      "Epoch: 6/15, Average loss: 203.0017\n",
      "Epoch: 7/15, Average loss: 198.5724\n",
      "Epoch: 8/15, Average loss: 195.6498\n",
      "Epoch: 9/15, Average loss: 193.2414\n",
      "Epoch: 10/15, Average loss: 191.0763\n",
      "Epoch: 11/15, Average loss: 189.6097\n",
      "Epoch: 12/15, Average loss: 188.1309\n",
      "Epoch: 13/15, Average loss: 186.6452\n",
      "Epoch: 14/15, Average loss: 185.3618\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63632454",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d843d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1353f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 128.8488\n",
      "Epoch: 1/15, Average loss: 118.9810\n",
      "Epoch: 2/15, Average loss: 115.5914\n",
      "Epoch: 3/15, Average loss: 113.6978\n",
      "Epoch: 4/15, Average loss: 112.2368\n",
      "Epoch: 5/15, Average loss: 111.3334\n",
      "Epoch: 6/15, Average loss: 110.5636\n",
      "Epoch: 7/15, Average loss: 109.7967\n",
      "Epoch: 8/15, Average loss: 109.3749\n",
      "Epoch: 9/15, Average loss: 108.9270\n",
      "Epoch: 10/15, Average loss: 108.5444\n",
      "Epoch: 11/15, Average loss: 108.2124\n",
      "Epoch: 12/15, Average loss: 107.9081\n",
      "Epoch: 13/15, Average loss: 107.7173\n",
      "Epoch: 14/15, Average loss: 107.4304\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 123.7543\n",
      "Epoch: 1/15, Average loss: 113.4090\n",
      "Epoch: 2/15, Average loss: 110.3765\n",
      "Epoch: 3/15, Average loss: 108.4472\n",
      "Epoch: 4/15, Average loss: 107.0033\n",
      "Epoch: 5/15, Average loss: 106.0164\n",
      "Epoch: 6/15, Average loss: 105.1018\n",
      "Epoch: 7/15, Average loss: 104.4306\n",
      "Epoch: 8/15, Average loss: 103.9221\n",
      "Epoch: 9/15, Average loss: 103.5604\n",
      "Epoch: 10/15, Average loss: 103.0635\n",
      "Epoch: 11/15, Average loss: 102.7131\n",
      "Epoch: 12/15, Average loss: 102.3782\n",
      "Epoch: 13/15, Average loss: 102.1153\n",
      "Epoch: 14/15, Average loss: 101.9149\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 125.0447\n",
      "Epoch: 1/15, Average loss: 114.6703\n",
      "Epoch: 2/15, Average loss: 111.3006\n",
      "Epoch: 3/15, Average loss: 109.4904\n",
      "Epoch: 4/15, Average loss: 108.0353\n",
      "Epoch: 5/15, Average loss: 106.9341\n",
      "Epoch: 6/15, Average loss: 105.9661\n",
      "Epoch: 7/15, Average loss: 105.4024\n",
      "Epoch: 8/15, Average loss: 104.9085\n",
      "Epoch: 9/15, Average loss: 104.3502\n",
      "Epoch: 10/15, Average loss: 103.9870\n",
      "Epoch: 11/15, Average loss: 103.5208\n",
      "Epoch: 12/15, Average loss: 103.1821\n",
      "Epoch: 13/15, Average loss: 102.9151\n",
      "Epoch: 14/15, Average loss: 102.5928\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 131.0479\n",
      "Epoch: 1/15, Average loss: 120.4279\n",
      "Epoch: 2/15, Average loss: 117.2300\n",
      "Epoch: 3/15, Average loss: 115.1342\n",
      "Epoch: 4/15, Average loss: 113.6301\n",
      "Epoch: 5/15, Average loss: 112.6094\n",
      "Epoch: 6/15, Average loss: 111.6296\n",
      "Epoch: 7/15, Average loss: 110.8688\n",
      "Epoch: 8/15, Average loss: 110.2754\n",
      "Epoch: 9/15, Average loss: 109.8956\n",
      "Epoch: 10/15, Average loss: 109.2989\n",
      "Epoch: 11/15, Average loss: 108.7665\n",
      "Epoch: 12/15, Average loss: 108.1630\n",
      "Epoch: 13/15, Average loss: 108.0868\n",
      "Epoch: 14/15, Average loss: 107.7894\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 132.3936\n",
      "Epoch: 1/15, Average loss: 121.1099\n",
      "Epoch: 2/15, Average loss: 117.7558\n",
      "Epoch: 3/15, Average loss: 115.5335\n",
      "Epoch: 4/15, Average loss: 114.1492\n",
      "Epoch: 5/15, Average loss: 112.9768\n",
      "Epoch: 6/15, Average loss: 112.2053\n",
      "Epoch: 7/15, Average loss: 111.4527\n",
      "Epoch: 8/15, Average loss: 110.5202\n",
      "Epoch: 9/15, Average loss: 110.0666\n",
      "Epoch: 10/15, Average loss: 109.5930\n",
      "Epoch: 11/15, Average loss: 109.1886\n",
      "Epoch: 12/15, Average loss: 108.6555\n",
      "Epoch: 13/15, Average loss: 108.5454\n",
      "Epoch: 14/15, Average loss: 107.9828\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 140.7163\n",
      "Epoch: 1/15, Average loss: 129.3556\n",
      "Epoch: 2/15, Average loss: 125.7106\n",
      "Epoch: 3/15, Average loss: 123.3617\n",
      "Epoch: 4/15, Average loss: 121.5969\n",
      "Epoch: 5/15, Average loss: 120.2030\n",
      "Epoch: 6/15, Average loss: 119.2593\n",
      "Epoch: 7/15, Average loss: 118.2916\n",
      "Epoch: 8/15, Average loss: 117.4886\n",
      "Epoch: 9/15, Average loss: 116.8479\n",
      "Epoch: 10/15, Average loss: 116.1349\n",
      "Epoch: 11/15, Average loss: 115.5264\n",
      "Epoch: 12/15, Average loss: 115.0161\n",
      "Epoch: 13/15, Average loss: 114.5984\n",
      "Epoch: 14/15, Average loss: 114.2253\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491db66",
   "metadata": {},
   "source": [
    "### 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f57addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b995627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 168.1074\n",
      "Epoch: 1/15, Average loss: 157.9048\n",
      "Epoch: 2/15, Average loss: 154.9265\n",
      "Epoch: 3/15, Average loss: 153.2129\n",
      "Epoch: 4/15, Average loss: 151.9832\n",
      "Epoch: 5/15, Average loss: 150.9830\n",
      "Epoch: 6/15, Average loss: 150.3146\n",
      "Epoch: 7/15, Average loss: 149.7127\n",
      "Epoch: 8/15, Average loss: 149.1699\n",
      "Epoch: 9/15, Average loss: 148.6808\n",
      "Epoch: 10/15, Average loss: 148.3465\n",
      "Epoch: 11/15, Average loss: 147.9847\n",
      "Epoch: 12/15, Average loss: 147.6339\n",
      "Epoch: 13/15, Average loss: 147.2490\n",
      "Epoch: 14/15, Average loss: 146.8601\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 161.4670\n",
      "Epoch: 1/15, Average loss: 150.4652\n",
      "Epoch: 2/15, Average loss: 147.3623\n",
      "Epoch: 3/15, Average loss: 145.4953\n",
      "Epoch: 4/15, Average loss: 144.2479\n",
      "Epoch: 5/15, Average loss: 143.1333\n",
      "Epoch: 6/15, Average loss: 142.4476\n",
      "Epoch: 7/15, Average loss: 141.6326\n",
      "Epoch: 8/15, Average loss: 140.9852\n",
      "Epoch: 9/15, Average loss: 140.5891\n",
      "Epoch: 10/15, Average loss: 140.1519\n",
      "Epoch: 11/15, Average loss: 139.6741\n",
      "Epoch: 12/15, Average loss: 139.1920\n",
      "Epoch: 13/15, Average loss: 138.9380\n",
      "Epoch: 14/15, Average loss: 138.5747\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 163.2038\n",
      "Epoch: 1/15, Average loss: 152.2257\n",
      "Epoch: 2/15, Average loss: 149.0665\n",
      "Epoch: 3/15, Average loss: 147.1936\n",
      "Epoch: 4/15, Average loss: 145.7688\n",
      "Epoch: 5/15, Average loss: 144.5255\n",
      "Epoch: 6/15, Average loss: 143.5279\n",
      "Epoch: 7/15, Average loss: 142.9947\n",
      "Epoch: 8/15, Average loss: 142.1002\n",
      "Epoch: 9/15, Average loss: 141.6346\n",
      "Epoch: 10/15, Average loss: 141.0501\n",
      "Epoch: 11/15, Average loss: 140.6280\n",
      "Epoch: 12/15, Average loss: 140.1687\n",
      "Epoch: 13/15, Average loss: 139.8108\n",
      "Epoch: 14/15, Average loss: 139.3231\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 169.1245\n",
      "Epoch: 1/15, Average loss: 158.5942\n",
      "Epoch: 2/15, Average loss: 155.2144\n",
      "Epoch: 3/15, Average loss: 152.8738\n",
      "Epoch: 4/15, Average loss: 151.2621\n",
      "Epoch: 5/15, Average loss: 149.9865\n",
      "Epoch: 6/15, Average loss: 149.0508\n",
      "Epoch: 7/15, Average loss: 148.3559\n",
      "Epoch: 8/15, Average loss: 147.4163\n",
      "Epoch: 9/15, Average loss: 146.8933\n",
      "Epoch: 10/15, Average loss: 146.3767\n",
      "Epoch: 11/15, Average loss: 145.6512\n",
      "Epoch: 12/15, Average loss: 145.1953\n",
      "Epoch: 13/15, Average loss: 144.8304\n",
      "Epoch: 14/15, Average loss: 144.4839\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 170.9077\n",
      "Epoch: 1/15, Average loss: 159.8842\n",
      "Epoch: 2/15, Average loss: 156.6826\n",
      "Epoch: 3/15, Average loss: 154.8419\n",
      "Epoch: 4/15, Average loss: 153.3940\n",
      "Epoch: 5/15, Average loss: 152.3461\n",
      "Epoch: 6/15, Average loss: 151.2014\n",
      "Epoch: 7/15, Average loss: 150.4247\n",
      "Epoch: 8/15, Average loss: 149.6140\n",
      "Epoch: 9/15, Average loss: 149.0386\n",
      "Epoch: 10/15, Average loss: 148.4432\n",
      "Epoch: 11/15, Average loss: 147.8556\n",
      "Epoch: 12/15, Average loss: 147.3405\n",
      "Epoch: 13/15, Average loss: 146.9495\n",
      "Epoch: 14/15, Average loss: 146.7316\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 176.2220\n",
      "Epoch: 1/15, Average loss: 164.9950\n",
      "Epoch: 2/15, Average loss: 161.8325\n",
      "Epoch: 3/15, Average loss: 159.7446\n",
      "Epoch: 4/15, Average loss: 158.2351\n",
      "Epoch: 5/15, Average loss: 156.9886\n",
      "Epoch: 6/15, Average loss: 156.0197\n",
      "Epoch: 7/15, Average loss: 154.9983\n",
      "Epoch: 8/15, Average loss: 154.4287\n",
      "Epoch: 9/15, Average loss: 153.5127\n",
      "Epoch: 10/15, Average loss: 153.0980\n",
      "Epoch: 11/15, Average loss: 152.2131\n",
      "Epoch: 12/15, Average loss: 151.8847\n",
      "Epoch: 13/15, Average loss: 151.2500\n",
      "Epoch: 14/15, Average loss: 150.9008\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1037f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913cfef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
