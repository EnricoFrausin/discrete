{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d041b96",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import VAE_priorCategorical, VAE_priorHFM\n",
    "import metadata as md\n",
    "from train import train\n",
    "from datasets import Dataset_HFM, Dataset_pureHFM, MNISTDigit2Dataset\n",
    "from utilities import sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58f0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(md.seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(md.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1647",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe9c74",
   "metadata": {},
   "source": [
    "## train over ExtendedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4db29123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eef56",
   "metadata": {},
   "source": [
    "## train over 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3bba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=32, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=32, shuffle=True)\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09921586",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0bae",
   "metadata": {},
   "source": [
    "## train over FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2094def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682e2",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2822c7",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db343",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4b01e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e9adabe",
   "metadata": {},
   "source": [
    "# Prior Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98cd69",
   "metadata": {},
   "source": [
    "## train over Extended_MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c091a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6ed9f",
   "metadata": {},
   "source": [
    "\n",
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 354.4975\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53143931",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 349.8097\n",
      "Epoch: 1/15, Average loss: 252.5678\n",
      "Epoch: 2/15, Average loss: 220.1362\n",
      "Epoch: 3/15, Average loss: 203.2145\n",
      "Epoch: 4/15, Average loss: 193.3282\n",
      "Epoch: 5/15, Average loss: 186.8372\n",
      "Epoch: 6/15, Average loss: 182.4854\n",
      "Epoch: 7/15, Average loss: 179.3084\n",
      "Epoch: 8/15, Average loss: 177.0011\n",
      "Epoch: 9/15, Average loss: 175.3268\n",
      "Epoch: 10/15, Average loss: 173.9250\n",
      "Epoch: 11/15, Average loss: 172.8551\n",
      "Epoch: 12/15, Average loss: 171.9738\n",
      "Epoch: 13/15, Average loss: 171.2925\n",
      "Epoch: 14/15, Average loss: 170.7406\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 350.6189\n",
      "Epoch: 1/15, Average loss: 250.9468\n",
      "Epoch: 2/15, Average loss: 217.6095\n",
      "Epoch: 3/15, Average loss: 200.6092\n",
      "Epoch: 4/15, Average loss: 190.5887\n",
      "Epoch: 5/15, Average loss: 183.7372\n",
      "Epoch: 6/15, Average loss: 178.4993\n",
      "Epoch: 7/15, Average loss: 174.6541\n",
      "Epoch: 8/15, Average loss: 171.8537\n",
      "Epoch: 9/15, Average loss: 169.7876\n",
      "Epoch: 10/15, Average loss: 167.9942\n",
      "Epoch: 11/15, Average loss: 166.7761\n",
      "Epoch: 12/15, Average loss: 165.5641\n",
      "Epoch: 13/15, Average loss: 164.6842\n",
      "Epoch: 14/15, Average loss: 163.9550\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 354.0489\n",
      "Epoch: 1/15, Average loss: 252.5556\n",
      "Epoch: 2/15, Average loss: 217.8604\n",
      "Epoch: 3/15, Average loss: 200.3722\n",
      "Epoch: 4/15, Average loss: 190.7886\n",
      "Epoch: 5/15, Average loss: 184.6503\n",
      "Epoch: 6/15, Average loss: 180.4723\n",
      "Epoch: 7/15, Average loss: 177.2928\n",
      "Epoch: 8/15, Average loss: 174.8085\n",
      "Epoch: 9/15, Average loss: 172.9309\n",
      "Epoch: 10/15, Average loss: 171.3527\n",
      "Epoch: 11/15, Average loss: 169.9989\n",
      "Epoch: 12/15, Average loss: 168.9262\n",
      "Epoch: 13/15, Average loss: 167.9097\n",
      "Epoch: 14/15, Average loss: 167.0546\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.5319\n",
      "Epoch: 1/15, Average loss: 257.6598\n",
      "Epoch: 2/15, Average loss: 222.6616\n",
      "Epoch: 3/15, Average loss: 205.1415\n",
      "Epoch: 4/15, Average loss: 195.8920\n",
      "Epoch: 5/15, Average loss: 190.0039\n",
      "Epoch: 6/15, Average loss: 185.8355\n",
      "Epoch: 7/15, Average loss: 182.7668\n",
      "Epoch: 8/15, Average loss: 180.3982\n",
      "Epoch: 9/15, Average loss: 178.4741\n",
      "Epoch: 10/15, Average loss: 176.9525\n",
      "Epoch: 11/15, Average loss: 175.7419\n",
      "Epoch: 12/15, Average loss: 174.6741\n",
      "Epoch: 13/15, Average loss: 173.7510\n",
      "Epoch: 14/15, Average loss: 172.8685\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.8176\n",
      "Epoch: 1/15, Average loss: 262.1270\n",
      "Epoch: 2/15, Average loss: 228.4061\n",
      "Epoch: 3/15, Average loss: 210.9515\n",
      "Epoch: 4/15, Average loss: 201.2823\n",
      "Epoch: 5/15, Average loss: 195.2321\n",
      "Epoch: 6/15, Average loss: 191.0820\n",
      "Epoch: 7/15, Average loss: 187.9966\n",
      "Epoch: 8/15, Average loss: 185.8052\n",
      "Epoch: 9/15, Average loss: 183.8282\n",
      "Epoch: 10/15, Average loss: 182.1945\n",
      "Epoch: 11/15, Average loss: 180.5830\n",
      "Epoch: 12/15, Average loss: 179.3044\n",
      "Epoch: 13/15, Average loss: 178.0446\n",
      "Epoch: 14/15, Average loss: 176.7816\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 363.8745\n",
      "Epoch: 1/15, Average loss: 265.8502\n",
      "Epoch: 2/15, Average loss: 234.5291\n",
      "Epoch: 3/15, Average loss: 215.7438\n",
      "Epoch: 4/15, Average loss: 205.0702\n",
      "Epoch: 5/15, Average loss: 198.5530\n",
      "Epoch: 6/15, Average loss: 194.0660\n",
      "Epoch: 7/15, Average loss: 190.3498\n",
      "Epoch: 8/15, Average loss: 187.6208\n",
      "Epoch: 9/15, Average loss: 185.2275\n",
      "Epoch: 10/15, Average loss: 183.3724\n",
      "Epoch: 11/15, Average loss: 181.8250\n",
      "Epoch: 12/15, Average loss: 180.4728\n",
      "Epoch: 13/15, Average loss: 179.2218\n",
      "Epoch: 14/15, Average loss: 178.1701\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 378.2936\n",
      "Epoch: 1/15, Average loss: 293.8599\n",
      "Epoch: 2/15, Average loss: 283.2834\n",
      "Epoch: 3/15, Average loss: 280.1153\n",
      "Epoch: 4/15, Average loss: 278.8265\n",
      "Epoch: 5/15, Average loss: 278.2186\n",
      "Epoch: 6/15, Average loss: 277.9094\n",
      "Epoch: 7/15, Average loss: 277.7441\n",
      "Epoch: 8/15, Average loss: 277.6542\n",
      "Epoch: 9/15, Average loss: 277.6033\n",
      "Epoch: 10/15, Average loss: 277.5740\n",
      "Epoch: 11/15, Average loss: 277.5544\n",
      "Epoch: 12/15, Average loss: 277.5417\n",
      "Epoch: 13/15, Average loss: 277.5356\n",
      "Epoch: 14/15, Average loss: 277.5299\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad702",
   "metadata": {},
   "source": [
    "## train over 2Digit_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bd9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c19a7f",
   "metadata": {},
   "source": [
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b37487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 285.8402\n",
      "Epoch: 1/15, Average loss: 192.3872\n",
      "Epoch: 2/15, Average loss: 173.1508\n",
      "Epoch: 3/15, Average loss: 165.8688\n",
      "Epoch: 4/15, Average loss: 161.6645\n",
      "Epoch: 5/15, Average loss: 158.7423\n",
      "Epoch: 6/15, Average loss: 156.5480\n",
      "Epoch: 7/15, Average loss: 154.9280\n",
      "Epoch: 8/15, Average loss: 153.5633\n",
      "Epoch: 9/15, Average loss: 152.5198\n",
      "Epoch: 10/15, Average loss: 151.5924\n",
      "Epoch: 11/15, Average loss: 150.8315\n",
      "Epoch: 12/15, Average loss: 150.1471\n",
      "Epoch: 13/15, Average loss: 149.6125\n",
      "Epoch: 14/15, Average loss: 149.1944\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 287.4816\n",
      "Epoch: 1/15, Average loss: 190.9797\n",
      "Epoch: 2/15, Average loss: 170.6302\n",
      "Epoch: 3/15, Average loss: 163.1861\n",
      "Epoch: 4/15, Average loss: 158.7912\n",
      "Epoch: 5/15, Average loss: 155.4250\n",
      "Epoch: 6/15, Average loss: 152.9431\n",
      "Epoch: 7/15, Average loss: 150.7422\n",
      "Epoch: 8/15, Average loss: 148.8840\n",
      "Epoch: 9/15, Average loss: 147.4339\n",
      "Epoch: 10/15, Average loss: 146.1615\n",
      "Epoch: 11/15, Average loss: 145.0763\n",
      "Epoch: 12/15, Average loss: 144.4174\n",
      "Epoch: 13/15, Average loss: 143.5539\n",
      "Epoch: 14/15, Average loss: 142.8752\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 290.9001\n",
      "Epoch: 1/15, Average loss: 196.5124\n",
      "Epoch: 2/15, Average loss: 173.9782\n",
      "Epoch: 3/15, Average loss: 165.1162\n",
      "Epoch: 4/15, Average loss: 160.2100\n",
      "Epoch: 5/15, Average loss: 156.8794\n",
      "Epoch: 6/15, Average loss: 154.1877\n",
      "Epoch: 7/15, Average loss: 152.0424\n",
      "Epoch: 8/15, Average loss: 150.1410\n",
      "Epoch: 9/15, Average loss: 148.6678\n",
      "Epoch: 10/15, Average loss: 147.4285\n",
      "Epoch: 11/15, Average loss: 146.3758\n",
      "Epoch: 12/15, Average loss: 145.3628\n",
      "Epoch: 13/15, Average loss: 144.6261\n",
      "Epoch: 14/15, Average loss: 143.9215\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 291.8053\n",
      "Epoch: 1/15, Average loss: 198.0273\n",
      "Epoch: 2/15, Average loss: 177.2334\n",
      "Epoch: 3/15, Average loss: 169.5986\n",
      "Epoch: 4/15, Average loss: 165.3874\n",
      "Epoch: 5/15, Average loss: 162.4505\n",
      "Epoch: 6/15, Average loss: 160.1371\n",
      "Epoch: 7/15, Average loss: 158.2938\n",
      "Epoch: 8/15, Average loss: 156.7026\n",
      "Epoch: 9/15, Average loss: 155.3247\n",
      "Epoch: 10/15, Average loss: 154.0009\n",
      "Epoch: 11/15, Average loss: 153.0138\n",
      "Epoch: 12/15, Average loss: 151.8354\n",
      "Epoch: 13/15, Average loss: 151.0117\n",
      "Epoch: 14/15, Average loss: 150.2594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 297.5956\n",
      "Epoch: 1/15, Average loss: 205.3879\n",
      "Epoch: 2/15, Average loss: 185.1229\n",
      "Epoch: 3/15, Average loss: 176.3958\n",
      "Epoch: 4/15, Average loss: 170.8192\n",
      "Epoch: 5/15, Average loss: 167.5699\n",
      "Epoch: 6/15, Average loss: 165.2748\n",
      "Epoch: 7/15, Average loss: 163.5643\n",
      "Epoch: 8/15, Average loss: 162.0262\n",
      "Epoch: 9/15, Average loss: 160.7202\n",
      "Epoch: 10/15, Average loss: 159.7310\n",
      "Epoch: 11/15, Average loss: 158.6797\n",
      "Epoch: 12/15, Average loss: 157.6331\n",
      "Epoch: 13/15, Average loss: 156.6957\n",
      "Epoch: 14/15, Average loss: 156.0575\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.9850\n",
      "Epoch: 1/15, Average loss: 233.6360\n",
      "Epoch: 2/15, Average loss: 234.9691\n",
      "Epoch: 3/15, Average loss: 236.2286\n",
      "Epoch: 4/15, Average loss: 235.9187\n",
      "Epoch: 5/15, Average loss: 235.7412\n",
      "Epoch: 6/15, Average loss: 235.6988\n",
      "Epoch: 7/15, Average loss: 235.6745\n",
      "Epoch: 8/15, Average loss: 235.6689\n",
      "Epoch: 9/15, Average loss: 235.6626\n",
      "Epoch: 10/15, Average loss: 235.6579\n",
      "Epoch: 11/15, Average loss: 235.6528\n",
      "Epoch: 12/15, Average loss: 235.6548\n",
      "Epoch: 13/15, Average loss: 235.6487\n",
      "Epoch: 14/15, Average loss: 235.6504\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f5400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 368.7919\n",
      "Epoch: 1/15, Average loss: 268.6005\n",
      "Epoch: 1/15, Average loss: 268.6005\n",
      "Epoch: 2/15, Average loss: 240.0480\n",
      "Epoch: 2/15, Average loss: 240.0480\n",
      "Epoch: 3/15, Average loss: 224.2530\n",
      "Epoch: 3/15, Average loss: 224.2530\n",
      "Epoch: 4/15, Average loss: 214.7301\n",
      "Epoch: 4/15, Average loss: 214.7301\n",
      "Epoch: 5/15, Average loss: 208.3547\n",
      "Epoch: 5/15, Average loss: 208.3547\n",
      "Epoch: 6/15, Average loss: 204.1407\n",
      "Epoch: 6/15, Average loss: 204.1407\n",
      "Epoch: 7/15, Average loss: 201.0423\n",
      "Epoch: 7/15, Average loss: 201.0423\n",
      "Epoch: 8/15, Average loss: 198.1865\n",
      "Epoch: 8/15, Average loss: 198.1865\n",
      "Epoch: 9/15, Average loss: 196.3216\n",
      "Epoch: 9/15, Average loss: 196.3216\n",
      "Epoch: 10/15, Average loss: 194.5197\n",
      "Epoch: 10/15, Average loss: 194.5197\n",
      "Epoch: 11/15, Average loss: 193.1328\n",
      "Epoch: 11/15, Average loss: 193.1328\n",
      "Epoch: 12/15, Average loss: 191.9021\n",
      "Epoch: 12/15, Average loss: 191.9021\n",
      "Epoch: 13/15, Average loss: 190.4889\n",
      "Epoch: 13/15, Average loss: 190.4889\n",
      "Epoch: 14/15, Average loss: 189.2890\n",
      "Epoch: 14/15, Average loss: 189.2890\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a0c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 375.6724\n",
      "Epoch: 1/15, Average loss: 288.8285\n",
      "Epoch: 1/15, Average loss: 288.8285\n",
      "Epoch: 2/15, Average loss: 276.6543\n",
      "Epoch: 2/15, Average loss: 276.6543\n",
      "Epoch: 3/15, Average loss: 278.5518\n",
      "Epoch: 3/15, Average loss: 278.5518\n",
      "Epoch: 4/15, Average loss: 278.8145\n",
      "Epoch: 4/15, Average loss: 278.8145\n",
      "Epoch: 5/15, Average loss: 278.2678\n",
      "Epoch: 5/15, Average loss: 278.2678\n",
      "Epoch: 6/15, Average loss: 277.9788\n",
      "Epoch: 6/15, Average loss: 277.9788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m8\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m     20\u001b[39m loss, KL, rec_loss = model(data, temp, _lambda, hard=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m train_loss += loss.item() * \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m     23\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d42e3",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84289a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 283.1685\n",
      "Epoch: 1/15, Average loss: 185.3117\n",
      "Epoch: 2/15, Average loss: 164.7155\n",
      "Epoch: 3/15, Average loss: 157.4244\n",
      "Epoch: 4/15, Average loss: 153.4415\n",
      "Epoch: 5/15, Average loss: 150.7438\n",
      "Epoch: 6/15, Average loss: 148.3479\n",
      "Epoch: 7/15, Average loss: 146.4487\n",
      "Epoch: 8/15, Average loss: 144.7900\n",
      "Epoch: 9/15, Average loss: 143.5119\n",
      "Epoch: 10/15, Average loss: 142.3876\n",
      "Epoch: 11/15, Average loss: 141.5184\n",
      "Epoch: 12/15, Average loss: 140.7268\n",
      "Epoch: 13/15, Average loss: 140.0654\n",
      "Epoch: 14/15, Average loss: 139.5796\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 285.5469\n",
      "Epoch: 1/15, Average loss: 185.9941\n",
      "Epoch: 2/15, Average loss: 164.2068\n",
      "Epoch: 3/15, Average loss: 156.4230\n",
      "Epoch: 4/15, Average loss: 152.1274\n",
      "Epoch: 5/15, Average loss: 148.9055\n",
      "Epoch: 6/15, Average loss: 146.4946\n",
      "Epoch: 7/15, Average loss: 144.2853\n",
      "Epoch: 8/15, Average loss: 142.2925\n",
      "Epoch: 9/15, Average loss: 140.3256\n",
      "Epoch: 10/15, Average loss: 138.7986\n",
      "Epoch: 11/15, Average loss: 137.3139\n",
      "Epoch: 12/15, Average loss: 136.3773\n",
      "Epoch: 13/15, Average loss: 135.4085\n",
      "Epoch: 14/15, Average loss: 134.5825\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 288.2157\n",
      "Epoch: 1/15, Average loss: 190.3229\n",
      "Epoch: 2/15, Average loss: 166.5433\n",
      "Epoch: 3/15, Average loss: 158.1974\n",
      "Epoch: 4/15, Average loss: 153.8388\n",
      "Epoch: 5/15, Average loss: 150.7942\n",
      "Epoch: 6/15, Average loss: 148.0541\n",
      "Epoch: 7/15, Average loss: 146.1519\n",
      "Epoch: 8/15, Average loss: 144.2605\n",
      "Epoch: 9/15, Average loss: 142.6083\n",
      "Epoch: 10/15, Average loss: 141.1847\n",
      "Epoch: 11/15, Average loss: 140.1628\n",
      "Epoch: 12/15, Average loss: 139.0452\n",
      "Epoch: 13/15, Average loss: 138.1159\n",
      "Epoch: 14/15, Average loss: 137.0060\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 292.6447\n",
      "Epoch: 1/15, Average loss: 197.9149\n",
      "Epoch: 2/15, Average loss: 174.9593\n",
      "Epoch: 3/15, Average loss: 166.1940\n",
      "Epoch: 4/15, Average loss: 161.4899\n",
      "Epoch: 5/15, Average loss: 158.5956\n",
      "Epoch: 6/15, Average loss: 156.1927\n",
      "Epoch: 7/15, Average loss: 154.1684\n",
      "Epoch: 8/15, Average loss: 152.5084\n",
      "Epoch: 9/15, Average loss: 150.9156\n",
      "Epoch: 10/15, Average loss: 149.8175\n",
      "Epoch: 11/15, Average loss: 148.8333\n",
      "Epoch: 12/15, Average loss: 147.9190\n",
      "Epoch: 13/15, Average loss: 147.0166\n",
      "Epoch: 14/15, Average loss: 146.2063\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 296.1927\n",
      "Epoch: 1/15, Average loss: 202.5603\n",
      "Epoch: 2/15, Average loss: 180.3169\n",
      "Epoch: 3/15, Average loss: 169.5245\n",
      "Epoch: 4/15, Average loss: 164.7354\n",
      "Epoch: 5/15, Average loss: 162.0428\n",
      "Epoch: 6/15, Average loss: 160.1690\n",
      "Epoch: 7/15, Average loss: 158.7887\n",
      "Epoch: 8/15, Average loss: 157.5044\n",
      "Epoch: 9/15, Average loss: 156.3388\n",
      "Epoch: 10/15, Average loss: 155.3318\n",
      "Epoch: 11/15, Average loss: 154.3924\n",
      "Epoch: 12/15, Average loss: 153.4030\n",
      "Epoch: 13/15, Average loss: 152.6664\n",
      "Epoch: 14/15, Average loss: 151.8791\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 298.6763\n",
      "Epoch: 1/15, Average loss: 209.6428\n",
      "Epoch: 2/15, Average loss: 189.1423\n",
      "Epoch: 3/15, Average loss: 182.1639\n",
      "Epoch: 4/15, Average loss: 176.5622\n",
      "Epoch: 5/15, Average loss: 172.3086\n",
      "Epoch: 6/15, Average loss: 169.1917\n",
      "Epoch: 7/15, Average loss: 166.8988\n",
      "Epoch: 8/15, Average loss: 165.0673\n",
      "Epoch: 9/15, Average loss: 163.5395\n",
      "Epoch: 10/15, Average loss: 161.8891\n",
      "Epoch: 11/15, Average loss: 160.9079\n",
      "Epoch: 12/15, Average loss: 159.9002\n",
      "Epoch: 13/15, Average loss: 158.9873\n",
      "Epoch: 14/15, Average loss: 157.9587\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 306.2162\n",
      "Epoch: 1/15, Average loss: 241.1762\n",
      "Epoch: 2/15, Average loss: 237.4376\n",
      "Epoch: 3/15, Average loss: 236.2604\n",
      "Epoch: 4/15, Average loss: 235.8461\n",
      "Epoch: 5/15, Average loss: 235.7243\n",
      "Epoch: 6/15, Average loss: 235.6853\n",
      "Epoch: 7/15, Average loss: 235.6682\n",
      "Epoch: 8/15, Average loss: 235.6617\n",
      "Epoch: 9/15, Average loss: 235.6551\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     63\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m10\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m     64\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m     20\u001b[39m loss, KL, rec_loss = model(data, temp, _lambda, hard=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m train_loss += loss.item() * \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m     23\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7130fe",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbdaa5",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 386.3299\n",
      "Epoch: 1/15, Average loss: 269.6630\n",
      "Epoch: 2/15, Average loss: 220.6156\n",
      "Epoch: 3/15, Average loss: 192.3674\n",
      "Epoch: 4/15, Average loss: 172.4641\n",
      "Epoch: 5/15, Average loss: 158.6117\n",
      "Epoch: 6/15, Average loss: 148.8546\n",
      "Epoch: 7/15, Average loss: 141.7337\n",
      "Epoch: 8/15, Average loss: 136.4395\n",
      "Epoch: 9/15, Average loss: 132.2159\n",
      "Epoch: 10/15, Average loss: 128.9444\n",
      "Epoch: 11/15, Average loss: 126.1562\n",
      "Epoch: 12/15, Average loss: 123.7926\n",
      "Epoch: 13/15, Average loss: 121.9730\n",
      "Epoch: 14/15, Average loss: 120.2335\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 386.8003\n",
      "Epoch: 1/15, Average loss: 270.2383\n",
      "Epoch: 2/15, Average loss: 221.5904\n",
      "Epoch: 3/15, Average loss: 193.3287\n",
      "Epoch: 4/15, Average loss: 173.1111\n",
      "Epoch: 5/15, Average loss: 158.6518\n",
      "Epoch: 6/15, Average loss: 148.4285\n",
      "Epoch: 7/15, Average loss: 140.9530\n",
      "Epoch: 8/15, Average loss: 135.4636\n",
      "Epoch: 9/15, Average loss: 131.2472\n",
      "Epoch: 10/15, Average loss: 127.9704\n",
      "Epoch: 11/15, Average loss: 125.2929\n",
      "Epoch: 12/15, Average loss: 123.1115\n",
      "Epoch: 13/15, Average loss: 121.3816\n",
      "Epoch: 14/15, Average loss: 119.8358\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 388.8020\n",
      "Epoch: 1/15, Average loss: 271.3259\n",
      "Epoch: 2/15, Average loss: 222.7423\n",
      "Epoch: 3/15, Average loss: 194.5868\n",
      "Epoch: 4/15, Average loss: 174.3165\n",
      "Epoch: 5/15, Average loss: 160.1383\n",
      "Epoch: 6/15, Average loss: 150.3916\n",
      "Epoch: 7/15, Average loss: 143.2736\n",
      "Epoch: 8/15, Average loss: 137.8577\n",
      "Epoch: 9/15, Average loss: 133.7821\n",
      "Epoch: 10/15, Average loss: 130.6661\n",
      "Epoch: 11/15, Average loss: 128.2295\n",
      "Epoch: 12/15, Average loss: 126.0127\n",
      "Epoch: 13/15, Average loss: 124.4271\n",
      "Epoch: 14/15, Average loss: 123.0456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST//ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 402.5911\n",
      "Epoch: 1/15, Average loss: 275.8961\n",
      "Epoch: 2/15, Average loss: 226.4554\n",
      "Epoch: 3/15, Average loss: 198.4908\n",
      "Epoch: 4/15, Average loss: 177.8793\n",
      "Epoch: 5/15, Average loss: 163.3509\n",
      "Epoch: 6/15, Average loss: 153.2155\n",
      "Epoch: 7/15, Average loss: 146.2363\n",
      "Epoch: 8/15, Average loss: 141.0705\n",
      "Epoch: 9/15, Average loss: 137.0449\n",
      "Epoch: 10/15, Average loss: 134.0465\n",
      "Epoch: 11/15, Average loss: 131.8412\n",
      "Epoch: 12/15, Average loss: 130.0399\n",
      "Epoch: 13/15, Average loss: 128.2418\n",
      "Epoch: 14/15, Average loss: 127.0624\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050da7b7",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f34279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 382.6704\n",
      "Epoch: 1/15, Average loss: 269.1358\n",
      "Epoch: 2/15, Average loss: 219.8319\n",
      "Epoch: 3/15, Average loss: 190.9535\n",
      "Epoch: 4/15, Average loss: 170.5004\n",
      "Epoch: 5/15, Average loss: 156.3013\n",
      "Epoch: 6/15, Average loss: 146.3802\n",
      "Epoch: 7/15, Average loss: 139.2002\n",
      "Epoch: 8/15, Average loss: 133.7200\n",
      "Epoch: 9/15, Average loss: 129.5677\n",
      "Epoch: 10/15, Average loss: 126.1323\n",
      "Epoch: 11/15, Average loss: 123.3453\n",
      "Epoch: 12/15, Average loss: 121.0505\n",
      "Epoch: 13/15, Average loss: 119.1494\n",
      "Epoch: 14/15, Average loss: 117.6010\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 384.0852\n",
      "Epoch: 1/15, Average loss: 267.8581\n",
      "Epoch: 2/15, Average loss: 218.3796\n",
      "Epoch: 3/15, Average loss: 189.0615\n",
      "Epoch: 4/15, Average loss: 168.0823\n",
      "Epoch: 5/15, Average loss: 153.4838\n",
      "Epoch: 6/15, Average loss: 143.4466\n",
      "Epoch: 7/15, Average loss: 136.2004\n",
      "Epoch: 8/15, Average loss: 130.9332\n",
      "Epoch: 9/15, Average loss: 126.8407\n",
      "Epoch: 10/15, Average loss: 123.6340\n",
      "Epoch: 11/15, Average loss: 121.0773\n",
      "Epoch: 12/15, Average loss: 118.7623\n",
      "Epoch: 13/15, Average loss: 116.8151\n",
      "Epoch: 14/15, Average loss: 115.2380\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 385.8713\n",
      "Epoch: 1/15, Average loss: 269.0690\n",
      "Epoch: 2/15, Average loss: 219.5711\n",
      "Epoch: 3/15, Average loss: 190.0897\n",
      "Epoch: 4/15, Average loss: 169.0395\n",
      "Epoch: 5/15, Average loss: 154.4246\n",
      "Epoch: 6/15, Average loss: 144.1625\n",
      "Epoch: 7/15, Average loss: 136.9041\n",
      "Epoch: 8/15, Average loss: 131.5134\n",
      "Epoch: 9/15, Average loss: 127.3652\n",
      "Epoch: 10/15, Average loss: 124.1533\n",
      "Epoch: 11/15, Average loss: 121.4891\n",
      "Epoch: 12/15, Average loss: 119.3849\n",
      "Epoch: 13/15, Average loss: 117.5831\n",
      "Epoch: 14/15, Average loss: 116.1856\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.6227\n",
      "Epoch: 1/15, Average loss: 270.5158\n",
      "Epoch: 2/15, Average loss: 221.6360\n",
      "Epoch: 3/15, Average loss: 192.9631\n",
      "Epoch: 4/15, Average loss: 171.9100\n",
      "Epoch: 5/15, Average loss: 157.1226\n",
      "Epoch: 6/15, Average loss: 146.9147\n",
      "Epoch: 7/15, Average loss: 139.7606\n",
      "Epoch: 8/15, Average loss: 134.5452\n",
      "Epoch: 9/15, Average loss: 130.5691\n",
      "Epoch: 10/15, Average loss: 127.6468\n",
      "Epoch: 11/15, Average loss: 125.3244\n",
      "Epoch: 12/15, Average loss: 123.3504\n",
      "Epoch: 13/15, Average loss: 121.7893\n",
      "Epoch: 14/15, Average loss: 120.3889\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.9649\n",
      "Epoch: 1/15, Average loss: 270.8257\n",
      "Epoch: 2/15, Average loss: 221.7338\n",
      "Epoch: 3/15, Average loss: 193.3308\n",
      "Epoch: 4/15, Average loss: 172.9608\n",
      "Epoch: 5/15, Average loss: 158.3673\n",
      "Epoch: 6/15, Average loss: 148.0558\n",
      "Epoch: 7/15, Average loss: 141.0201\n",
      "Epoch: 8/15, Average loss: 135.9111\n",
      "Epoch: 9/15, Average loss: 131.9691\n",
      "Epoch: 10/15, Average loss: 129.0454\n",
      "Epoch: 11/15, Average loss: 126.8269\n",
      "Epoch: 12/15, Average loss: 124.8382\n",
      "Epoch: 13/15, Average loss: 123.3273\n",
      "Epoch: 14/15, Average loss: 121.9604\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 402.9144\n",
      "Epoch: 1/15, Average loss: 278.1830\n",
      "Epoch: 2/15, Average loss: 229.3540\n",
      "Epoch: 3/15, Average loss: 201.5456\n",
      "Epoch: 4/15, Average loss: 181.0566\n",
      "Epoch: 5/15, Average loss: 165.8562\n",
      "Epoch: 6/15, Average loss: 155.0658\n",
      "Epoch: 7/15, Average loss: 147.2022\n",
      "Epoch: 8/15, Average loss: 141.7347\n",
      "Epoch: 9/15, Average loss: 137.6772\n",
      "Epoch: 10/15, Average loss: 134.4249\n",
      "Epoch: 11/15, Average loss: 132.2274\n",
      "Epoch: 12/15, Average loss: 130.1863\n",
      "Epoch: 13/15, Average loss: 128.6803\n",
      "Epoch: 14/15, Average loss: 127.5914\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 399.4419\n",
      "Epoch: 1/15, Average loss: 279.3267\n",
      "Epoch: 2/15, Average loss: 231.7823\n",
      "Epoch: 3/15, Average loss: 205.5778\n",
      "Epoch: 4/15, Average loss: 184.9356\n",
      "Epoch: 5/15, Average loss: 169.8275\n",
      "Epoch: 6/15, Average loss: 158.6164\n",
      "Epoch: 7/15, Average loss: 150.7300\n",
      "Epoch: 8/15, Average loss: 144.6163\n",
      "Epoch: 9/15, Average loss: 140.0995\n",
      "Epoch: 10/15, Average loss: 137.0610\n",
      "Epoch: 11/15, Average loss: 134.2746\n",
      "Epoch: 12/15, Average loss: 132.3369\n",
      "Epoch: 13/15, Average loss: 130.9848\n",
      "Epoch: 14/15, Average loss: 129.7188\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bc2f",
   "metadata": {},
   "source": [
    "## train over expandedHFM 16-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM\n",
    "val_loader = val_loader_expandedHFM\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b4f7f",
   "metadata": {},
   "source": [
    "### latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 369.4557\n",
      "Epoch: 1/20, Average loss: 253.0922\n",
      "Epoch: 2/20, Average loss: 202.8551\n",
      "Epoch: 3/20, Average loss: 172.7191\n",
      "Epoch: 4/20, Average loss: 152.2356\n",
      "Epoch: 5/20, Average loss: 137.6990\n",
      "Epoch: 6/20, Average loss: 126.6692\n",
      "Epoch: 7/20, Average loss: 118.0560\n",
      "Epoch: 8/20, Average loss: 111.1311\n",
      "Epoch: 9/20, Average loss: 105.9597\n",
      "Epoch: 10/20, Average loss: 101.8351\n",
      "Epoch: 11/20, Average loss: 98.4729\n",
      "Epoch: 12/20, Average loss: 95.4016\n",
      "Epoch: 13/20, Average loss: 92.9106\n",
      "Epoch: 14/20, Average loss: 90.9402\n",
      "Epoch: 15/20, Average loss: 89.1270\n",
      "Epoch: 16/20, Average loss: 87.6784\n",
      "Epoch: 17/20, Average loss: 86.5530\n",
      "Epoch: 18/20, Average loss: 85.3882\n",
      "Epoch: 19/20, Average loss: 84.4254\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e5b3",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 2, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b165db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 393.1789\n",
      "Epoch: 1/20, Average loss: 300.7643\n",
      "Epoch: 2/20, Average loss: 241.3813\n",
      "Epoch: 3/20, Average loss: 207.7646\n",
      "Epoch: 4/20, Average loss: 179.9620\n",
      "Epoch: 5/20, Average loss: 160.1453\n",
      "Epoch: 6/20, Average loss: 144.9132\n",
      "Epoch: 7/20, Average loss: 133.3975\n",
      "Epoch: 8/20, Average loss: 125.1795\n",
      "Epoch: 9/20, Average loss: 118.3757\n",
      "Epoch: 10/20, Average loss: 112.1573\n",
      "Epoch: 11/20, Average loss: 107.3208\n",
      "Epoch: 12/20, Average loss: 103.3839\n",
      "Epoch: 13/20, Average loss: 99.9120\n",
      "Epoch: 14/20, Average loss: 96.7396\n",
      "Epoch: 15/20, Average loss: 94.4459\n",
      "Epoch: 16/20, Average loss: 92.6706\n",
      "Epoch: 17/20, Average loss: 90.7141\n",
      "Epoch: 18/20, Average loss: 88.7004\n",
      "Epoch: 19/20, Average loss: 87.0102\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1eed",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 3, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 399.2147\n",
      "Epoch: 1/20, Average loss: 324.6712\n",
      "Epoch: 2/20, Average loss: 275.4050\n",
      "Epoch: 3/20, Average loss: 239.8818\n",
      "Epoch: 4/20, Average loss: 216.3649\n",
      "Epoch: 5/20, Average loss: 198.0273\n",
      "Epoch: 6/20, Average loss: 182.2649\n",
      "Epoch: 7/20, Average loss: 170.8574\n",
      "Epoch: 8/20, Average loss: 162.9704\n",
      "Epoch: 9/20, Average loss: 154.4810\n",
      "Epoch: 10/20, Average loss: 146.7258\n",
      "Epoch: 11/20, Average loss: 139.4719\n",
      "Epoch: 12/20, Average loss: 134.2167\n",
      "Epoch: 13/20, Average loss: 129.6427\n",
      "Epoch: 14/20, Average loss: 125.4191\n",
      "Epoch: 15/20, Average loss: 121.2396\n",
      "Epoch: 16/20, Average loss: 118.2277\n",
      "Epoch: 17/20, Average loss: 115.6689\n",
      "Epoch: 18/20, Average loss: 112.8884\n",
      "Epoch: 19/20, Average loss: 111.4160\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde52c1d",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 4, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47314339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 405.6318\n",
      "Epoch: 1/20, Average loss: 331.2717\n",
      "Epoch: 2/20, Average loss: 293.0868\n",
      "Epoch: 3/20, Average loss: 260.4425\n",
      "Epoch: 4/20, Average loss: 235.3888\n",
      "Epoch: 5/20, Average loss: 217.5635\n",
      "Epoch: 6/20, Average loss: 207.8059\n",
      "Epoch: 7/20, Average loss: 200.4518\n",
      "Epoch: 8/20, Average loss: 194.3792\n",
      "Epoch: 9/20, Average loss: 189.3980\n",
      "Epoch: 10/20, Average loss: 184.9584\n",
      "Epoch: 11/20, Average loss: 179.7132\n",
      "Epoch: 12/20, Average loss: 174.9633\n",
      "Epoch: 13/20, Average loss: 170.7941\n",
      "Epoch: 14/20, Average loss: 167.3850\n",
      "Epoch: 15/20, Average loss: 164.2246\n",
      "Epoch: 16/20, Average loss: 161.3431\n",
      "Epoch: 17/20, Average loss: 159.5106\n",
      "Epoch: 18/20, Average loss: 156.5435\n",
      "Epoch: 19/20, Average loss: 154.0239\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26814b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.6061\n",
      "Epoch: 1/20, Average loss: 368.8077\n",
      "Epoch: 2/20, Average loss: 357.9416\n",
      "Epoch: 3/20, Average loss: 353.8225\n",
      "Epoch: 4/20, Average loss: 351.9900\n",
      "Epoch: 5/20, Average loss: 351.0825\n",
      "Epoch: 6/20, Average loss: 350.5930\n",
      "Epoch: 7/20, Average loss: 350.3083\n",
      "Epoch: 8/20, Average loss: 350.1508\n",
      "Epoch: 9/20, Average loss: 350.0325\n",
      "Epoch: 10/20, Average loss: 349.9596\n",
      "Epoch: 11/20, Average loss: 349.9249\n",
      "Epoch: 12/20, Average loss: 349.8968\n",
      "Epoch: 13/20, Average loss: 349.8726\n",
      "Epoch: 14/20, Average loss: 349.8558\n",
      "Epoch: 15/20, Average loss: 349.8398\n",
      "Epoch: 16/20, Average loss: 349.8369\n",
      "Epoch: 17/20, Average loss: 349.8276\n",
      "Epoch: 18/20, Average loss: 349.8294\n",
      "Epoch: 19/20, Average loss: 349.8175\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 5 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.4123\n",
      "Epoch: 1/20, Average loss: 369.0100\n",
      "Epoch: 2/20, Average loss: 357.9196\n",
      "Epoch: 3/20, Average loss: 353.9314\n",
      "Epoch: 4/20, Average loss: 351.9628\n",
      "Epoch: 5/20, Average loss: 351.0653\n",
      "Epoch: 6/20, Average loss: 350.5855\n",
      "Epoch: 7/20, Average loss: 350.3135\n",
      "Epoch: 8/20, Average loss: 350.1441\n",
      "Epoch: 9/20, Average loss: 350.0360\n",
      "Epoch: 10/20, Average loss: 349.9780\n",
      "Epoch: 11/20, Average loss: 349.9259\n",
      "Epoch: 12/20, Average loss: 349.9004\n",
      "Epoch: 13/20, Average loss: 349.8688\n",
      "Epoch: 14/20, Average loss: 349.8592\n",
      "Epoch: 15/20, Average loss: 349.8426\n",
      "Epoch: 16/20, Average loss: 349.8388\n",
      "Epoch: 17/20, Average loss: 349.8324\n",
      "Epoch: 18/20, Average loss: 349.8224\n",
      "Epoch: 19/20, Average loss: 349.8243\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 6 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
