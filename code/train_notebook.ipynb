{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d041b96",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import VAE_priorCategorical, VAE_priorHFM\n",
    "import metadata as md\n",
    "from train import train\n",
    "from datasets import Dataset_HFM, Dataset_pureHFM, MNISTDigit2Dataset\n",
    "from utilities import sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58f0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(md.seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(md.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1647",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe9c74",
   "metadata": {},
   "source": [
    "## train over ExtendedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db29123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eef56",
   "metadata": {},
   "source": [
    "## train over 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a3bba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=32, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=32, shuffle=True)\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09921586",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f7171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0bae",
   "metadata": {},
   "source": [
    "## train over FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2094def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682e2",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2822c7",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db343",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9adabe",
   "metadata": {},
   "source": [
    "# Prior Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98cd69",
   "metadata": {},
   "source": [
    "## train over Extended_MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c091a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6ed9f",
   "metadata": {},
   "source": [
    "\n",
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 354.4975\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53143931",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a349d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 349.8097\n",
      "Epoch: 1/15, Average loss: 252.5678\n",
      "Epoch: 2/15, Average loss: 220.1362\n",
      "Epoch: 3/15, Average loss: 203.2145\n",
      "Epoch: 4/15, Average loss: 193.3282\n",
      "Epoch: 5/15, Average loss: 186.8372\n",
      "Epoch: 6/15, Average loss: 182.4854\n",
      "Epoch: 7/15, Average loss: 179.3084\n",
      "Epoch: 8/15, Average loss: 177.0011\n",
      "Epoch: 9/15, Average loss: 175.3268\n",
      "Epoch: 10/15, Average loss: 173.9250\n",
      "Epoch: 11/15, Average loss: 172.8551\n",
      "Epoch: 12/15, Average loss: 171.9738\n",
      "Epoch: 13/15, Average loss: 171.2925\n",
      "Epoch: 14/15, Average loss: 170.7406\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 350.6189\n",
      "Epoch: 1/15, Average loss: 250.9468\n",
      "Epoch: 2/15, Average loss: 217.6095\n",
      "Epoch: 3/15, Average loss: 200.6092\n",
      "Epoch: 4/15, Average loss: 190.5887\n",
      "Epoch: 5/15, Average loss: 183.7372\n",
      "Epoch: 6/15, Average loss: 178.4993\n",
      "Epoch: 7/15, Average loss: 174.6541\n",
      "Epoch: 8/15, Average loss: 171.8537\n",
      "Epoch: 9/15, Average loss: 169.7876\n",
      "Epoch: 10/15, Average loss: 167.9942\n",
      "Epoch: 11/15, Average loss: 166.7761\n",
      "Epoch: 12/15, Average loss: 165.5641\n",
      "Epoch: 13/15, Average loss: 164.6842\n",
      "Epoch: 14/15, Average loss: 163.9550\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 354.0489\n",
      "Epoch: 1/15, Average loss: 252.5556\n",
      "Epoch: 2/15, Average loss: 217.8604\n",
      "Epoch: 3/15, Average loss: 200.3722\n",
      "Epoch: 4/15, Average loss: 190.7886\n",
      "Epoch: 5/15, Average loss: 184.6503\n",
      "Epoch: 6/15, Average loss: 180.4723\n",
      "Epoch: 7/15, Average loss: 177.2928\n",
      "Epoch: 8/15, Average loss: 174.8085\n",
      "Epoch: 9/15, Average loss: 172.9309\n",
      "Epoch: 10/15, Average loss: 171.3527\n",
      "Epoch: 11/15, Average loss: 169.9989\n",
      "Epoch: 12/15, Average loss: 168.9262\n",
      "Epoch: 13/15, Average loss: 167.9097\n",
      "Epoch: 14/15, Average loss: 167.0546\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.5319\n",
      "Epoch: 1/15, Average loss: 257.6598\n",
      "Epoch: 2/15, Average loss: 222.6616\n",
      "Epoch: 3/15, Average loss: 205.1415\n",
      "Epoch: 4/15, Average loss: 195.8920\n",
      "Epoch: 5/15, Average loss: 190.0039\n",
      "Epoch: 6/15, Average loss: 185.8355\n",
      "Epoch: 7/15, Average loss: 182.7668\n",
      "Epoch: 8/15, Average loss: 180.3982\n",
      "Epoch: 9/15, Average loss: 178.4741\n",
      "Epoch: 10/15, Average loss: 176.9525\n",
      "Epoch: 11/15, Average loss: 175.7419\n",
      "Epoch: 12/15, Average loss: 174.6741\n",
      "Epoch: 13/15, Average loss: 173.7510\n",
      "Epoch: 14/15, Average loss: 172.8685\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.8176\n",
      "Epoch: 1/15, Average loss: 262.1270\n",
      "Epoch: 2/15, Average loss: 228.4061\n",
      "Epoch: 3/15, Average loss: 210.9515\n",
      "Epoch: 4/15, Average loss: 201.2823\n",
      "Epoch: 5/15, Average loss: 195.2321\n",
      "Epoch: 6/15, Average loss: 191.0820\n",
      "Epoch: 7/15, Average loss: 187.9966\n",
      "Epoch: 8/15, Average loss: 185.8052\n",
      "Epoch: 9/15, Average loss: 183.8282\n",
      "Epoch: 10/15, Average loss: 182.1945\n",
      "Epoch: 11/15, Average loss: 180.5830\n",
      "Epoch: 12/15, Average loss: 179.3044\n",
      "Epoch: 13/15, Average loss: 178.0446\n",
      "Epoch: 14/15, Average loss: 176.7816\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 363.8745\n",
      "Epoch: 1/15, Average loss: 265.8502\n",
      "Epoch: 2/15, Average loss: 234.5291\n",
      "Epoch: 3/15, Average loss: 215.7438\n",
      "Epoch: 4/15, Average loss: 205.0702\n",
      "Epoch: 5/15, Average loss: 198.5530\n",
      "Epoch: 6/15, Average loss: 194.0660\n",
      "Epoch: 7/15, Average loss: 190.3498\n",
      "Epoch: 8/15, Average loss: 187.6208\n",
      "Epoch: 9/15, Average loss: 185.2275\n",
      "Epoch: 10/15, Average loss: 183.3724\n",
      "Epoch: 11/15, Average loss: 181.8250\n",
      "Epoch: 12/15, Average loss: 180.4728\n",
      "Epoch: 13/15, Average loss: 179.2218\n",
      "Epoch: 14/15, Average loss: 178.1701\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 378.2936\n",
      "Epoch: 1/15, Average loss: 293.8599\n",
      "Epoch: 2/15, Average loss: 283.2834\n",
      "Epoch: 3/15, Average loss: 280.1153\n",
      "Epoch: 4/15, Average loss: 278.8265\n",
      "Epoch: 5/15, Average loss: 278.2186\n",
      "Epoch: 6/15, Average loss: 277.9094\n",
      "Epoch: 7/15, Average loss: 277.7441\n",
      "Epoch: 8/15, Average loss: 277.6542\n",
      "Epoch: 9/15, Average loss: 277.6033\n",
      "Epoch: 10/15, Average loss: 277.5740\n",
      "Epoch: 11/15, Average loss: 277.5544\n",
      "Epoch: 12/15, Average loss: 277.5417\n",
      "Epoch: 13/15, Average loss: 277.5356\n",
      "Epoch: 14/15, Average loss: 277.5299\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad702",
   "metadata": {},
   "source": [
    "## train over 2Digit_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bd9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e7b7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 304.0161\n",
      "Epoch: 1/15, Average loss: 218.2176\n",
      "Epoch: 2/15, Average loss: 200.2183\n",
      "Epoch: 3/15, Average loss: 193.5813\n",
      "Epoch: 4/15, Average loss: 190.6368\n",
      "Epoch: 5/15, Average loss: 189.0655\n",
      "Epoch: 6/15, Average loss: 187.8614\n",
      "Epoch: 7/15, Average loss: 187.0031\n",
      "Epoch: 8/15, Average loss: 186.1542\n",
      "Epoch: 9/15, Average loss: 185.7021\n",
      "Epoch: 10/15, Average loss: 185.1782\n",
      "Epoch: 11/15, Average loss: 184.6376\n",
      "Epoch: 12/15, Average loss: 184.3533\n",
      "Epoch: 13/15, Average loss: 183.8985\n",
      "Epoch: 14/15, Average loss: 183.5778\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld4_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=4, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld4_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc8eae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 252.4005\n",
      "Epoch: 1/15, Average loss: 250.1789\n",
      "Epoch: 2/15, Average loss: 250.0696\n",
      "Epoch: 3/15, Average loss: 250.0042\n",
      "Epoch: 4/15, Average loss: 249.9637\n",
      "Epoch: 5/15, Average loss: 249.9323\n",
      "Epoch: 6/15, Average loss: 249.9188\n",
      "Epoch: 7/15, Average loss: 233.9203\n",
      "Epoch: 8/15, Average loss: 212.0527\n",
      "Epoch: 9/15, Average loss: 205.7089\n",
      "Epoch: 10/15, Average loss: 200.1853\n",
      "Epoch: 11/15, Average loss: 195.8478\n",
      "Epoch: 12/15, Average loss: 193.2903\n",
      "Epoch: 13/15, Average loss: 191.1994\n",
      "Epoch: 14/15, Average loss: 189.7921\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld4_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=4, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=False).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld4_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a84eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8MAAAGNCAYAAAA1uKMDAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAARo5JREFUeJzt3QmQXWlZP+CbPenOvu+ZJDMTmC0zwzYI/llFQdlBRGQTQaREVBTFDbGwsBDBDbWsUkCRQmRRFEFEdkGWWTN7Mtn3zt7ZMzP9r+9WZTIz9317zplOd7r7PE9VauDLl3PPPef7zjnvPff87pi+vr6+FgAAADTI2Au9AgAAADDUFMMAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEUwwAN87Wvfa31xje+sXXZZZe1Zs2a1ZowYUJrzpw5rSc+8YmtX/zFX2x9+ctfbvX19V3o1RyRXve617XGjBnT+shHPnKhV2VUsn0BOJ/Gn9elATBs7du3r/WqV72q9aUvfan9/5csWdJ6ylOe0poxY0br8OHDrVtvvbX1oQ99qP3nmmuuad1www0XepUBAAaNYhigAQ4dOtR66lOf2rrrrrtaj3nMY1p/9Vd/1XrGM57R0a8UxB/84Adbn/jEJy7IegIADBXFMEADvPWtb20XwqtWrWp9+9vfbn89OnLFFVe0/u7v/q718z//80O+jgAAQ8kzwwCj3D333NP6+Mc/3v7f5a5vVgg/WHl++OGe/vSnt5/XLM8cf/Ob32w9//nPb82bN681duzYhzzDeeLEidaf/MmftK677rrWzJkzW5MnT26tWbOm9Y53vKO1f//+hyzzXe96V3uZ/RXf3/ve99p9yte677333gfay7PNZR0WLFjQfu65vK9LLrmk9TM/8zOtb3zjG+GyvvKVr7Re/vKXt5YuXdqaNGlSe/2f8IQntNfjwet25syZ1sc+9rH218rLnfTp06e3pkyZ0n4fv/RLv9TauXNn69G4/vrr28tcvnx5+/Vnz57d+tEf/dHWf/7nf9ZeVvlq++/8zu+0rrzyylZ3d3d7eYsXL25/9f33fu/32u/hwcr2Kh+KXH311a25c+e2+5ft8IpXvKL1/e9/P3yN3//9329v+/Lf8p5/7ud+rv0aZVuc/eDkrDvvvLP10z/9062FCxe29/natWtb//zP/xwu96KLLmovd/Pmza3Pfvaz7W8tlG08bdq09jh7NNvj0WzfXbt2td72tre1Lr300vY6d3V1tZYtW9Z61rOe1Xr/+9//qNYBgBGkD4BR7U//9E9LGlbfrFmz+u67775HvZynPe1p7eW85S1v6Rs7dmzfZZdd1vdTP/VTfc95znP6Pv7xj7f77Nixo+/KK69s95s9e3bfs5/97L4Xv/jFfStWrGi3XXTRRX2bN29+YJm7du3qmzhxYl93d3ffwYMHw9d9zWte0/637373ux9o+8hHPtI3ZsyY9p8nPelJfa94xSv6XvCCF/Rde+21fePGjet729ve1rGct771re3llD9XX311e92f+9zn9q1atard9tWvfvWBvtu2bWu3zZgxo++6667re/nLX973vOc9r2/x4sXt9nnz5vWtX7++4zVe+9rXtv/+wx/+cLgfynY7+/ove9nL+p761Ke23//D398jOXbsWN8VV1zxwLo8//nPb7+fpz/96X0LFy5stz98e65evbr9Wtdcc017W73kJS9p78PSd/z48X2f+tSnOl7nXe96V/vvX//617eXu3z58r6f/Mmf7HvGM57R3s7l797//vf3fec73+mbNm1a35o1a9rr8eQnP/mBbf2JT3yiY7lnx8Ov/MqvtP/7+Mc/vu+Vr3xl3xOf+MQH/t2f//mfD+r2LWPv7P4s7+uFL3xhexz98A//cHvsln0PwOimGAYY5V796le3L/if9axnDWg5Z4vh8udDH/pQx9/ff//9fU95ylPaf/+GN7yh78iRIw/83ZkzZ/re/va3t/+uFFIP9qpXvard/oEPfKBjmT09PX2TJk3qmzBhQrt4OWvlypXtf/PNb36z49/s2bOn74YbbnhIWymsSv85c+b0feUrX+n4N9/97nf7tm7d+sD/L+v+b//2b32nTp16SL/Tp0/3vfOd72wvqxTHVYu1L37xi+3Cfe7cuX1f//rXH/J3t9xyS9/SpUvb/+5rX/taXxUf/ehH2/1LMV/W6cHKBx5lOQ9f989+9rN9Bw4c6FhWaS/FcNk2x48fD4vh8ufNb35zez+e9bnPfa7dXorgUty+5z3vaY+Bh38Ic/HFF6fFcNkmH/vYxx7yd6V4Lu1lndatWzdo27cUx6XtTW9600PWuyjb9Mtf/nLHegMwuiiGAUa5UjCVi/5yxy5y0003tYuMh/95eKF5thh+5jOfGS7nC1/4wgN35R5cND24SDt7N/PBRc73vve9dtsll1zSUZS8973vbf9duWv4YF1dXZXv3JV1KXdPy3I+/elP950P5Y5iuQv54IK/v2Kt3L0u7dHd1+KTn/xk++9f+tKXVnr9973vfekHCI9G2b5leZ///OfDYrjcOT1x4kTHv7vqqqvaf1/u6D5835XtXu6wlr/fsmVLWAy/6EUvCtenbIfy92984xsHbfuWbziUts985jPpdgFgdBOgBdBw27Zta330ox/taC/PbpZnOR/uZS97Wbicz3/+8+3/vvSlL22NH995einPFv+///f/2onVJcSrPHNalGd2n/zkJ7e+853vtP7rv/6r9WM/9mPt9vvvv7/1N3/zN+3/XX7/+OHPNJdnl1/zmte0n/ksPwVVlp89R9rT09N+TvbFL35xq46bb7659T//8z+tTZs2tY4dO9Zep6I8u1z+94YNG9qv/Ug/aVWeey7P2ZZnnCNlWxdlu1RRtlnxvve9r/0b0T/xEz/Rfj72kZTnfst+Ks/3lmeOzz6Dfdttt7X/W0LWnve853X8u5I8Xp6pfbjyjPYtt9zSeu5zn9t+BvjByhgozwYfOHCg/brlOd6He+1rXxuuZ2n/9Kc/3d7Hj+TRbt8yhkqq+m/+5m+2f1f7Oc95Tmvq1KmP+HoAjB6KYYBRrhSBRSkII6WQKsXAWc9+9rPbBWCmFDiRjRs3tv/7u7/7u+0//Xn4upRQqlIM/+Vf/uUDxfB//Md/tLZs2dIuNn/oh37oIf1LEVPW+x//8R/bf0rwUikQn/nMZ7Ze/epXP6TwKssoSvjVwwu2TCl8y3JKuFN/jhw58ojLKoV02b4lWKyEOvUn20dRcfcbv/EbrT/+4z9uF47lfZXCtIRnvfCFL2wXhQ//cODd73536w//8A87grWqvJ+okC3OFo/Z35f9Upw8eTL8+5UrV/bbvn379tZgbd+yf//7v/+79U//9E/tD3DGjRvXuuyyy9ofAJUPfMpYAmB0UwwDjHLXXnttu2C84YYb2nczszuoVZU7cJGzd01LMbF69ep+l3H55Zc/5P+X4uPXfu3XWl/4whfaxU0phj70oQ+Fd4WLxz72se27mF/60pfaCdHljl9JuC7/+w/+4A/aKcclVfrReuc739kuhEuS9B/90R+1C+3yocLEiRPbf1+K81K8P/hDhMzZ7VIKx1J0nS9lvd785je3/v3f/731rW99q/W///u/rQ9/+MPtP2V9v/rVr7ZTpovPfOYz7UTosg7lA4dS6J1NhS6F9G/91m+13vve96bv55HGzEDHVGYwt29Z55IYXt57uVtetl/589d//dftP+UDhTIGSpEMwOikGAYY5cod1Le//e2tgwcPtn9ipvz/wVB+kqYodyZLYVtH+UrtL/zCL7R/Kqjc9X3jG9/YvmtXvvr7yle+Mv035Su9Z7/WW+5qfuADH2jfAS0/1VS+El2KwbN3Le++++52cVXl7vAnP/nJ9n/LTwNdddVVHX+/fv362tulvO7f//3fn9fCsdylLz+XVP4U5SeSyocA5b/lK9RlWzz4/ZQ7w29605sG9H7Op/LBR/kJpocrP7lUlJ9+GuztW+4Glz+//uu/3h4f5QOV8hNR5UOGf/iHf2i9/vWvr7U8AEYOvzMMMMpdfPHF7d+SLX71V3+1/azoYCjPjRb/8i//UumO3sOVArY8l1oKmvI7xWUZb3jDG9I70Q9Xfqe23P0sv218/PjxdvFbPP7xj2/f1S1fkf3Xf/3XSssqz7kWK1as6Pi78lxzeU61qnIHthTUvb29rS9+8YutwVTuCL/lLW9p/++bbrqp0vvZu3dv+4OHC6F8YyFSitAHP+s7VNu3FNTlN4ZLMfzwbQjA6KMYBmiA8pXjUhSXO4DlK75f//rXw37ljlyV5zQj5Y5wKcZKmFG5mxY9/1ruTpdQrLPBTQ9WCtZShJTC7W//9m/bd/jOFnYPVgrdcgc4Wn75qvShQ4faX209e1ex3EH+7d/+7fb/LndFv/GNb3T8u3In9cHvu3wNu/iLv/iLh/QrX80uX02u6z3veU/7v2W7lDuOD1cK/+9+97vtr31XUb6+W97H2a8In1WeBz5bED648D37fsp2PX369APt5YOR8szxYH1AUuV9fOITn3hI26c+9al2eFbZb2fveA/G9i0FdwlXe7hSVJ8N7oo+PABg9PA1aYAGmDVrVvt5yFJslnCscsetFItXX311+05qCR8qhfK6devahcOVV17ZvqNaRyley53XH//xH2+nU5eipnwFtnxNuRRgJWCrLP++++5rve51rwsTp0uQVrkzXJTlRGFdZVnla9/la61lPUtw1IQJE9qF/P/93/+1+5Tid968eQ/8m5I4XQrZUog/7WlPa4dylUCt8tXqkqxc1q08Y3u2gH7Xu97Vfo65BIGVrxiXZ5zLHdRSbP/wD/9w+25k1eTnojx/+md/9mft9X7BC17Q/mCivP6MGTPaRX1JrS7LL6FYJdX4kZQPM8ryygcI5b3Mnz+/XcSV91+Ws2TJktY73vGOB/r/8i//crv4K1+TX7VqVeu6665rF85lOV1dXa2f/dmffWC7D6WyX8rX4MuHG2U/3nPPPe2itXj/+98ffkX9fG3f8hx1+SCg7MsyD8ocKR/WlHlSPhwoaefl6/oAjF6KYYCGKAXTl7/85XYx/PGPf7x90V/uLpY7rSX1t4RWlTunZ5N0H82zraWwKAXZRz7ykfbztuVnd8qd4vLsb/m7cle1FCvRz/QUpXheuHBha/fu3WFwVlGCkkpRWwq5G2+8sf0V31Igl+W/5CUvad9NfngScPn6awlFKnevy78t61h+4ql8EFDedymKHlx4leWU5ZdnbkshVYq0UkSWr2GX56GrFKxRoV/Wq9xtLoV32Q9lG5f3WwraUvxXDYAqHyaUr4+X4Kzbb7+9va6l8CsfPJTCt+zH8pNLZ5X3WLZVeSa7FPQlqbu8bilEy3sq2+ZCKMVw+abCBz/4wdbnPve59gcx5cOGUsjXfba97vYthXPZLuVDjRIuV76RUMZpeX64fGhU7jKfDSADYHQaU35s+EKvBAAUpVj/kR/5kfZdvTvuuKPyTyExspQ7/uUnr0qAVvZTXQAw2DwzDMCwUL4+Xb6efDboSyEMAAwmX5MG4IIqv4tbvq79gx/8oP3V5fIccHmGFQBgMLkzDMAFVZ53Lc8YlzTn8tvA5XnWKFwLAOB88swwAAAAjePOMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEUwwAAADSOYhgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOMohgEAAGgcxTAAAACNoxgGAACgcRTDAAAANI5iGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaBzFMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEUwwAAADSOYhgAAIDGUQwDAADQOOOrdhwzZszgrgk8SF9fX2u4MycGf3uOGzcu7DthwoSOtvvuuy/sO3Zs9c/8zpw5E7Znyx5K5gQ8lDkBD2VONI9rp4HPCXeGAQAAaBzFMAAAAI2jGAYAAKBxFMMAAAA0TuUALYDzEZKRhTJE7ZMmTQr7Ru3ZcidOnFg57OHkyZNh+4kTJzra7r///mEbGAHQdNE5aCQETNFsrp2GnjvDAAAANI5iGAAAgMZRDAMAANA4imEAAAAaR4AWjMKghfPVfyiNHz++cgjEtGnTKvft7u7uaDt16lTYt7e3t/K6HTt2LOwrQAug2vknOydl7ePGjRvQOmThPVGwVha2Vbed4ce10yP37W7QtZM7wwAAADSOYhgAAIDGUQwDAADQOIphAAAAGkcxDAAAQONIkx5G6qYqDjTNUPLh8JTt77Fjx1ZqezTtg5GumfXN1mHixIkdbV1dXWHfqVOndrTNmDGjcnrimTNnam37qH+Wqnj69OmONnNtcAx1wqf9COf3WiZLh84SbqP2rG+UTpsdt6NjfJZue++991Z+PceMoePa6RzXTvW4MwwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHFGRYDWaA+eit5Hnfd2//33D3gdhEAMnTohCZMnT67ct5gwYUJH2/jx1Q8DWaBIFIiQBS1kYyl6L93d3WHfmTNndrTNmzcv7Dt9+vRKQQ39rfPRo0crb7dobpo/gzMnon2Q7Zc6ASh1AnKyOVEnGCU7nkf9Bys0zBgdWYY6PO58rEcUlpWdq7JzW50AoGgZvb29Yd8jR45UOu4Xx48fD9uzYwFDw7XTOa6d6nFnGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJxhmyZdJ0G5Tnud1M4sxex8pIRG7VHSYtaeLTdK06uTjJqlT2eJ1BJIBybaX1nK4axZszraFi9eHPZdsmRJ2D5nzpzKrxeNmyxFM0ro7Onpqdw3G0tdXV2VExGjtuw9RwmOxYkTJ8L2w4cPd7QdOnQo7GtODEx0bMvSJ6Nk2aitv/bo2JYlZkaJs8eOHQv7RsuokzxdN6l3oOe7uozzC3ueyJJzo+uFrO/5SKQe6DjI0nunTJlSeR5nabjR+SNKBc6OA1lC7smTJ8N2vyQwdFw7nePaaeDcGQYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DjjR1IIRBaikgUiZIFUVR94z4KnsjCpOusQPXjf3d0d9o3as9CJOmEP2cP40YPw2cP/UUhMne3TdNGYzsJALr/88o62xz3ucWHfVatWVQ5EmDFjRuWxm42lffv2dbRt2rQp7Ltt27bKQQvZHIwCH7LtFgWuZGM/C3aYNGnSkIYTNVm0XbOgkoULF3a0XXzxxZX7ZsfSKCir2LJlS6W2Yv/+/ZWPo3WOmXXCE+sEJGXjts66GfsDk+2v6BonmxNRe3T86u86os61U3ROyALoovGRrcO0adPC9gULFgwoQCt7b1FYVjZfs2Wcj0AyqnHtdI5rp4FzZxgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOMohgEAAGicQUmTzpLCooS1LEkwSkTM0gWnT59eOYHxvvvuC/tG6YdZkmCUOpilC0ZphsXcuXM72pYuXRr2XblyZaV/n6Wx7dixI+y7efPmsH3r1q0dbdu3b6+cYpel5lFtfKxZsybs+4xnPKOj7dprrw37ZumA0bzK5mA0f7L092j+POYxj6k8vopdu3ZVnoPRemRzLToeSc4dvqL9FaVaZsnRT37yk8O+q1evDtunTJnS0Xbs2LGw7+23397RdtNNN1Xuu3PnzrBvnWNmNgej9jqpt9l4zs6ZUXvWl2qyY3GUWjt79uzKqelREm4xa9asygmw2biLrgGiJPXi6NGjlcdM9nrRtU+WFB+N/+j6LTsO1Pm1k2xeDcfk3NHAtdM5rp0Gzp1hAAAAGkcxDAAAQOMohgEAAGgcxTAAAACNoxgGAACgccYP6YuNH185ITpKT8xS3pYsWVI5ETFLEozSPLNE0Sj9MEuVmzlzZuUU1CuuuCLsGyUlZtsi2sbbtm0L+65bty5sv/HGGytvixMnTnS0nTp1asQkyF1oURL61VdfHfaN0g8vueSSygmFxd69ezva9u3b16oqS/WNUkmzsb927dqw/aKLLupoO3ToUNj33nvvfYQ17X8ZPT09tRIRozEtOXfoZOMuSojOjqPR+MpEx7VsvkbnmeyY2dvbW2s8R2MsS6eNjv3ZeSlLNo1k58xoTpyPpNGmiBKJs2PmokWLKl1DZNdDq1atCvtmv0oRjfNsH0bjfM+ePWHf3bt3d7QdOHCg1jksStFevHhx5eu6KNG6v4ToOqJ55TwxOFw7nePaaeDcGQYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DiDEqCVBXxEYR7ZQ+VRIMKKFStqBWhFD9hn6xYFLWQhKlFIQvQQfBb2kK3zggULwr7RNspCW6J1u/TSS8O+WbjK8ePHO9rWr18f9o0eps+2cZMDtLJtMnny5MphJ1FgShZAd9ttt4Xt3/72tzvabr311srBKFOmTKkc4HDllVeGfR/72MeG7VEo3LJlyyqHXGThRFFYQ7Td+wuBiEInsvEc7esmj/26om2VHauiY+bSpUsrhzJm4yabr9HxPBvPu3bt6mjbsWNH2DcL9akTfhUFaGXjPDp/ZO85C5SJ+mfnzGifNn1ORPsrG6PRmM4CtObPn185KCs7f3R3d7eqqhN2Gp3bolCt7DokW+fsfUShR1u3bq18vqsTNlQ0fUwPBtdO57h2GjzuDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaBzFMAAAAI0zpGnSUXpiV1dX2DdKLMsSDrNUuChBMUuvjtI1J0yYULnvuHHjKr/nLBE0Sy7cvn175XTVKNkx28bZOkfLzl5Pcu75F6WgZ4nlZ86cCftmqbXXX3995UTEI0eOVB7PUdr4zp07w76nTp0K25/ylKdUTkGNxm42Rg8dOlR5G9edKxHj//zLEpSjhM4stTMTjY8s3TkaYzNnzqz8iwFR0m+WepsldGZzMHrf2TiP2rPlZgnR0bY4cOBA5XTVKKm0SepcD0XJsNkvVURzJRvP0djPrkWytNg6adLROi9atKjWdWR0LI5+DSQ7h2Xp6NE2ys5V2Xk32kZ+XWPouHY6x7VTPe4MAwAA0DiKYQAAABpHMQwAAEDjKIYBAABonEEJ0KojC2WI2rPAjSygIArAysK2FixYUPlB8ShA6/jx45UfpM+CRrKH5qP3Ha1D9oD9nDlzwr5ZqEb0XrJwr2z/UU20XbMwgyiIoG7Ax/79+zva9u3bV2t8VA3ZyYITsjm4ePHiyuFEUSBGFLqX9c0CXrL2KBwiC3Xi/MuOM9H4zwKfsnCO6FgczZNs7GZjJhp3UdhQf3M+CvvJ5lU0/rPAyOickAWPZcf+6PWyc3T0PpoeoBVdt2THsGjcZQGfdc4TWeBZdC2SzYlo3KxcuTLse/nll3e0XXLJJZVDw7K5kl1/ReGj2XiuM0az9ugYIyhrcLh2Ose108ANvzUCAACAQaYYBgAAoHEUwwAAADSOYhgAAIDGUQwDAADQOIOSJp2l50VpfVnq2uHDhysnH0ZJ0FkiW5RsVjc1LZIlO2apgydPnqy8jCh1Mkub6+7urrTd+9ueu3btqpyKHSXySU+sLkouzJIxo/1YJ+k1S5fNxlKU4Jul+kbrnKWP7t69u3KC6apVq8K+dcZYlLyeHTMuuuiisP2OO+6onFzJ+ZeN8+hYlY277HgeJVtmiaLROM+Or9G5Jkt3Hj9+fOVjf5bEGc3j7DgQJY1m2yf7tYboXJMltB46dKhy36acP6L9mL33KIU8S5aNrjmOHTsW9s3myubNmzva9u7dW3ncRfs7O3/Mnz8/7JslUkfbIhvnc+fOrXwNGO2POr92UkiTHjqunc5x7TRw7gwDAADQOIphAAAAGkcxDAAAQOMohgEAAGicCx6glQU79PT0dLRNmTIl7JsFMESvl4UnTJ8+vXLYSfTAe52Qkaw9e8A+CsTIHv6PAsmikJnitttuC9s3bNjQ0bZv376wbxYew6MPgdizZ0/l8IRsfGXjfOHChZXC47K5GQW/ZWM0C7PI2nt7eysHsUSvl43FKBAjCwu6+OKLK4dD3HzzzWFfzv/5IxszUdDP9u3bw77ZOJ8zZ07l81IkO25HgYhZUFYWxFJHtB7ZeSkKRom2Q3/bLQrhyoIWo3NQto2bEjgU7Ztsf0Xt2TE+Ch/Ntml2HRHNtyzsNFpG9nrRmMnOd9m5Jnq9aDxnx/koVCsL5oquQ/vbTwwd107nuHYaOHeGAQAAaBzFMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxhnSNOko7fLEiROV0zWjlMT+lhGtR5ZIHfXdu3dv2Ddqz1I0s5TQKGk0asuS6bLX27lzZ0fbnXfeGfa96aabwvb169d3tB08eLByMh2tAaV53nXXXWHfNWvWdLStWLEi7Dtx4sSwPUrSzFJkowTxKMExe39ZUmmWqnjq1KnK4zxadpa0GCWYLlq0KOybJY0uXbq0o23atGlhX86/KC0zSw/dtGlT2Hf16tW1fo0gEqVu1kk/zvpmKdPROTObg1mqdWTs2LEDSuTNfoEhO2fecccdldetKaJzZ7Zv9+/fX3m/RNcL2bExuz6JxmM2vuoct6NzSpbYnKXhRmm/2fkuOkZnx4HoOin7JY4sCT075/HouXY6x7XT4HFnGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJxBSZOuk54YpXNmqWlZwlqW/hYlvWVp0gcOHOho27p1a+W05SjtsT9ZcmfVpLgsXXXHjh0dbRs2bKjcN3sv0TrUTTClVWn8b9y4Mex7yy23VPr3/aV8RymyWYr55MmTK6do1klGzZILo7T4LD0xeh/ZGI22UZbeu3jx4rB9+fLllZIWizFjxgwocbjpom2V7dvouL1ly5awb5aIv2TJko62ZcuWVR5LR48eDfvu3r270tioKzsPRuuWzcFovmbHjCg1uuju7q6cvhslAEdzuEnnlOh9Zr+YEZ2rp06dGvaNjm3ZmMn2QXTsz0TjJpuv0VzJ3nPWHl3vZdeA0TZetWpV2PfSSy+tnEyfHUsGmjZPda6dznHtNHDuDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJwhDdCKHoTOwjKivtnD6nPnzq0cjBKFfhS33nprR9u3vvWtsO+NN95YOUAre/g7eog9e2g+erg96xsFa2UBL9kD/dGymxJqMtSigIIoeCcbo1lIQhYUt2/fvgEFFIwbN65yCEQW2pKNxzrBbVEgTxbsEI3dbDxnwXZRWE22jRmYKEQjCzWJAkV6enrCvlmw1sqVKzva5s2bF/aNzkFHjhwJ+0bhO1nQT51grWzsnjhxoqPt0KFDteZgJBvnCxcurBw8FgXKZOFNo022b6P9mO2XaExnAVrTpk2rfIyPxky2ztnYjc4J2RiNzldZUFYWUhUdo7PXi84Js2bNCvtedtllHW233357rfDR7JqK88+10zmunQauGWcjAAAAeBDFMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxrngadJZGluUvJYlkGUJlsuXL6+csHbnnXdWSo3OEgajFOf+ktfqpDNHyY7Zv4+SoKPUvf4S6+ok5DEwUZJglq4ZpeFmaaBZanqUoJilKkbjo076e5Z4nqXv7tmzp6PtwIEDYd8oETRKrM1Sa6MU4v5SFaPjUZaGG81Xc2pg6hzvskTXKA00S1yuk5g5Y8aMsO/SpUsrnZP6+0WE6FiQnWui+ZodH6JtVDclNHrfWVJvNjebIJv70fbOjsVRWmyW7jx9+vTKx7U61wbZ8S56H9l7juZrdr7LknOj8Z9dy0TrPGfOnMrXkStWrAj73nTTTZVfL9sWzgkD49rpHNdOA+fOMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxhnSAK2qD0xn4RCzZ88O+2YhB9OmTeto2717d9h3+/btHW0bN26sHGaRPfBe5+Hv7KHygYYK1H0AXQDQ0In2VxYcsnfv3soBKNHYz8bpwYMHw75R2FwWVBK9jywwIguBqDMHo/eXBRl1dXVVDlPK1rmnp6dyUBMDEx1rsmPjhAkTKgV29DdXojmRjdHo9bLgkEWLFnW0rV27tvL4ypa9devWsG8UCBOtb7aNsnNxFqAV9Y+OA9n7qBMi2ZRxnh1fozCc7JwchfpkYVtZiFAUkpOFj9Y59kcBR9mxOJuD8+bNqzx2o3NbFsQahQhlYVvZnIjmVTYnGBjXTue4dho4d4YBAABoHMUwAAAAjaMYBgAAoHEUwwAAADSOYhgAAIDGGbZp0lFaX5YmHaUAZomIWfpblNaYJURnaaV13l/UnvWNZMl0kSzNMHs9ydFDJ9rWWcJnlMSZJfhlqYpR/yzZL0pEzMZSnWTY7PV27NjR0XbbbbdVnvPz588P+3Z3d1eew9m2v/322yvtDwZHliYdpeRmCcrZMqKEzp07d1bum6WPRsfXNWvWhH2z8RglUm/YsKHyrxxkx/KVK1d2tC1fvjzsm72/6PyabbfoWNL080x0zMzO69HxPNt+0bbOrpGy14sSqbPjdpTKm50nomVk12TZ60XJ5Fmie53U9Gid6yRP1329po//gXLtdI5rp4FzZxgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOOMqACtqK2/EIgomCF7AD1ajyw8YerUqZXDEOqEuWTbIgryigIusvbsAf06D+4zOKJxk43naOxm4y4LC6oT2hKNu/MRApEF00Uhdrfeemvl0JYVK1ZUDgDK1iELgdi+fXvlEAjBKAMTHQez8Ry11wm3yYKnsvERtWfhPXPnzu1omzlzZtj3oosuCtuXLFnS0bZ3796wb09PT+X5Gs2JxYsXh32z48O2bds62tavXz/gcK+miN5/dhytc16Pjo1R0Fx/4yNaRtSWzYls3aLjazRus1Cg4qqrrupomzNnTuX3l83t7Joqkm3P7DjF+efa6RzXTgNn5gIAANA4imEAAAAaRzEMAABA4yiGAQAAaBzFMAAAAI0zbNOkoxTmLHXtyJEjlRPkstdbsGBB5YS1GTNmVE6gy5JGo/5ZMl2U0halx2WyFLvs9bL+DI06KaFZ3yw5N0rry/Z31J69Xp0UwOz1ojTPPXv2VB67WSpplP6eyZISozTc3t7eystlYLLxFR3Ps2N8loYb7dt9+/aFfY8ePVp5udOnT+9oW758edh32bJlYfvChQs72i655JKwb7TsU6dOhX2jbZSdDzZv3hy2r1u3rqNtw4YNlX/Zoem/ZlAnTbpOOm00HrPrkGyuRGOhznkim69RezT/io0bN1Yej9kxPkqWz86N0XVWNn8yTU9Iv9BcO53j2qked4YBAABoHMUwAAAAjaMYBgAAoHEUwwAAADSOYhgAAIDGueBp0lmSWpSaFiV59pesvGjRokrpgsXKlSsrp79F65H1zRI66/SNkjjrpEBmKW91kxIZfnMi65uNpWjc1Ek5rJMSWle0zlEifLbOx48fD/tOmjSpcmJkNleiZUfpo4VE0YGps/2iMZPtlyy1NjoOZmMpOtdkx9FojK5fv77y+adYunRpR9vs2bPDvl1dXZW3ZTSvsu2zadOmsH3Hjh0dbVu2bKmcdtp0ddJp6xy3o2Vkx7XsVzDqrFud+Rqtc5Ysm6VJ33TTTZWO8cWcOXMqz9e9e/d2tO3evbvWeM6uyxgarp3Oce1UjzvDAAAANI5iGAAAgMZRDAMAANA4imEAAAAa54IHaGUPoEehCvv27asc5FEsX768o23u3Llh39WrV3e0TZ06tXKAVhbKcPr06bA9erg9C2UYP75zNx05cqTyA+/ZQ+l1Hv7nwov2S7YPsyCPge7bwRwb0bKzMIsolCGba1HgQ7Z9spCYqL1OOB4Dk+2vaBxkoTfRcbSYMGFC5bF06NChygFA0bplQT9Z8NSCBQs62ubPnx/2nTZtWuXjQ3SuyQIqs+25c+fOyn2FNVZTJ2RnqMOazsexP1rnbGxs3rw5bI/GdDQvi8WLF1ea71lQ6YYNG8K+2ZyvE9TE0HHtdI5rp5g7wwAAADSOYhgAAIDGUQwDAADQOIphAAAAGkcxDAAAQONc8DTpLGHtxIkTHW27du0K+958881he5R69oQnPCHsO2vWrI62RYsWVV63/fv3h30PHDgQtp85c6ZyImJPT0/lNOkopTpKj+sv0U364ehMHx2JsvcRjd0szTA6DtRNjIzaR8s2Hgmi42WWihylwhZjx8af/U6cOLHScrNjaXYsjlJyswTTPXv2hO133XVX5V856OrqqpxeHY3/bBtn57ZoO0fnxuGaHjrSjcTjT53U22xebdq0qfK105w5cyrPn2gZ2djP1i16LyNxPzWBa6dz7nPt5M4wAAAAzaMYBgAAoHEUwwAAADSOYhgAAIDGueABWlmgSBRUkoWMZA9jR8s+fPhw2HfhwoWVwhcy+/btC9u3bt0atkfvJQvbipad9Y1CTU6fPh32HY4PscOjNdoDMZosC/iIQqqycJssICoK0MpCfaKAqGy50XE3ex+ZaOxm55rx4ztP5+PGjav8Wtm6ZeeP6Pyanc+hrmwO9vb2Vg4Jja6TovmevV62DtmcEBTHSNTn2smdYQAAAJpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHHG9FWMCxszZkzrQpswYULY3tXVFbZPnz69ckL0zJkzKydxRomZUappfymHUfvRo0crpyfWSTPMEj6Hc1LccF634TQnaA5zolXpGD12bPwZb51k5ex9RInLWQpztL/q7sORsM8vpJGwfZwnhte2z/ZHnf00nK+phsM6PBJzguE2J9wZBgAAoHEUwwAAADSOYhgAAIDGUQwDAADQOCMqQOt8yMJVxo8fP6AArbqBClF7tow6yx0tRsL7Gy1zgpHBnBie6zwS9stoNRK2/UicE4xc5gQ8lAAtAAAACCiGAQAAaBzFMAAAAI2jGAYAAKBxFMMAAAA0TuU0aQAAABgt3BkGAACgcRTDAAAANI5iGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaBzFMAAAAI0zvmrHMWPGDO6awIP09fW1hjtzgqFkTsBDmRPwUObE6FVnu2XjIFpG3wgYMwNR5f25MwwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEqB2gBAAA0XRZoNXbs2Mp9x4/vLMMmTZoU9p0yZUrYPnny5I62rq6uyssYk6zb0aNHO9oOHDhQuW9x8uTJjrb777+/Ndy4MwwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEEaAGNkwVGZPr6+iovI+oLw505QVPG7kBFAUnZOK8z9s2TkRWKFYVfZSFVc+fODftG7YsWLQr7LliwIGyfM2dOR9vChQsrh2319vaGfffu3dvRdtddd4V9169fH7Zv3ry5o+348eNh3wsZrOXOMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DgjKk16MBMDo2WPxBTAOttouKwzj37fno8E2PPRdziLtlGWBno+0nCjRMTRsi2HG3Pi0TEnGO7XZdlyo/Ys1Tdrr9p33LhxrTruvffeSm1Z+5kzZ8K+5srQnSey42CUwjxjxozK6c5LliwJ+65YsaKjbfXq1WHfrH358uUdbV1dXWHf6H2fPn067Hvs2LGOtnXr1rXqOHr0aEfbzp07w77ZegwFd4YBAABoHMUwAAAAjaMYBgAAoHEUwwAAADTO+NEc8DEcwraGQ/BB3XUYDuvM4IRARPu2ztgdzuO8TuBKFoyStdfZ9qdOnaocosLAmBP9MyeoOw6yMKk6IVXnY8zUCZPKXi9at0mTJoV9p0yZUik0qT/RmD558mTlcKLe3t6wb7YMHv04z/pOmDAhbI/GQnd3d9h3+vTpHW3Lli0L+65ataqjbeHChZWXWxw/frzyWIrmz9SpUysHgV166aVh361bt4btO3bs6Gg7ePBg2FeAFgAAAAwhxTAAAACNoxgGAACgcRTDAAAANI5iGAAAgMa54GnSWaJblA5YJ80wW8bEiRPDvlF7liQYpQ5myaFRimaWOpglqUVJgtly77vvvsrrFvUt7r///hGVmNoU0VzJkg+zcR4tI9rfWZpnlgA71GNmoCnCWaJolqoYHUuybRG9v2wbZ+1UY06cY05QNyk8S8OdNm1aR9vcuXMrj4/s2im65siuZaK05SwlN7uWicZdV1dX2HfmzJmV03uzY0l0fDh06FDYd//+/ZW3RXRtaJ4MTPbrAll7dF7Jxnk0lrK5Fi03Sw9fv3592H706NFKCdPZshcvXhz2Xbt2beXzwfLly8P2GTNmVD4eXUjuDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJwhDdCKHkzPggiikKrswe0sEGHevHkdbStWrAj7rly5snLfaLnZOkSBCsXu3bs72jZu3Bj23bJlS0fb1q1bw747d+7saDt48GDYN3tIPwprqBNQwdDNn2xORAEO2TjNgtui4I8s1CQaS9mYGeqQmOgYk4VZzJ8/v/L2PHHiRNh3165dHW179+4N+0bb3pyqzpw4x5wgmxNZENSyZcvC9lWrVlXuO2vWrEoBXFnoU3Z9cuDAgbB93759lUOqojmYBexFQT/Re+vvGBMdC7LXi9YtC4PNgvA4/7IArTpBi1GQ4JEjRypfs2fLzZYRHUuzeRUt+7LLLgv7zpkzp3LfbM5H59fheDx3ZxgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOMohgEAAGic8UOZxhYl5WUpzHPnzq2U4lwsXbo0bL/kkks62tasWRP2vfTSSyunaEYJg1kSZ5YOePTo0Y627du3h303b97c0fa9730v7Pvtb3+70r/vL80zSsIbzBRUqomSYWfPnl15PGdzKEsV37ZtW+UxGqV5Hj9+vPL4Gurk3CxdNZvzUbJ8liIcHf+yFMgobX44Ji0OV+bEOeZEs2QJw9E1x8KFC8O+11xzTdgeJcZGCdPFokWLKqctR+uWJbdHKbvFjh07Kv3iRpZInf3CR/QLJtn16eTJkytft2Zp0lHab9ZXmnQ12XEi2n7ZNq2TEB1dxxc9PT2Vx110Tjl16lTYt7e3N2yPzivZOazO8aEv2J5ZLTBp0qTK2zPbxheSO8MAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpHMQwAAEDjDGmadJ0Ey5kzZ1ZKLSwuuuiisH358uWV0v6ydcuSP6M0tSzlLVpulhqYvb8oEfTEiROVkxb37NlTK+ma4SkaB4sXLw77PulJT6qcsJ4l+23atKmj7fvf/37Y94477qicVp4lMEbrkaVDRseYbDxHczCbl1kScZRYnyUAR6mRWeJwlL47HJMWhytz4hxzgmyfr169Ouy7du3asP3yyy+vfO0U/ZJGliwbzdfoWq+4+OKLw/bdu3dXuu7J5muU4pyN0Sw1OmuPEtKza9woqTpLkzb+ByY6ZmbbNEtLjtKZs75R++HDh8O+0TEzO45mCdHRe8mO/dF8mxIkqRfTpk1rVVVnXmXJ2heSO8MAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpnUBKUsoCPMWPGVA7bivpmD3lnoQOnTp3qaNu4cWPlsJMspCp6OD4LjJg3b17YvmTJksqBEVOnTu1oW7BgQdh31qxZlYNR6uy/bJ8ydKLwkSx07Yorrgjbr7rqqspzMApdmT9/fuVAkZtuuinsm4VORPOtzrEkUydYKJvHUYjQnDlzwr5ROMTWrVvDvlmgEtWYE+eYE2TXQ9kYvfLKK8P2lStXdrTt27cv7HvzzTdXDrSKxm52jRQF22XvJesbXWdFAVzF/v37K187ZQFH0fuLlpvNwdOnT1deLtVFx8YsQCvbt3WO29EysuNztIy6gWl1judRjXBJMn+iuZmN0ez4ELXX2cZDxZ1hAAAAGkcxDAAAQOMohgEAAGgcxTAAAACNoxgGAACgcQYlTToTJaRFKZNZEnSWVpYl/u3atauj7dixY2Hf3t7ejrajR4+GfaN1zhKts2TT6667rnIS58yZMzvapk2bVjnBNEumy5LwJBeOnOTcKNE1SwPNkjiz+TNjxoxKbf2l70ayJME9e/ZUOg5kr1cnrTFbhywpMbJs2bKwPVrnLDn3zjvvHNA6NJ050f/rmRPNE10DZGnSWcJ6lER7++23h32/853vdLStW7eu8nVWV1dX2Hf58uVh+xOf+MSOtic96Ulh31WrVlW+zorW7eTJk2Hf7Doyas/6Hjx4sPL1MAMTXdPWSd/PlpFdW0fHqzrX1dm6Ree7bL5mNUI0J9auXRv2jc6lGzZsCPtm7dGvA9RNyx4K7gwDAADQOIphAAAAGkcxDAAAQOMohgEAAGicQQnQyh7+jh4gz8IFsgfFI0eOHAnbo0CQKCShOH78eOXQjih8ZNKkSWHfLBAhCnPJAkyicK4s/CraFlkIRLaM6OF2oVoXXjSvsuC2WbNmVQ5ayMZdNAcXLFgQ9n3yk588oPlTXH/99R1tPT09Yd9onbMxWie4L5sr0bbPglii7XbLLbeEfaOQi+x4Ridz4hxzguxaZN68eWHfLPwt2ufZuNu5c2eltmwZ2fXitm3bKr9edl33ohe9qKNtzZo1Yd+pU6d2tJ04cSLse/jw4crrnAWxRu2C4oZO3ZCqqD3rWyc8sU7wYRbsGI3dyy67rHIA3aogVCsbj3fffXfYd/369ZXnigAtAAAAGAYUwwAAADSOYhgAAIDGUQwDAADQOIphAAAAGmdQ0qSzpLCBJuVl/z5KA81kyZhRe5bwWSf9LUuVmz9/fuW00yjlc8+ePWHfffv2VUrK7i8xVXL08BTtl2wfZsmw0Rw6cOBA2DdK0szSR2fOnNnR9rjHPa7W8SF6L+vWrQv77t+/f0Dp6FlKaJ3k9Sw5d8qUKR1tq1evDvtG23PHjh1hXzqZE+eYE2T7Nhtf2T6PdHV1VW7PUm+j8Zxd12XzNZLNiSg5Orr2KqZPn97R1t3dXeu6bvLkyZWPD3X2E9Vk1+FRjZD9Aky2z6NjWDYnonEQtWX7PKtpsnVesmRJR9s111wT9r3uuusqv4+bb7658q8AbNmyJWw/depUayRwZxgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOMMSoBWFsAUBW5k4QnRQ+VZ33HjxlVetzpBJZno9aZOnRr2XbVqVdj+2Mc+tvJD7FEwyt133x323b59e+WQjCxcheEpGrvZuK0TIrR3796w765duyqHQCxYsKBSIElx7bXXVg4lyYJYbrvttkrhcdk4z4I2srCHaBnZcSfaFtnxIXt/VGNOnGNONEt2nRWN56NHj4Z9d+/eHbZHYZ5RIFx2jZPNn0OHDnW09fT01Lo+icbjkSNHwr5REF7WN5rHEyZMqBWgFc3vaOxn2zM6vtQNOmuyLHgq2taLFi0K+2btUfBaFsYWhaZFAVzZWMrG17Rp08L2ZcuWVX4fUcjhliT8KgqmW79+fdg3O8ZEx6nsXBO1D1WgrzvDAAAANI5iGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4wxpmvRA052z5WYpoXWSyaL2LEkwSsHMUqOvu+66sH3x4sUdbceOHQv7Rultt99+e9h3z549ldNAhyqljfMj2l+HDx8O+2ZjKUo5zFLaDx48WCnZvNi2bVvlOTFv3rywfe3atZUT1qP3kaUcRutcJ3UyS3HMknOjdZ47d27YN9v2VGNOnGNONEuWyBqlMGep0Tt27Kj8elnCbbQfV65cGfaN0s2zFNooFTsbY1kCeXR86O3tbVWVJcVn2z6aK0uXLg37Ll++vKNt06ZNYd/smNZk0XiMkpKzX295zGMeE/bNjtHRNXt2bIzGTTZGJ02aVDl5Onu9qD07nkeJ5duDX6HJ2rP5mr1e9L6zui9rHwruDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaBzFMAAAAI0zKGnSdWSJxlGqWNY3S/aL2rNExCg5evbs2WHfKCnxcY97XK1kumjdtm7dGva94YYbOtruvPPOsG+UlFgnbZvhK5oTx48fr5x6m6UtZum0USppNkaj5M8sZfeyyy4L2xcuXNjRduWVV4Z9o7l5zz33hH23bNnS0Xby5Mmw76JFiyq319lu2X7Kkhmpxpw4x5ygbpr05s2bK6fhZkmvURpulDSejdHoFzCKI0eOVE6nzVKfo3XLriMPHTpU+Zc4srTfqD1Lk66TTrx3796Otqb/Gkh0rIlSo4unPvWpldOks+NddL2cjY86qcjR2M1SsbME/qh+OXHiROVfVTiSzLXIrFmzBpz+ntUe0bpFx7PB4M4wAAAAjaMYBgAAoHEUwwAAADSOYhgAAIDGGbYBWlF7naCsLCxr/Pj4LUcPrF9yySVh3yc84QkdbWvXrg37dnd3h+09PT0dbXfddVfY97bbbqsciHH69OnWQEXbrc5+YuhkAQ6HDx8O26MwtSwMJApg2LVrV9h33759HW07d+6sHFRSXHvttR1ta9asCfteffXVlYOFDhw4UGl9syCKLMgoC+OLtlEUgNJfiBCPnjlxjjnRPNF4zvbtxo0bw/bp06dXDkeL5lsUXJVdD2XXSNm1TBROlAUWReE9WaBodHzIwr2ybXHRRRdVDsWK5k8WThTNq6EKFrrQsuv7+fPnVw7Fio6D0fbvb3xEx6so8CnbX/PmzQv7RmFz2XE0O7dF65Gd76IQxzHJNo4Cu7KwrToBx9ncjkIcBWgBAADAIFEMAwAA0DiKYQAAABpHMQwAAEDjKIYBAABonAueJj2YohTM2bNnh32jhM5rrrkm7Pv4xz++UrJdcebMmbB906ZNHW1btmwJ+0bJ0VkaW5TcliXT1WnP0hojEqaHTpYumI2PKE09Sx2MEm6zVNIodTNKTO/v9Q4ePNjR1tvbW3kOLl26tHJS/OLFi8O+WTpkNP537NgR9l23bl1H2w9+8IOwr+Tc88+cOMecaJ5ov2Rp5du2bQvbp06dWmnMZOM/e70oqTdLK89++SN6f9mYicZunV/ciNJt+zvGRNsoSuYuVqxYUenf95f22wTZe4+St6Ntml2fz5gxI+ybpdxnY6FqQnRWe0S/XJAdc7PzRJTwnP3izNGjRyuP58mTJ1dOR8/mYHQsmDhxYmug+/981xnuDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJxREaA1bty4sL2rq6vyA/aXX355R9vatWsrP4x/4sSJsG/2EHsUoLV9+/awbxT4kD2AXifoqk4oQ/aw+n333TfgZfDot18WBhIFlWTjI+sbBUZk4T1RsEMWHpctIwpa2LlzZ9j3zjvvrBSCl8357JiRhWRE7QcOHAj73nbbbZXa6oa50MmcOMecIBvPUcBOsWvXrrB90qRJlftG46BOAF02JzJRwGc2RqMwpGx8RUFGWfhVdp0VzaFoWxbd3d2VAov6Czttguy9R8f+aJtmtUC2bzNR/2wcRPs8O75GQYtZKGN2fI3mVRZoVaee6A3OS3WPz9H+y86vF1JzZxgAAACNpRgGAACgcRTDAAAANI5iGAAAgMZRDAMAANA4IypNOks/zpLQ5syZUzld8+qrr+5oW7ZsWdg3SlOLEuH6S/7csWNH5aS4KIUuS8KLUuzGj493c5ZyGKXQZWnZURJklhRXJ3maVqXkwixFM0sjjBJBs9TBqD17vahvNg6y14sSSLN5dffdd3e0zZs3L+y7YMGCjrapU6cOOLUzex9btmzpaNu2bVvY15wYGHPiHHOiWer8wkN2/t6/f3/YHl0zRGnL2Zg+depU2Pfo0aOV5082DqLrr4MHD1b+1Y5o7Ge/KJL1nTBhQtgezbcsRThaRp1fA2mK7Lo/GkvZ9XZ0zMyOdzNnzqw837JfAYjmVZbGHh0b77nnnrBvduyPxk123R/VS5OTFPNo22fJ9NkcrHMevJC/OOPOMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxhm2AVrRg9tZEEFXV1fYvmjRosoBWllYViQKuurp6Qn77t69O2yPQiOyh9ijoIUsXCJ6AD0KnOgvVCMKJoja6vaNCEtpDWhbHTt2LOy7efPmyuEQ2byqExYUBSJk4y4LCYlCV85HsFAURpGFwWTBKNHxKNsWUcBEtp8uZGDEaGBOnGNOkI2lbNxloTfRPqgToFUnKC4bB9n1SRQMtHfv3rDvhg0bOtpmzJgR9o1CSVesWBH2nT9/fuVr0UOHDlUO9Mu2W5Nlx4PoOvz666+vvNxs7M+dO7dyWGx23R8dX6Mwt2Lr1q2Vx3M2j6NjdDZfozqjK6mhovNgNl+zcV4nNC86dg3V+cCdYQAAABpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHFGVJp0lmo5bdq0sH3x4sUdbQsWLKicUBilx2WJZ2PHxp8rZIlu0bplCaZZeyRKacsSCqP00SwBO0q8y7ZRlIDaXxIe1UTjLkvuzvZXlBoYjf0sbTEbS1Gqb5aQm6UDRu1Z32g9snEXJZiej7lWZ1tICR0c5kT/62FONE+dNOnsGiDaN5MmTar8etmvRETrkaWjZ9df0TKy14sS5Lu7uyun7GbzNWuPEqmzFPuNGzdW3k9NTljPtnWUanzjjTeGfbdv397R9o1vfKPyPszGY3auiZKVe3t7w77R+M+O29k4j47R2XyN2idOnBj2jeqa7Lhd51yT9c329VBwZxgAAIDGUQwDAADQOIphAAAAGkcxDAAAQOMohgEAAGicEZUmnaWjTZ06NWyfOXNm5dS06PWy1MEo2S9LjV60aFHYHqWVZgmm0bpFSXpZmnSWopqlhO7fv79S0mKWFJcl3jEw0biL9nexa9euyvs8S2nfs2dP5X0brdtgJufWEa1HlogYzbXMUL8POpkTj445MXrVSZPOxm6UnJtdn0SysRTt86xvnblS55rjrrvuCtuj9cius3bu3Bm2R9eoBw4cqJwmnSVP15mDo002DqJE4mwsRUnOddPzBzp2szEa9c2Ojdk4iOZrlsaepbdXfb1s3eokr9c9Dw4Fd4YBAABoHMUwAAAAjaMYBgAAoHEUwwAAADTOiArQyowdG9f0J06cqPTQfRaY0tXVFfadPn165QfCs4fYo/eXPWAfrXO2faKggGg7ZGEwxcGDBzvajhw5UnndhuPD8U0LC+rp6am83CwY5dChQ5XHc50QiLrtAyW8Z/QyJx4dc2L0qhPclrVH1yLZNcdAQ3bOx9jP3kd0PZSFiUVhWdu3bw/7zpo1q/Jx4+jRo5XXbffu3WHfLKipyeoEMw2H7Xc+jq91llGn75jzENA21Oew882dYQAAABpHMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHGGbZp0lECWpQBGCZ/Frl27OtpuvfXWyuuwbNmysL27u7tyYlqWNBolPGepg9H727p1a9h3y5YtHW3btm2rnBpd7Ny5s1LSYrZPsvc8UlLlRsOcyPZtlBI6ceLEsO/x48crv559y4VgTsAjqzsWs1+2GGia9GDJXi96H9mva0S/jJFdW2bHhzpp2dF1Up1keqpr4rF4sJKnRyt3hgEAAGgcxTAAAACNoxgGAACgcRTDAAAANM6YvopPTkchCYMper1x48aFfbu6usL2mTNndrTNmzcv7Dt37tyOtgULFoR9o2WMHx9nkR05ciRs7+3trRzWELWfPHmyckhM9Fr9LSMKhMkCHKL2e++9tzVQI+GB/qGeE5GxY+PPs7K5EgV/ZGEg0T7IwkeioJIshGUk7NvhaCRsN3PiHHNi8I2E7TYc5gTDb/8P1tg1J6D+nHBnGAAAgMZRDAMAANA4imEAAAAaRzEMAABA4yiGAQAAaJxhmyY9WKmiWdJolAadJYpOnjy58nKzFOZTp051tJ05cybsGyWQ1kl3znbx+UgdlIg4stYtGqdZ36j9fIw7Hp2RsD3Nif7bR8I+HElGwvYcznOC0cecgIeSJg0AAAABxTAAAACNoxgGAACgcRTDAAAANM6oDtBi5BICceHXuU7faH+NhH04koyE7WlOnGNODL6RsD1H4pxg5DIn4KEEaAEAAEBAMQwAAEDjKIYBAABoHMUwAAAAjaMYBgAAoHEqp0kDAADAaOHOMAAAAI2jGAYAAKBxFMMAAAA0jmIYAACAxlEMAwAA0DiKYQAAABpHMQwAAEDjKIYBAABoHMUwAAAArab5/+RPYPetZJKzAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c19a7f",
   "metadata": {},
   "source": [
    "### 8 features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b37487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 293.0254\n",
      "Epoch: 1/15, Average loss: 199.2283\n",
      "Epoch: 2/15, Average loss: 179.8768\n",
      "Epoch: 3/15, Average loss: 173.0299\n",
      "Epoch: 4/15, Average loss: 169.1212\n",
      "Epoch: 5/15, Average loss: 166.2844\n",
      "Epoch: 6/15, Average loss: 164.3290\n",
      "Epoch: 7/15, Average loss: 162.8076\n",
      "Epoch: 8/15, Average loss: 161.5842\n",
      "Epoch: 9/15, Average loss: 160.7148\n",
      "Epoch: 10/15, Average loss: 159.9534\n",
      "Epoch: 11/15, Average loss: 159.2769\n",
      "Epoch: 12/15, Average loss: 158.8208\n",
      "Epoch: 13/15, Average loss: 158.2286\n",
      "Epoch: 14/15, Average loss: 157.8266\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 293.5113\n",
      "Epoch: 1/15, Average loss: 198.2328\n",
      "Epoch: 2/15, Average loss: 178.4778\n",
      "Epoch: 3/15, Average loss: 170.6506\n",
      "Epoch: 4/15, Average loss: 165.6475\n",
      "Epoch: 5/15, Average loss: 162.1394\n",
      "Epoch: 6/15, Average loss: 159.5660\n",
      "Epoch: 7/15, Average loss: 157.7338\n",
      "Epoch: 8/15, Average loss: 156.2479\n",
      "Epoch: 9/15, Average loss: 155.0138\n",
      "Epoch: 10/15, Average loss: 153.9606\n",
      "Epoch: 11/15, Average loss: 153.0693\n",
      "Epoch: 12/15, Average loss: 152.4010\n",
      "Epoch: 13/15, Average loss: 151.7946\n",
      "Epoch: 14/15, Average loss: 151.3296\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 297.2092\n",
      "Epoch: 1/15, Average loss: 201.5075\n",
      "Epoch: 2/15, Average loss: 180.5769\n",
      "Epoch: 3/15, Average loss: 172.8325\n",
      "Epoch: 4/15, Average loss: 168.2767\n",
      "Epoch: 5/15, Average loss: 164.9350\n",
      "Epoch: 6/15, Average loss: 162.4663\n",
      "Epoch: 7/15, Average loss: 160.6125\n",
      "Epoch: 8/15, Average loss: 159.1465\n",
      "Epoch: 9/15, Average loss: 158.0652\n",
      "Epoch: 10/15, Average loss: 156.9120\n",
      "Epoch: 11/15, Average loss: 156.1592\n",
      "Epoch: 12/15, Average loss: 155.3253\n",
      "Epoch: 13/15, Average loss: 154.7250\n",
      "Epoch: 14/15, Average loss: 154.2013\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 299.0114\n",
      "Epoch: 1/15, Average loss: 204.6393\n",
      "Epoch: 2/15, Average loss: 183.8510\n",
      "Epoch: 3/15, Average loss: 176.4470\n",
      "Epoch: 4/15, Average loss: 172.9547\n",
      "Epoch: 5/15, Average loss: 170.6624\n",
      "Epoch: 6/15, Average loss: 169.1338\n",
      "Epoch: 7/15, Average loss: 167.6985\n",
      "Epoch: 8/15, Average loss: 166.4294\n",
      "Epoch: 9/15, Average loss: 165.4667\n",
      "Epoch: 10/15, Average loss: 164.3483\n",
      "Epoch: 11/15, Average loss: 163.4033\n",
      "Epoch: 12/15, Average loss: 162.6376\n",
      "Epoch: 13/15, Average loss: 161.7045\n",
      "Epoch: 14/15, Average loss: 161.0767\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 307.8540\n",
      "Epoch: 1/15, Average loss: 220.9278\n",
      "Epoch: 2/15, Average loss: 194.7669\n",
      "Epoch: 3/15, Average loss: 184.7988\n",
      "Epoch: 4/15, Average loss: 180.1304\n",
      "Epoch: 5/15, Average loss: 177.7514\n",
      "Epoch: 6/15, Average loss: 175.4698\n",
      "Epoch: 7/15, Average loss: 173.7945\n",
      "Epoch: 8/15, Average loss: 172.4466\n",
      "Epoch: 9/15, Average loss: 171.3992\n",
      "Epoch: 10/15, Average loss: 170.2958\n",
      "Epoch: 11/15, Average loss: 169.1930\n",
      "Epoch: 12/15, Average loss: 168.4157\n",
      "Epoch: 13/15, Average loss: 167.6548\n",
      "Epoch: 14/15, Average loss: 166.8264\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f5400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 317.1253\n",
      "Epoch: 1/15, Average loss: 254.0380\n",
      "Epoch: 2/15, Average loss: 250.4241\n",
      "Epoch: 3/15, Average loss: 249.7298\n",
      "Epoch: 4/15, Average loss: 249.5778\n",
      "Epoch: 5/15, Average loss: 249.5387\n",
      "Epoch: 6/15, Average loss: 249.5223\n",
      "Epoch: 7/15, Average loss: 249.5062\n",
      "Epoch: 8/15, Average loss: 249.5075\n",
      "Epoch: 9/15, Average loss: 249.5052\n",
      "Epoch: 10/15, Average loss: 249.4995\n",
      "Epoch: 11/15, Average loss: 249.4920\n",
      "Epoch: 12/15, Average loss: 249.4916\n",
      "Epoch: 13/15, Average loss: 249.4944\n",
      "Epoch: 14/15, Average loss: 249.4895\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46a0c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 314.7471\n",
      "Epoch: 1/15, Average loss: 246.3329\n",
      "Epoch: 2/15, Average loss: 242.4432\n",
      "Epoch: 3/15, Average loss: 246.4289\n",
      "Epoch: 4/15, Average loss: 249.6111\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m8\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:19\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     17\u001b[39m global_batch_idx += \u001b[32m1\u001b[39m\n\u001b[32m     18\u001b[39m data = data.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss, KL, rec_loss = model(data, temp, _lambda, hard=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     21\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:962\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    960\u001b[39m     per_device_and_dtype_grads = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_zero_grad_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_groups\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/profiler.py:787\u001b[39m, in \u001b[36mrecord_function.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    784\u001b[39m \u001b[38;5;66;03m# TODO: Too slow with __torch_function__ handling enabled\u001b[39;00m\n\u001b[32m    785\u001b[39m \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/76410\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDisableTorchFunctionSubclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    788\u001b[39m         torch.ops.profiler._record_function_exit._RecordFunction(record)\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d42e3",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84289a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 289.6640\n",
      "Epoch: 1/15, Average loss: 193.0851\n",
      "Epoch: 2/15, Average loss: 173.0547\n",
      "Epoch: 3/15, Average loss: 165.9744\n",
      "Epoch: 4/15, Average loss: 162.1465\n",
      "Epoch: 5/15, Average loss: 159.2105\n",
      "Epoch: 6/15, Average loss: 156.8267\n",
      "Epoch: 7/15, Average loss: 154.7964\n",
      "Epoch: 8/15, Average loss: 153.1571\n",
      "Epoch: 9/15, Average loss: 151.8358\n",
      "Epoch: 10/15, Average loss: 150.8027\n",
      "Epoch: 11/15, Average loss: 149.9243\n",
      "Epoch: 12/15, Average loss: 149.1409\n",
      "Epoch: 13/15, Average loss: 148.5724\n",
      "Epoch: 14/15, Average loss: 147.9692\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 291.2133\n",
      "Epoch: 1/15, Average loss: 192.8380\n",
      "Epoch: 2/15, Average loss: 172.2742\n",
      "Epoch: 3/15, Average loss: 165.7251\n",
      "Epoch: 4/15, Average loss: 162.3980\n",
      "Epoch: 5/15, Average loss: 159.8999\n",
      "Epoch: 6/15, Average loss: 157.8492\n",
      "Epoch: 7/15, Average loss: 155.8201\n",
      "Epoch: 8/15, Average loss: 153.9527\n",
      "Epoch: 9/15, Average loss: 152.0119\n",
      "Epoch: 10/15, Average loss: 150.3478\n",
      "Epoch: 11/15, Average loss: 148.7277\n",
      "Epoch: 12/15, Average loss: 147.1640\n",
      "Epoch: 13/15, Average loss: 145.8842\n",
      "Epoch: 14/15, Average loss: 144.8714\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 294.2355\n",
      "Epoch: 1/15, Average loss: 196.2685\n",
      "Epoch: 2/15, Average loss: 173.8741\n",
      "Epoch: 3/15, Average loss: 166.0807\n",
      "Epoch: 4/15, Average loss: 162.0633\n",
      "Epoch: 5/15, Average loss: 158.9627\n",
      "Epoch: 6/15, Average loss: 156.5541\n",
      "Epoch: 7/15, Average loss: 154.5101\n",
      "Epoch: 8/15, Average loss: 152.6384\n",
      "Epoch: 9/15, Average loss: 150.9864\n",
      "Epoch: 10/15, Average loss: 149.4495\n",
      "Epoch: 11/15, Average loss: 148.0731\n",
      "Epoch: 12/15, Average loss: 146.9253\n",
      "Epoch: 13/15, Average loss: 146.0283\n",
      "Epoch: 14/15, Average loss: 144.9650\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 298.5488\n",
      "Epoch: 1/15, Average loss: 202.9798\n",
      "Epoch: 2/15, Average loss: 181.3341\n",
      "Epoch: 3/15, Average loss: 173.6317\n",
      "Epoch: 4/15, Average loss: 169.8361\n",
      "Epoch: 5/15, Average loss: 167.2894\n",
      "Epoch: 6/15, Average loss: 165.4415\n",
      "Epoch: 7/15, Average loss: 163.9719\n",
      "Epoch: 8/15, Average loss: 162.8791\n",
      "Epoch: 9/15, Average loss: 161.7283\n",
      "Epoch: 10/15, Average loss: 160.6633\n",
      "Epoch: 11/15, Average loss: 159.9641\n",
      "Epoch: 12/15, Average loss: 159.1463\n",
      "Epoch: 13/15, Average loss: 158.3637\n",
      "Epoch: 14/15, Average loss: 157.6858\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.3491\n",
      "Epoch: 1/15, Average loss: 215.5182\n",
      "Epoch: 2/15, Average loss: 188.7768\n",
      "Epoch: 3/15, Average loss: 179.3800\n",
      "Epoch: 4/15, Average loss: 174.9331\n",
      "Epoch: 5/15, Average loss: 172.1443\n",
      "Epoch: 6/15, Average loss: 170.2548\n",
      "Epoch: 7/15, Average loss: 168.3283\n",
      "Epoch: 8/15, Average loss: 167.0273\n",
      "Epoch: 9/15, Average loss: 165.5205\n",
      "Epoch: 10/15, Average loss: 164.6139\n",
      "Epoch: 11/15, Average loss: 163.4669\n",
      "Epoch: 12/15, Average loss: 162.5855\n",
      "Epoch: 13/15, Average loss: 161.6188\n",
      "Epoch: 14/15, Average loss: 160.9562\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 307.4619\n",
      "Epoch: 1/15, Average loss: 224.2687\n",
      "Epoch: 2/15, Average loss: 198.2998\n",
      "Epoch: 3/15, Average loss: 185.7454\n",
      "Epoch: 4/15, Average loss: 180.7001\n",
      "Epoch: 5/15, Average loss: 177.3740\n",
      "Epoch: 6/15, Average loss: 175.3895\n",
      "Epoch: 7/15, Average loss: 173.9622\n",
      "Epoch: 8/15, Average loss: 172.5908\n",
      "Epoch: 9/15, Average loss: 171.5822\n",
      "Epoch: 10/15, Average loss: 170.8760\n",
      "Epoch: 11/15, Average loss: 169.7851\n",
      "Epoch: 12/15, Average loss: 169.0375\n",
      "Epoch: 13/15, Average loss: 168.5531\n",
      "Epoch: 14/15, Average loss: 167.5461\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c66f978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 318.0827\n",
      "Epoch: 1/15, Average loss: 254.0073\n",
      "Epoch: 2/15, Average loss: 250.4167\n",
      "Epoch: 3/15, Average loss: 249.7302\n",
      "Epoch: 4/15, Average loss: 249.5816\n",
      "Epoch: 5/15, Average loss: 249.5373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m10\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:20\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     18\u001b[39m data = data.to(device)\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m loss, KL, rec_loss = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhard\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m loss.backward()\n\u001b[32m     22\u001b[39m train_loss += loss.item() * \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/model.py:114\u001b[39m, in \u001b[36mVAE_priorCategorical.forward\u001b[39m\u001b[34m(self, data, temp, _lambda, hard)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    112\u001b[39m     data_flat = data\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m logits_z_flat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_flat\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch_size, latent_dim * categorical_dim)\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Reshape per gumbel_softmax e calcolo KL\u001b[39;00m\n\u001b[32m    116\u001b[39m logits_z = logits_z_flat.view(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.latent_dim, \u001b[38;5;28mself\u001b[39m.categorical_dim) \u001b[38;5;66;03m# (batch_size, latent_dim, categorical_dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/model.py:99\u001b[39m, in \u001b[36mVAE_priorCategorical.encode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     98\u001b[39m     \u001b[38;5;66;03m# x ha shape (batch_size, input_dim)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = module(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7130fe",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbdaa5",
   "metadata": {},
   "source": [
    "### 8 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 386.3299\n",
      "Epoch: 1/15, Average loss: 269.6630\n",
      "Epoch: 2/15, Average loss: 220.6156\n",
      "Epoch: 3/15, Average loss: 192.3674\n",
      "Epoch: 4/15, Average loss: 172.4641\n",
      "Epoch: 5/15, Average loss: 158.6117\n",
      "Epoch: 6/15, Average loss: 148.8546\n",
      "Epoch: 7/15, Average loss: 141.7337\n",
      "Epoch: 8/15, Average loss: 136.4395\n",
      "Epoch: 9/15, Average loss: 132.2159\n",
      "Epoch: 10/15, Average loss: 128.9444\n",
      "Epoch: 11/15, Average loss: 126.1562\n",
      "Epoch: 12/15, Average loss: 123.7926\n",
      "Epoch: 13/15, Average loss: 121.9730\n",
      "Epoch: 14/15, Average loss: 120.2335\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 386.8003\n",
      "Epoch: 1/15, Average loss: 270.2383\n",
      "Epoch: 2/15, Average loss: 221.5904\n",
      "Epoch: 3/15, Average loss: 193.3287\n",
      "Epoch: 4/15, Average loss: 173.1111\n",
      "Epoch: 5/15, Average loss: 158.6518\n",
      "Epoch: 6/15, Average loss: 148.4285\n",
      "Epoch: 7/15, Average loss: 140.9530\n",
      "Epoch: 8/15, Average loss: 135.4636\n",
      "Epoch: 9/15, Average loss: 131.2472\n",
      "Epoch: 10/15, Average loss: 127.9704\n",
      "Epoch: 11/15, Average loss: 125.2929\n",
      "Epoch: 12/15, Average loss: 123.1115\n",
      "Epoch: 13/15, Average loss: 121.3816\n",
      "Epoch: 14/15, Average loss: 119.8358\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 388.8020\n",
      "Epoch: 1/15, Average loss: 271.3259\n",
      "Epoch: 2/15, Average loss: 222.7423\n",
      "Epoch: 3/15, Average loss: 194.5868\n",
      "Epoch: 4/15, Average loss: 174.3165\n",
      "Epoch: 5/15, Average loss: 160.1383\n",
      "Epoch: 6/15, Average loss: 150.3916\n",
      "Epoch: 7/15, Average loss: 143.2736\n",
      "Epoch: 8/15, Average loss: 137.8577\n",
      "Epoch: 9/15, Average loss: 133.7821\n",
      "Epoch: 10/15, Average loss: 130.6661\n",
      "Epoch: 11/15, Average loss: 128.2295\n",
      "Epoch: 12/15, Average loss: 126.0127\n",
      "Epoch: 13/15, Average loss: 124.4271\n",
      "Epoch: 14/15, Average loss: 123.0456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST//ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 402.5911\n",
      "Epoch: 1/15, Average loss: 275.8961\n",
      "Epoch: 2/15, Average loss: 226.4554\n",
      "Epoch: 3/15, Average loss: 198.4908\n",
      "Epoch: 4/15, Average loss: 177.8793\n",
      "Epoch: 5/15, Average loss: 163.3509\n",
      "Epoch: 6/15, Average loss: 153.2155\n",
      "Epoch: 7/15, Average loss: 146.2363\n",
      "Epoch: 8/15, Average loss: 141.0705\n",
      "Epoch: 9/15, Average loss: 137.0449\n",
      "Epoch: 10/15, Average loss: 134.0465\n",
      "Epoch: 11/15, Average loss: 131.8412\n",
      "Epoch: 12/15, Average loss: 130.0399\n",
      "Epoch: 13/15, Average loss: 128.2418\n",
      "Epoch: 14/15, Average loss: 127.0624\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050da7b7",
   "metadata": {},
   "source": [
    "### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f34279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 382.6704\n",
      "Epoch: 1/15, Average loss: 269.1358\n",
      "Epoch: 2/15, Average loss: 219.8319\n",
      "Epoch: 3/15, Average loss: 190.9535\n",
      "Epoch: 4/15, Average loss: 170.5004\n",
      "Epoch: 5/15, Average loss: 156.3013\n",
      "Epoch: 6/15, Average loss: 146.3802\n",
      "Epoch: 7/15, Average loss: 139.2002\n",
      "Epoch: 8/15, Average loss: 133.7200\n",
      "Epoch: 9/15, Average loss: 129.5677\n",
      "Epoch: 10/15, Average loss: 126.1323\n",
      "Epoch: 11/15, Average loss: 123.3453\n",
      "Epoch: 12/15, Average loss: 121.0505\n",
      "Epoch: 13/15, Average loss: 119.1494\n",
      "Epoch: 14/15, Average loss: 117.6010\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 384.0852\n",
      "Epoch: 1/15, Average loss: 267.8581\n",
      "Epoch: 2/15, Average loss: 218.3796\n",
      "Epoch: 3/15, Average loss: 189.0615\n",
      "Epoch: 4/15, Average loss: 168.0823\n",
      "Epoch: 5/15, Average loss: 153.4838\n",
      "Epoch: 6/15, Average loss: 143.4466\n",
      "Epoch: 7/15, Average loss: 136.2004\n",
      "Epoch: 8/15, Average loss: 130.9332\n",
      "Epoch: 9/15, Average loss: 126.8407\n",
      "Epoch: 10/15, Average loss: 123.6340\n",
      "Epoch: 11/15, Average loss: 121.0773\n",
      "Epoch: 12/15, Average loss: 118.7623\n",
      "Epoch: 13/15, Average loss: 116.8151\n",
      "Epoch: 14/15, Average loss: 115.2380\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 385.8713\n",
      "Epoch: 1/15, Average loss: 269.0690\n",
      "Epoch: 2/15, Average loss: 219.5711\n",
      "Epoch: 3/15, Average loss: 190.0897\n",
      "Epoch: 4/15, Average loss: 169.0395\n",
      "Epoch: 5/15, Average loss: 154.4246\n",
      "Epoch: 6/15, Average loss: 144.1625\n",
      "Epoch: 7/15, Average loss: 136.9041\n",
      "Epoch: 8/15, Average loss: 131.5134\n",
      "Epoch: 9/15, Average loss: 127.3652\n",
      "Epoch: 10/15, Average loss: 124.1533\n",
      "Epoch: 11/15, Average loss: 121.4891\n",
      "Epoch: 12/15, Average loss: 119.3849\n",
      "Epoch: 13/15, Average loss: 117.5831\n",
      "Epoch: 14/15, Average loss: 116.1856\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.6227\n",
      "Epoch: 1/15, Average loss: 270.5158\n",
      "Epoch: 2/15, Average loss: 221.6360\n",
      "Epoch: 3/15, Average loss: 192.9631\n",
      "Epoch: 4/15, Average loss: 171.9100\n",
      "Epoch: 5/15, Average loss: 157.1226\n",
      "Epoch: 6/15, Average loss: 146.9147\n",
      "Epoch: 7/15, Average loss: 139.7606\n",
      "Epoch: 8/15, Average loss: 134.5452\n",
      "Epoch: 9/15, Average loss: 130.5691\n",
      "Epoch: 10/15, Average loss: 127.6468\n",
      "Epoch: 11/15, Average loss: 125.3244\n",
      "Epoch: 12/15, Average loss: 123.3504\n",
      "Epoch: 13/15, Average loss: 121.7893\n",
      "Epoch: 14/15, Average loss: 120.3889\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 390.9649\n",
      "Epoch: 1/15, Average loss: 270.8257\n",
      "Epoch: 2/15, Average loss: 221.7338\n",
      "Epoch: 3/15, Average loss: 193.3308\n",
      "Epoch: 4/15, Average loss: 172.9608\n",
      "Epoch: 5/15, Average loss: 158.3673\n",
      "Epoch: 6/15, Average loss: 148.0558\n",
      "Epoch: 7/15, Average loss: 141.0201\n",
      "Epoch: 8/15, Average loss: 135.9111\n",
      "Epoch: 9/15, Average loss: 131.9691\n",
      "Epoch: 10/15, Average loss: 129.0454\n",
      "Epoch: 11/15, Average loss: 126.8269\n",
      "Epoch: 12/15, Average loss: 124.8382\n",
      "Epoch: 13/15, Average loss: 123.3273\n",
      "Epoch: 14/15, Average loss: 121.9604\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 402.9144\n",
      "Epoch: 1/15, Average loss: 278.1830\n",
      "Epoch: 2/15, Average loss: 229.3540\n",
      "Epoch: 3/15, Average loss: 201.5456\n",
      "Epoch: 4/15, Average loss: 181.0566\n",
      "Epoch: 5/15, Average loss: 165.8562\n",
      "Epoch: 6/15, Average loss: 155.0658\n",
      "Epoch: 7/15, Average loss: 147.2022\n",
      "Epoch: 8/15, Average loss: 141.7347\n",
      "Epoch: 9/15, Average loss: 137.6772\n",
      "Epoch: 10/15, Average loss: 134.4249\n",
      "Epoch: 11/15, Average loss: 132.2274\n",
      "Epoch: 12/15, Average loss: 130.1863\n",
      "Epoch: 13/15, Average loss: 128.6803\n",
      "Epoch: 14/15, Average loss: 127.5914\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 399.4419\n",
      "Epoch: 1/15, Average loss: 279.3267\n",
      "Epoch: 2/15, Average loss: 231.7823\n",
      "Epoch: 3/15, Average loss: 205.5778\n",
      "Epoch: 4/15, Average loss: 184.9356\n",
      "Epoch: 5/15, Average loss: 169.8275\n",
      "Epoch: 6/15, Average loss: 158.6164\n",
      "Epoch: 7/15, Average loss: 150.7300\n",
      "Epoch: 8/15, Average loss: 144.6163\n",
      "Epoch: 9/15, Average loss: 140.0995\n",
      "Epoch: 10/15, Average loss: 137.0610\n",
      "Epoch: 11/15, Average loss: 134.2746\n",
      "Epoch: 12/15, Average loss: 132.3369\n",
      "Epoch: 13/15, Average loss: 130.9848\n",
      "Epoch: 14/15, Average loss: 129.7188\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bc2f",
   "metadata": {},
   "source": [
    "## train over expandedHFM 16-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM\n",
    "val_loader = val_loader_expandedHFM\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b4f7f",
   "metadata": {},
   "source": [
    "### latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 369.4557\n",
      "Epoch: 1/20, Average loss: 253.0922\n",
      "Epoch: 2/20, Average loss: 202.8551\n",
      "Epoch: 3/20, Average loss: 172.7191\n",
      "Epoch: 4/20, Average loss: 152.2356\n",
      "Epoch: 5/20, Average loss: 137.6990\n",
      "Epoch: 6/20, Average loss: 126.6692\n",
      "Epoch: 7/20, Average loss: 118.0560\n",
      "Epoch: 8/20, Average loss: 111.1311\n",
      "Epoch: 9/20, Average loss: 105.9597\n",
      "Epoch: 10/20, Average loss: 101.8351\n",
      "Epoch: 11/20, Average loss: 98.4729\n",
      "Epoch: 12/20, Average loss: 95.4016\n",
      "Epoch: 13/20, Average loss: 92.9106\n",
      "Epoch: 14/20, Average loss: 90.9402\n",
      "Epoch: 15/20, Average loss: 89.1270\n",
      "Epoch: 16/20, Average loss: 87.6784\n",
      "Epoch: 17/20, Average loss: 86.5530\n",
      "Epoch: 18/20, Average loss: 85.3882\n",
      "Epoch: 19/20, Average loss: 84.4254\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e5b3",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 2, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b165db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 393.1789\n",
      "Epoch: 1/20, Average loss: 300.7643\n",
      "Epoch: 2/20, Average loss: 241.3813\n",
      "Epoch: 3/20, Average loss: 207.7646\n",
      "Epoch: 4/20, Average loss: 179.9620\n",
      "Epoch: 5/20, Average loss: 160.1453\n",
      "Epoch: 6/20, Average loss: 144.9132\n",
      "Epoch: 7/20, Average loss: 133.3975\n",
      "Epoch: 8/20, Average loss: 125.1795\n",
      "Epoch: 9/20, Average loss: 118.3757\n",
      "Epoch: 10/20, Average loss: 112.1573\n",
      "Epoch: 11/20, Average loss: 107.3208\n",
      "Epoch: 12/20, Average loss: 103.3839\n",
      "Epoch: 13/20, Average loss: 99.9120\n",
      "Epoch: 14/20, Average loss: 96.7396\n",
      "Epoch: 15/20, Average loss: 94.4459\n",
      "Epoch: 16/20, Average loss: 92.6706\n",
      "Epoch: 17/20, Average loss: 90.7141\n",
      "Epoch: 18/20, Average loss: 88.7004\n",
      "Epoch: 19/20, Average loss: 87.0102\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1eed",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 3, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 399.2147\n",
      "Epoch: 1/20, Average loss: 324.6712\n",
      "Epoch: 2/20, Average loss: 275.4050\n",
      "Epoch: 3/20, Average loss: 239.8818\n",
      "Epoch: 4/20, Average loss: 216.3649\n",
      "Epoch: 5/20, Average loss: 198.0273\n",
      "Epoch: 6/20, Average loss: 182.2649\n",
      "Epoch: 7/20, Average loss: 170.8574\n",
      "Epoch: 8/20, Average loss: 162.9704\n",
      "Epoch: 9/20, Average loss: 154.4810\n",
      "Epoch: 10/20, Average loss: 146.7258\n",
      "Epoch: 11/20, Average loss: 139.4719\n",
      "Epoch: 12/20, Average loss: 134.2167\n",
      "Epoch: 13/20, Average loss: 129.6427\n",
      "Epoch: 14/20, Average loss: 125.4191\n",
      "Epoch: 15/20, Average loss: 121.2396\n",
      "Epoch: 16/20, Average loss: 118.2277\n",
      "Epoch: 17/20, Average loss: 115.6689\n",
      "Epoch: 18/20, Average loss: 112.8884\n",
      "Epoch: 19/20, Average loss: 111.4160\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde52c1d",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 4, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47314339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 405.6318\n",
      "Epoch: 1/20, Average loss: 331.2717\n",
      "Epoch: 2/20, Average loss: 293.0868\n",
      "Epoch: 3/20, Average loss: 260.4425\n",
      "Epoch: 4/20, Average loss: 235.3888\n",
      "Epoch: 5/20, Average loss: 217.5635\n",
      "Epoch: 6/20, Average loss: 207.8059\n",
      "Epoch: 7/20, Average loss: 200.4518\n",
      "Epoch: 8/20, Average loss: 194.3792\n",
      "Epoch: 9/20, Average loss: 189.3980\n",
      "Epoch: 10/20, Average loss: 184.9584\n",
      "Epoch: 11/20, Average loss: 179.7132\n",
      "Epoch: 12/20, Average loss: 174.9633\n",
      "Epoch: 13/20, Average loss: 170.7941\n",
      "Epoch: 14/20, Average loss: 167.3850\n",
      "Epoch: 15/20, Average loss: 164.2246\n",
      "Epoch: 16/20, Average loss: 161.3431\n",
      "Epoch: 17/20, Average loss: 159.5106\n",
      "Epoch: 18/20, Average loss: 156.5435\n",
      "Epoch: 19/20, Average loss: 154.0239\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26814b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.6061\n",
      "Epoch: 1/20, Average loss: 368.8077\n",
      "Epoch: 2/20, Average loss: 357.9416\n",
      "Epoch: 3/20, Average loss: 353.8225\n",
      "Epoch: 4/20, Average loss: 351.9900\n",
      "Epoch: 5/20, Average loss: 351.0825\n",
      "Epoch: 6/20, Average loss: 350.5930\n",
      "Epoch: 7/20, Average loss: 350.3083\n",
      "Epoch: 8/20, Average loss: 350.1508\n",
      "Epoch: 9/20, Average loss: 350.0325\n",
      "Epoch: 10/20, Average loss: 349.9596\n",
      "Epoch: 11/20, Average loss: 349.9249\n",
      "Epoch: 12/20, Average loss: 349.8968\n",
      "Epoch: 13/20, Average loss: 349.8726\n",
      "Epoch: 14/20, Average loss: 349.8558\n",
      "Epoch: 15/20, Average loss: 349.8398\n",
      "Epoch: 16/20, Average loss: 349.8369\n",
      "Epoch: 17/20, Average loss: 349.8276\n",
      "Epoch: 18/20, Average loss: 349.8294\n",
      "Epoch: 19/20, Average loss: 349.8175\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 5 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.4123\n",
      "Epoch: 1/20, Average loss: 369.0100\n",
      "Epoch: 2/20, Average loss: 357.9196\n",
      "Epoch: 3/20, Average loss: 353.9314\n",
      "Epoch: 4/20, Average loss: 351.9628\n",
      "Epoch: 5/20, Average loss: 351.0653\n",
      "Epoch: 6/20, Average loss: 350.5855\n",
      "Epoch: 7/20, Average loss: 350.3135\n",
      "Epoch: 8/20, Average loss: 350.1441\n",
      "Epoch: 9/20, Average loss: 350.0360\n",
      "Epoch: 10/20, Average loss: 349.9780\n",
      "Epoch: 11/20, Average loss: 349.9259\n",
      "Epoch: 12/20, Average loss: 349.9004\n",
      "Epoch: 13/20, Average loss: 349.8688\n",
      "Epoch: 14/20, Average loss: 349.8592\n",
      "Epoch: 15/20, Average loss: 349.8426\n",
      "Epoch: 16/20, Average loss: 349.8388\n",
      "Epoch: 17/20, Average loss: 349.8324\n",
      "Epoch: 18/20, Average loss: 349.8224\n",
      "Epoch: 19/20, Average loss: 349.8243\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 6 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48cd883",
   "metadata": {},
   "source": [
    "## CoarseGrainMNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1961281",
   "metadata": {},
   "source": [
    "### 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b427f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dd9522",
   "metadata": {},
   "source": [
    "#### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db70be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 289.6801\n",
      "Epoch: 1/15, Average loss: 193.2628\n",
      "Epoch: 2/15, Average loss: 173.1857\n",
      "Epoch: 3/15, Average loss: 166.2264\n",
      "Epoch: 4/15, Average loss: 162.2819\n",
      "Epoch: 5/15, Average loss: 159.5451\n",
      "Epoch: 6/15, Average loss: 157.3774\n",
      "Epoch: 7/15, Average loss: 155.7244\n",
      "Epoch: 8/15, Average loss: 154.3177\n",
      "Epoch: 9/15, Average loss: 153.0372\n",
      "Epoch: 10/15, Average loss: 152.0837\n",
      "Epoch: 11/15, Average loss: 151.2058\n",
      "Epoch: 12/15, Average loss: 150.6141\n",
      "Epoch: 13/15, Average loss: 149.9940\n",
      "Epoch: 14/15, Average loss: 149.4595\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 290.8905\n",
      "Epoch: 1/15, Average loss: 192.6489\n",
      "Epoch: 2/15, Average loss: 171.9591\n",
      "Epoch: 3/15, Average loss: 165.0285\n",
      "Epoch: 4/15, Average loss: 161.3574\n",
      "Epoch: 5/15, Average loss: 158.6042\n",
      "Epoch: 6/15, Average loss: 156.4119\n",
      "Epoch: 7/15, Average loss: 154.2437\n",
      "Epoch: 8/15, Average loss: 152.3802\n",
      "Epoch: 9/15, Average loss: 150.4624\n",
      "Epoch: 10/15, Average loss: 148.9291\n",
      "Epoch: 11/15, Average loss: 147.4861\n",
      "Epoch: 12/15, Average loss: 146.2182\n",
      "Epoch: 13/15, Average loss: 145.1909\n",
      "Epoch: 14/15, Average loss: 144.2715\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 293.8857\n",
      "Epoch: 1/15, Average loss: 195.2444\n",
      "Epoch: 2/15, Average loss: 173.3541\n",
      "Epoch: 3/15, Average loss: 165.8520\n",
      "Epoch: 4/15, Average loss: 161.9161\n",
      "Epoch: 5/15, Average loss: 158.9183\n",
      "Epoch: 6/15, Average loss: 156.7009\n",
      "Epoch: 7/15, Average loss: 154.7471\n",
      "Epoch: 8/15, Average loss: 153.1507\n",
      "Epoch: 9/15, Average loss: 151.4836\n",
      "Epoch: 10/15, Average loss: 150.0993\n",
      "Epoch: 11/15, Average loss: 148.6495\n",
      "Epoch: 12/15, Average loss: 147.4614\n",
      "Epoch: 13/15, Average loss: 146.1274\n",
      "Epoch: 14/15, Average loss: 145.2253\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 298.1447\n",
      "Epoch: 1/15, Average loss: 202.0074\n",
      "Epoch: 2/15, Average loss: 180.3035\n",
      "Epoch: 3/15, Average loss: 172.4941\n",
      "Epoch: 4/15, Average loss: 168.4466\n",
      "Epoch: 5/15, Average loss: 165.8237\n",
      "Epoch: 6/15, Average loss: 163.8447\n",
      "Epoch: 7/15, Average loss: 162.5710\n",
      "Epoch: 8/15, Average loss: 161.3496\n",
      "Epoch: 9/15, Average loss: 160.2445\n",
      "Epoch: 10/15, Average loss: 159.3711\n",
      "Epoch: 11/15, Average loss: 158.4322\n",
      "Epoch: 12/15, Average loss: 157.7368\n",
      "Epoch: 13/15, Average loss: 156.9747\n",
      "Epoch: 14/15, Average loss: 156.2230\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.3239\n",
      "Epoch: 1/15, Average loss: 218.3334\n",
      "Epoch: 2/15, Average loss: 191.2042\n",
      "Epoch: 3/15, Average loss: 180.2265\n",
      "Epoch: 4/15, Average loss: 174.9965\n",
      "Epoch: 5/15, Average loss: 171.8572\n",
      "Epoch: 6/15, Average loss: 169.7922\n",
      "Epoch: 7/15, Average loss: 168.4368\n",
      "Epoch: 8/15, Average loss: 167.1125\n",
      "Epoch: 9/15, Average loss: 165.8994\n",
      "Epoch: 10/15, Average loss: 164.8349\n",
      "Epoch: 11/15, Average loss: 163.8959\n",
      "Epoch: 12/15, Average loss: 163.0649\n",
      "Epoch: 13/15, Average loss: 162.1713\n",
      "Epoch: 14/15, Average loss: 161.3726\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 317.2487\n",
      "Epoch: 1/15, Average loss: 254.0201\n",
      "Epoch: 2/15, Average loss: 250.4007\n",
      "Epoch: 3/15, Average loss: 249.7340\n",
      "Epoch: 4/15, Average loss: 249.5865\n",
      "Epoch: 5/15, Average loss: 249.5347\n",
      "Epoch: 6/15, Average loss: 249.5146\n",
      "Epoch: 7/15, Average loss: 249.5115\n",
      "Epoch: 8/15, Average loss: 249.5062\n",
      "Epoch: 9/15, Average loss: 249.5032\n",
      "Epoch: 10/15, Average loss: 249.5002\n",
      "Epoch: 11/15, Average loss: 249.4957\n",
      "Epoch: 12/15, Average loss: 249.4947\n",
      "Epoch: 13/15, Average loss: 249.4885\n",
      "Epoch: 14/15, Average loss: 249.4943\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6429e3e",
   "metadata": {},
   "source": [
    "#### 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8bad28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 284.8414\n",
      "Epoch: 1/15, Average loss: 183.1887\n",
      "Epoch: 2/15, Average loss: 160.9913\n",
      "Epoch: 3/15, Average loss: 153.3714\n",
      "Epoch: 4/15, Average loss: 149.6740\n",
      "Epoch: 5/15, Average loss: 147.1825\n",
      "Epoch: 6/15, Average loss: 145.0476\n",
      "Epoch: 7/15, Average loss: 143.0832\n",
      "Epoch: 8/15, Average loss: 141.3419\n",
      "Epoch: 9/15, Average loss: 139.7237\n",
      "Epoch: 10/15, Average loss: 138.2031\n",
      "Epoch: 11/15, Average loss: 137.0340\n",
      "Epoch: 12/15, Average loss: 135.8692\n",
      "Epoch: 13/15, Average loss: 135.0475\n",
      "Epoch: 14/15, Average loss: 134.2537\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 287.2679\n",
      "Epoch: 1/15, Average loss: 183.3102\n",
      "Epoch: 2/15, Average loss: 159.8067\n",
      "Epoch: 3/15, Average loss: 151.8794\n",
      "Epoch: 4/15, Average loss: 148.0233\n",
      "Epoch: 5/15, Average loss: 145.5684\n",
      "Epoch: 6/15, Average loss: 143.8484\n",
      "Epoch: 7/15, Average loss: 142.2859\n",
      "Epoch: 8/15, Average loss: 140.7207\n",
      "Epoch: 9/15, Average loss: 139.1885\n",
      "Epoch: 10/15, Average loss: 137.8671\n",
      "Epoch: 11/15, Average loss: 136.5372\n",
      "Epoch: 12/15, Average loss: 135.0640\n",
      "Epoch: 13/15, Average loss: 133.6468\n",
      "Epoch: 14/15, Average loss: 132.2269\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 291.3247\n",
      "Epoch: 1/15, Average loss: 189.4815\n",
      "Epoch: 2/15, Average loss: 165.3948\n",
      "Epoch: 3/15, Average loss: 156.5871\n",
      "Epoch: 4/15, Average loss: 152.1247\n",
      "Epoch: 5/15, Average loss: 149.2069\n",
      "Epoch: 6/15, Average loss: 146.9006\n",
      "Epoch: 7/15, Average loss: 145.2097\n",
      "Epoch: 8/15, Average loss: 143.5446\n",
      "Epoch: 9/15, Average loss: 142.1040\n",
      "Epoch: 10/15, Average loss: 140.8038\n",
      "Epoch: 11/15, Average loss: 139.7880\n",
      "Epoch: 12/15, Average loss: 138.7302\n",
      "Epoch: 13/15, Average loss: 137.7867\n",
      "Epoch: 14/15, Average loss: 136.9561\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 294.4380\n",
      "Epoch: 1/15, Average loss: 197.1466\n",
      "Epoch: 2/15, Average loss: 174.0055\n",
      "Epoch: 3/15, Average loss: 164.6640\n",
      "Epoch: 4/15, Average loss: 159.7816\n",
      "Epoch: 5/15, Average loss: 156.3291\n",
      "Epoch: 6/15, Average loss: 153.7942\n",
      "Epoch: 7/15, Average loss: 151.8755\n",
      "Epoch: 8/15, Average loss: 150.1544\n",
      "Epoch: 9/15, Average loss: 148.9018\n",
      "Epoch: 10/15, Average loss: 147.7834\n",
      "Epoch: 11/15, Average loss: 146.6513\n",
      "Epoch: 12/15, Average loss: 145.7310\n",
      "Epoch: 13/15, Average loss: 144.8953\n",
      "Epoch: 14/15, Average loss: 144.1614\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.0485\n",
      "Epoch: 1/15, Average loss: 208.7481\n",
      "Epoch: 2/15, Average loss: 181.6258\n",
      "Epoch: 3/15, Average loss: 172.1791\n",
      "Epoch: 4/15, Average loss: 167.9994\n",
      "Epoch: 5/15, Average loss: 165.4432\n",
      "Epoch: 6/15, Average loss: 163.2534\n",
      "Epoch: 7/15, Average loss: 161.6258\n",
      "Epoch: 8/15, Average loss: 160.4867\n",
      "Epoch: 9/15, Average loss: 159.4412\n",
      "Epoch: 10/15, Average loss: 158.5491\n",
      "Epoch: 11/15, Average loss: 157.7499\n",
      "Epoch: 12/15, Average loss: 156.9607\n",
      "Epoch: 13/15, Average loss: 156.3114\n",
      "Epoch: 14/15, Average loss: 155.6266\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 315.5198\n",
      "Epoch: 1/15, Average loss: 247.1757\n",
      "Epoch: 2/15, Average loss: 240.7263\n",
      "Epoch: 3/15, Average loss: 249.1617\n",
      "Epoch: 4/15, Average loss: 249.6637\n",
      "Epoch: 5/15, Average loss: 249.6161\n",
      "Epoch: 6/15, Average loss: 249.5454\n",
      "Epoch: 7/15, Average loss: 249.5319\n",
      "Epoch: 8/15, Average loss: 249.5192\n",
      "Epoch: 9/15, Average loss: 249.5209\n",
      "Epoch: 10/15, Average loss: 249.5165\n",
      "Epoch: 11/15, Average loss: 249.5104\n",
      "Epoch: 12/15, Average loss: 249.5086\n",
      "Epoch: 13/15, Average loss: 249.5043\n",
      "Epoch: 14/15, Average loss: 249.5034\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6bd50",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e733da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2224e",
   "metadata": {},
   "source": [
    "#### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6a0514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 133.2970\n",
      "Epoch: 1/15, Average loss: 122.0446\n",
      "Epoch: 2/15, Average loss: 118.3386\n",
      "Epoch: 3/15, Average loss: 116.1568\n",
      "Epoch: 4/15, Average loss: 114.5595\n",
      "Epoch: 5/15, Average loss: 113.3461\n",
      "Epoch: 6/15, Average loss: 112.3307\n",
      "Epoch: 7/15, Average loss: 111.6382\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c2d68",
   "metadata": {},
   "source": [
    "#### 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16aa1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 118.9974\n",
      "Epoch: 1/15, Average loss: 108.9178\n",
      "Epoch: 2/15, Average loss: 105.5291\n",
      "Epoch: 3/15, Average loss: 103.4321\n",
      "Epoch: 4/15, Average loss: 102.0191\n",
      "Epoch: 5/15, Average loss: 100.9225\n",
      "Epoch: 6/15, Average loss: 100.1128\n",
      "Epoch: 7/15, Average loss: 99.4753\n",
      "Epoch: 8/15, Average loss: 98.8654\n",
      "Epoch: 9/15, Average loss: 98.3422\n",
      "Epoch: 10/15, Average loss: 97.9134\n",
      "Epoch: 11/15, Average loss: 97.4991\n",
      "Epoch: 12/15, Average loss: 97.2033\n",
      "Epoch: 13/15, Average loss: 96.9071\n",
      "Epoch: 14/15, Average loss: 96.6482\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 117.3612\n",
      "Epoch: 1/15, Average loss: 107.0709\n",
      "Epoch: 2/15, Average loss: 103.5259\n",
      "Epoch: 3/15, Average loss: 101.2345\n",
      "Epoch: 4/15, Average loss: 99.6001\n",
      "Epoch: 5/15, Average loss: 98.1619\n",
      "Epoch: 6/15, Average loss: 97.1757\n",
      "Epoch: 7/15, Average loss: 96.2539\n",
      "Epoch: 8/15, Average loss: 95.5477\n",
      "Epoch: 9/15, Average loss: 94.8823\n",
      "Epoch: 10/15, Average loss: 94.2359\n",
      "Epoch: 11/15, Average loss: 93.7869\n",
      "Epoch: 12/15, Average loss: 93.3176\n",
      "Epoch: 13/15, Average loss: 92.9119\n",
      "Epoch: 14/15, Average loss: 92.4953\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 122.3740\n",
      "Epoch: 1/15, Average loss: 112.0866\n",
      "Epoch: 2/15, Average loss: 108.6039\n",
      "Epoch: 3/15, Average loss: 106.2506\n",
      "Epoch: 4/15, Average loss: 104.6647\n",
      "Epoch: 5/15, Average loss: 103.3501\n",
      "Epoch: 6/15, Average loss: 102.1215\n",
      "Epoch: 7/15, Average loss: 101.0890\n",
      "Epoch: 8/15, Average loss: 100.2522\n",
      "Epoch: 9/15, Average loss: 99.5271\n",
      "Epoch: 10/15, Average loss: 98.8354\n",
      "Epoch: 11/15, Average loss: 98.1452\n",
      "Epoch: 12/15, Average loss: 97.5811\n",
      "Epoch: 13/15, Average loss: 97.1100\n",
      "Epoch: 14/15, Average loss: 96.5631\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 129.2032\n",
      "Epoch: 1/15, Average loss: 117.8792\n",
      "Epoch: 2/15, Average loss: 114.1404\n",
      "Epoch: 3/15, Average loss: 112.0842\n",
      "Epoch: 4/15, Average loss: 110.5199\n",
      "Epoch: 5/15, Average loss: 109.2949\n",
      "Epoch: 6/15, Average loss: 108.3539\n",
      "Epoch: 7/15, Average loss: 107.4957\n",
      "Epoch: 8/15, Average loss: 106.7806\n",
      "Epoch: 9/15, Average loss: 106.0463\n",
      "Epoch: 10/15, Average loss: 105.5339\n",
      "Epoch: 11/15, Average loss: 104.9863\n",
      "Epoch: 12/15, Average loss: 104.4786\n",
      "Epoch: 13/15, Average loss: 104.0505\n",
      "Epoch: 14/15, Average loss: 103.6725\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 140.3720\n",
      "Epoch: 1/15, Average loss: 127.4048\n",
      "Epoch: 2/15, Average loss: 123.9756\n",
      "Epoch: 3/15, Average loss: 121.8321\n",
      "Epoch: 4/15, Average loss: 120.3979\n",
      "Epoch: 5/15, Average loss: 119.1422\n",
      "Epoch: 6/15, Average loss: 118.0931\n",
      "Epoch: 7/15, Average loss: 117.2640\n",
      "Epoch: 8/15, Average loss: 116.5848\n",
      "Epoch: 9/15, Average loss: 115.8807\n",
      "Epoch: 10/15, Average loss: 115.2746\n",
      "Epoch: 11/15, Average loss: 114.7942\n",
      "Epoch: 12/15, Average loss: 114.2328\n",
      "Epoch: 13/15, Average loss: 113.8319\n",
      "Epoch: 14/15, Average loss: 113.2900\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 205.5973\n",
      "Epoch: 1/15, Average loss: 205.1645\n",
      "Epoch: 2/15, Average loss: 205.9930\n",
      "Epoch: 3/15, Average loss: 205.8193\n",
      "Epoch: 4/15, Average loss: 205.8344\n",
      "Epoch: 5/15, Average loss: 206.0479\n",
      "Epoch: 6/15, Average loss: 206.1282\n",
      "Epoch: 7/15, Average loss: 206.1215\n",
      "Epoch: 8/15, Average loss: 206.1184\n",
      "Epoch: 9/15, Average loss: 206.1139\n",
      "Epoch: 10/15, Average loss: 206.1331\n",
      "Epoch: 11/15, Average loss: 206.1321\n",
      "Epoch: 12/15, Average loss: 206.1303\n",
      "Epoch: 13/15, Average loss: 206.1289\n",
      "Epoch: 14/15, Average loss: 206.0932\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/2MNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f60be0",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd9771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abf7ef",
   "metadata": {},
   "source": [
    "#### 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea6b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 198.2102\n",
      "Epoch: 1/15, Average loss: 182.7440\n",
      "Epoch: 2/15, Average loss: 178.3392\n",
      "Epoch: 3/15, Average loss: 175.9371\n",
      "Epoch: 4/15, Average loss: 174.3433\n",
      "Epoch: 5/15, Average loss: 173.1451\n",
      "Epoch: 6/15, Average loss: 172.2648\n",
      "Epoch: 7/15, Average loss: 171.4805\n",
      "Epoch: 8/15, Average loss: 170.9616\n",
      "Epoch: 9/15, Average loss: 170.4049\n",
      "Epoch: 10/15, Average loss: 169.9646\n",
      "Epoch: 11/15, Average loss: 169.6006\n",
      "Epoch: 12/15, Average loss: 169.3137\n",
      "Epoch: 13/15, Average loss: 168.9561\n",
      "Epoch: 14/15, Average loss: 168.6767\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 191.4708\n",
      "Epoch: 1/15, Average loss: 175.8902\n",
      "Epoch: 2/15, Average loss: 171.7526\n",
      "Epoch: 3/15, Average loss: 169.3644\n",
      "Epoch: 4/15, Average loss: 167.7059\n",
      "Epoch: 5/15, Average loss: 166.3757\n",
      "Epoch: 6/15, Average loss: 165.3819\n",
      "Epoch: 7/15, Average loss: 164.5822\n",
      "Epoch: 8/15, Average loss: 163.8546\n",
      "Epoch: 9/15, Average loss: 163.2931\n",
      "Epoch: 10/15, Average loss: 162.7565\n",
      "Epoch: 11/15, Average loss: 162.2840\n",
      "Epoch: 12/15, Average loss: 161.8526\n",
      "Epoch: 13/15, Average loss: 161.5285\n",
      "Epoch: 14/15, Average loss: 161.1831\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 190.2715\n",
      "Epoch: 1/15, Average loss: 175.0285\n",
      "Epoch: 2/15, Average loss: 171.1064\n",
      "Epoch: 3/15, Average loss: 168.7353\n",
      "Epoch: 4/15, Average loss: 166.9982\n",
      "Epoch: 5/15, Average loss: 165.7492\n",
      "Epoch: 6/15, Average loss: 164.7856\n",
      "Epoch: 7/15, Average loss: 164.0175\n",
      "Epoch: 8/15, Average loss: 163.3324\n",
      "Epoch: 9/15, Average loss: 162.6994\n",
      "Epoch: 10/15, Average loss: 162.0603\n",
      "Epoch: 11/15, Average loss: 161.6777\n",
      "Epoch: 12/15, Average loss: 161.2303\n",
      "Epoch: 13/15, Average loss: 160.8520\n",
      "Epoch: 14/15, Average loss: 160.4095\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 205.7477\n",
      "Epoch: 1/15, Average loss: 187.9862\n",
      "Epoch: 2/15, Average loss: 182.9358\n",
      "Epoch: 3/15, Average loss: 179.8979\n",
      "Epoch: 4/15, Average loss: 177.6414\n",
      "Epoch: 5/15, Average loss: 176.0550\n",
      "Epoch: 6/15, Average loss: 174.5772\n",
      "Epoch: 7/15, Average loss: 173.3865\n",
      "Epoch: 8/15, Average loss: 172.4173\n",
      "Epoch: 9/15, Average loss: 171.5957\n",
      "Epoch: 10/15, Average loss: 170.7571\n",
      "Epoch: 11/15, Average loss: 170.1503\n",
      "Epoch: 12/15, Average loss: 169.4242\n",
      "Epoch: 13/15, Average loss: 168.8996\n",
      "Epoch: 14/15, Average loss: 168.4701\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 209.4241\n",
      "Epoch: 1/15, Average loss: 191.0073\n",
      "Epoch: 2/15, Average loss: 185.6736\n",
      "Epoch: 3/15, Average loss: 182.3992\n",
      "Epoch: 4/15, Average loss: 179.9405\n",
      "Epoch: 5/15, Average loss: 178.3684\n",
      "Epoch: 6/15, Average loss: 176.6909\n",
      "Epoch: 7/15, Average loss: 175.4929\n",
      "Epoch: 8/15, Average loss: 174.4900\n",
      "Epoch: 9/15, Average loss: 173.5293\n",
      "Epoch: 10/15, Average loss: 172.6143\n",
      "Epoch: 11/15, Average loss: 171.9010\n",
      "Epoch: 12/15, Average loss: 171.3949\n",
      "Epoch: 13/15, Average loss: 170.7261\n",
      "Epoch: 14/15, Average loss: 170.2107\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 218.3337\n",
      "Epoch: 1/15, Average loss: 202.8109\n",
      "Epoch: 2/15, Average loss: 197.2647\n",
      "Epoch: 3/15, Average loss: 193.8361\n",
      "Epoch: 4/15, Average loss: 191.9366\n",
      "Epoch: 5/15, Average loss: 189.8902\n",
      "Epoch: 6/15, Average loss: 188.1672\n",
      "Epoch: 7/15, Average loss: 186.7733\n",
      "Epoch: 8/15, Average loss: 185.6743\n",
      "Epoch: 9/15, Average loss: 184.4807\n",
      "Epoch: 10/15, Average loss: 183.5472\n",
      "Epoch: 11/15, Average loss: 182.6977\n",
      "Epoch: 12/15, Average loss: 181.9317\n",
      "Epoch: 13/15, Average loss: 181.2428\n",
      "Epoch: 14/15, Average loss: 180.6594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c241729",
   "metadata": {},
   "source": [
    "#### 15 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44961d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 178.4766\n",
      "Epoch: 1/15, Average loss: 163.7615\n",
      "Epoch: 2/15, Average loss: 159.6368\n",
      "Epoch: 3/15, Average loss: 157.4472\n",
      "Epoch: 4/15, Average loss: 155.9564\n",
      "Epoch: 5/15, Average loss: 154.7375\n",
      "Epoch: 6/15, Average loss: 153.9053\n",
      "Epoch: 7/15, Average loss: 153.1358\n",
      "Epoch: 8/15, Average loss: 152.5865\n",
      "Epoch: 9/15, Average loss: 152.1161\n",
      "Epoch: 10/15, Average loss: 151.6140\n",
      "Epoch: 11/15, Average loss: 151.2042\n",
      "Epoch: 12/15, Average loss: 150.8507\n",
      "Epoch: 13/15, Average loss: 150.5132\n",
      "Epoch: 14/15, Average loss: 150.2438\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 172.4355\n",
      "Epoch: 1/15, Average loss: 158.1250\n",
      "Epoch: 2/15, Average loss: 154.2267\n",
      "Epoch: 3/15, Average loss: 151.8399\n",
      "Epoch: 4/15, Average loss: 150.1762\n",
      "Epoch: 5/15, Average loss: 149.0276\n",
      "Epoch: 6/15, Average loss: 148.0015\n",
      "Epoch: 7/15, Average loss: 147.2197\n",
      "Epoch: 8/15, Average loss: 146.5538\n",
      "Epoch: 9/15, Average loss: 145.9382\n",
      "Epoch: 10/15, Average loss: 145.4336\n",
      "Epoch: 11/15, Average loss: 145.0281\n",
      "Epoch: 12/15, Average loss: 144.6519\n",
      "Epoch: 13/15, Average loss: 144.2280\n",
      "Epoch: 14/15, Average loss: 143.9937\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 179.0610\n",
      "Epoch: 1/15, Average loss: 163.5054\n",
      "Epoch: 2/15, Average loss: 158.9969\n",
      "Epoch: 3/15, Average loss: 156.1494\n",
      "Epoch: 4/15, Average loss: 154.1368\n",
      "Epoch: 5/15, Average loss: 152.7141\n",
      "Epoch: 6/15, Average loss: 151.4204\n",
      "Epoch: 7/15, Average loss: 150.4117\n",
      "Epoch: 8/15, Average loss: 149.6142\n",
      "Epoch: 9/15, Average loss: 148.8934\n",
      "Epoch: 10/15, Average loss: 148.1707\n",
      "Epoch: 11/15, Average loss: 147.5932\n",
      "Epoch: 12/15, Average loss: 147.1064\n",
      "Epoch: 13/15, Average loss: 146.5838\n",
      "Epoch: 14/15, Average loss: 146.1754\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 193.0307\n",
      "Epoch: 1/15, Average loss: 176.4693\n",
      "Epoch: 2/15, Average loss: 171.6324\n",
      "Epoch: 3/15, Average loss: 168.7103\n",
      "Epoch: 4/15, Average loss: 166.3397\n",
      "Epoch: 5/15, Average loss: 164.6631\n",
      "Epoch: 6/15, Average loss: 163.1415\n",
      "Epoch: 7/15, Average loss: 161.9177\n",
      "Epoch: 8/15, Average loss: 160.9073\n",
      "Epoch: 9/15, Average loss: 159.9258\n",
      "Epoch: 10/15, Average loss: 159.0555\n",
      "Epoch: 11/15, Average loss: 158.3024\n",
      "Epoch: 12/15, Average loss: 157.5198\n",
      "Epoch: 13/15, Average loss: 156.9133\n",
      "Epoch: 14/15, Average loss: 156.4554\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 212.4901\n",
      "Epoch: 1/15, Average loss: 192.5207\n",
      "Epoch: 2/15, Average loss: 186.1393\n",
      "Epoch: 3/15, Average loss: 182.4426\n",
      "Epoch: 4/15, Average loss: 179.7289\n",
      "Epoch: 5/15, Average loss: 177.6532\n",
      "Epoch: 6/15, Average loss: 175.9425\n",
      "Epoch: 7/15, Average loss: 174.5435\n",
      "Epoch: 8/15, Average loss: 173.3061\n",
      "Epoch: 9/15, Average loss: 172.2748\n",
      "Epoch: 10/15, Average loss: 171.2507\n",
      "Epoch: 11/15, Average loss: 170.5040\n",
      "Epoch: 12/15, Average loss: 169.6989\n",
      "Epoch: 13/15, Average loss: 168.9785\n",
      "Epoch: 14/15, Average loss: 168.2881\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 262.0850\n",
      "Epoch: 1/15, Average loss: 245.1961\n",
      "Epoch: 2/15, Average loss: 229.1636\n",
      "Epoch: 3/15, Average loss: 219.2737\n",
      "Epoch: 4/15, Average loss: 213.0814\n",
      "Epoch: 5/15, Average loss: 209.7017\n",
      "Epoch: 6/15, Average loss: 208.6066\n",
      "Epoch: 7/15, Average loss: 206.5329\n",
      "Epoch: 8/15, Average loss: 205.3455\n",
      "Epoch: 9/15, Average loss: 204.0234\n",
      "Epoch: 10/15, Average loss: 202.3824\n",
      "Epoch: 11/15, Average loss: 203.2192\n",
      "Epoch: 12/15, Average loss: 203.3701\n",
      "Epoch: 13/15, Average loss: 201.5452\n",
      "Epoch: 14/15, Average loss: 200.0678\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 15, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=15, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/MNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/CoarseGrainMNIST/EMNIST/ld15_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca481267",
   "metadata": {},
   "source": [
    "## FineGrainMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d0052a",
   "metadata": {},
   "source": [
    "### EMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11bc426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab444bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 350.1548\n",
      "Epoch: 1/15, Average loss: 253.1009\n",
      "Epoch: 2/15, Average loss: 220.7762\n",
      "Epoch: 3/15, Average loss: 203.9781\n",
      "Epoch: 4/15, Average loss: 193.5875\n",
      "Epoch: 5/15, Average loss: 186.7863\n",
      "Epoch: 6/15, Average loss: 182.2406\n",
      "Epoch: 7/15, Average loss: 179.1544\n",
      "Epoch: 8/15, Average loss: 176.9054\n",
      "Epoch: 9/15, Average loss: 175.2276\n",
      "Epoch: 10/15, Average loss: 174.0070\n",
      "Epoch: 11/15, Average loss: 172.9156\n",
      "Epoch: 12/15, Average loss: 171.9834\n",
      "Epoch: 13/15, Average loss: 171.3669\n",
      "Epoch: 14/15, Average loss: 170.7154\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 352.0988\n",
      "Epoch: 1/15, Average loss: 251.0358\n",
      "Epoch: 2/15, Average loss: 217.2974\n",
      "Epoch: 3/15, Average loss: 200.0545\n",
      "Epoch: 4/15, Average loss: 189.6971\n",
      "Epoch: 5/15, Average loss: 182.8278\n",
      "Epoch: 6/15, Average loss: 177.8364\n",
      "Epoch: 7/15, Average loss: 174.0655\n",
      "Epoch: 8/15, Average loss: 171.3912\n",
      "Epoch: 9/15, Average loss: 169.0858\n",
      "Epoch: 10/15, Average loss: 167.4931\n",
      "Epoch: 11/15, Average loss: 166.1182\n",
      "Epoch: 12/15, Average loss: 165.0023\n",
      "Epoch: 13/15, Average loss: 164.0743\n",
      "Epoch: 14/15, Average loss: 163.2456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 353.1490\n",
      "Epoch: 1/15, Average loss: 252.2755\n",
      "Epoch: 2/15, Average loss: 218.2468\n",
      "Epoch: 3/15, Average loss: 200.4980\n",
      "Epoch: 4/15, Average loss: 190.2130\n",
      "Epoch: 5/15, Average loss: 183.5315\n",
      "Epoch: 6/15, Average loss: 179.1006\n",
      "Epoch: 7/15, Average loss: 175.7140\n",
      "Epoch: 8/15, Average loss: 173.1074\n",
      "Epoch: 9/15, Average loss: 171.1868\n",
      "Epoch: 10/15, Average loss: 169.6781\n",
      "Epoch: 11/15, Average loss: 168.3934\n",
      "Epoch: 12/15, Average loss: 167.1742\n",
      "Epoch: 13/15, Average loss: 166.2797\n",
      "Epoch: 14/15, Average loss: 165.4711\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 359.6497\n",
      "Epoch: 1/15, Average loss: 256.7990\n",
      "Epoch: 2/15, Average loss: 222.4290\n",
      "Epoch: 3/15, Average loss: 205.5026\n",
      "Epoch: 4/15, Average loss: 196.8224\n",
      "Epoch: 5/15, Average loss: 191.4309\n",
      "Epoch: 6/15, Average loss: 187.7220\n",
      "Epoch: 7/15, Average loss: 184.9673\n",
      "Epoch: 8/15, Average loss: 182.7030\n",
      "Epoch: 9/15, Average loss: 180.6239\n",
      "Epoch: 10/15, Average loss: 178.9949\n",
      "Epoch: 11/15, Average loss: 177.4248\n",
      "Epoch: 12/15, Average loss: 176.2233\n",
      "Epoch: 13/15, Average loss: 175.0779\n",
      "Epoch: 14/15, Average loss: 174.1343\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 359.5036\n",
      "Epoch: 1/15, Average loss: 259.3319\n",
      "Epoch: 2/15, Average loss: 224.9798\n",
      "Epoch: 3/15, Average loss: 206.6746\n",
      "Epoch: 4/15, Average loss: 196.8557\n",
      "Epoch: 5/15, Average loss: 190.7042\n",
      "Epoch: 6/15, Average loss: 186.5188\n",
      "Epoch: 7/15, Average loss: 183.5274\n",
      "Epoch: 8/15, Average loss: 181.0153\n",
      "Epoch: 9/15, Average loss: 179.1720\n",
      "Epoch: 10/15, Average loss: 177.6520\n",
      "Epoch: 11/15, Average loss: 176.4386\n",
      "Epoch: 12/15, Average loss: 175.2991\n",
      "Epoch: 13/15, Average loss: 174.4466\n",
      "Epoch: 14/15, Average loss: 173.5305\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 373.2254\n",
      "Epoch: 1/15, Average loss: 274.5785\n",
      "Epoch: 2/15, Average loss: 244.4096\n",
      "Epoch: 3/15, Average loss: 225.6048\n",
      "Epoch: 4/15, Average loss: 214.5333\n",
      "Epoch: 5/15, Average loss: 207.8231\n",
      "Epoch: 6/15, Average loss: 203.0017\n",
      "Epoch: 7/15, Average loss: 198.5724\n",
      "Epoch: 8/15, Average loss: 195.6498\n",
      "Epoch: 9/15, Average loss: 193.2414\n",
      "Epoch: 10/15, Average loss: 191.0763\n",
      "Epoch: 11/15, Average loss: 189.6097\n",
      "Epoch: 12/15, Average loss: 188.1309\n",
      "Epoch: 13/15, Average loss: 186.6452\n",
      "Epoch: 14/15, Average loss: 185.3618\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63632454",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d843d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1353f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 128.8488\n",
      "Epoch: 1/15, Average loss: 118.9810\n",
      "Epoch: 2/15, Average loss: 115.5914\n",
      "Epoch: 3/15, Average loss: 113.6978\n",
      "Epoch: 4/15, Average loss: 112.2368\n",
      "Epoch: 5/15, Average loss: 111.3334\n",
      "Epoch: 6/15, Average loss: 110.5636\n",
      "Epoch: 7/15, Average loss: 109.7967\n",
      "Epoch: 8/15, Average loss: 109.3749\n",
      "Epoch: 9/15, Average loss: 108.9270\n",
      "Epoch: 10/15, Average loss: 108.5444\n",
      "Epoch: 11/15, Average loss: 108.2124\n",
      "Epoch: 12/15, Average loss: 107.9081\n",
      "Epoch: 13/15, Average loss: 107.7173\n",
      "Epoch: 14/15, Average loss: 107.4304\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 123.7543\n",
      "Epoch: 1/15, Average loss: 113.4090\n",
      "Epoch: 2/15, Average loss: 110.3765\n",
      "Epoch: 3/15, Average loss: 108.4472\n",
      "Epoch: 4/15, Average loss: 107.0033\n",
      "Epoch: 5/15, Average loss: 106.0164\n",
      "Epoch: 6/15, Average loss: 105.1018\n",
      "Epoch: 7/15, Average loss: 104.4306\n",
      "Epoch: 8/15, Average loss: 103.9221\n",
      "Epoch: 9/15, Average loss: 103.5604\n",
      "Epoch: 10/15, Average loss: 103.0635\n",
      "Epoch: 11/15, Average loss: 102.7131\n",
      "Epoch: 12/15, Average loss: 102.3782\n",
      "Epoch: 13/15, Average loss: 102.1153\n",
      "Epoch: 14/15, Average loss: 101.9149\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 125.0447\n",
      "Epoch: 1/15, Average loss: 114.6703\n",
      "Epoch: 2/15, Average loss: 111.3006\n",
      "Epoch: 3/15, Average loss: 109.4904\n",
      "Epoch: 4/15, Average loss: 108.0353\n",
      "Epoch: 5/15, Average loss: 106.9341\n",
      "Epoch: 6/15, Average loss: 105.9661\n",
      "Epoch: 7/15, Average loss: 105.4024\n",
      "Epoch: 8/15, Average loss: 104.9085\n",
      "Epoch: 9/15, Average loss: 104.3502\n",
      "Epoch: 10/15, Average loss: 103.9870\n",
      "Epoch: 11/15, Average loss: 103.5208\n",
      "Epoch: 12/15, Average loss: 103.1821\n",
      "Epoch: 13/15, Average loss: 102.9151\n",
      "Epoch: 14/15, Average loss: 102.5928\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 131.0479\n",
      "Epoch: 1/15, Average loss: 120.4279\n",
      "Epoch: 2/15, Average loss: 117.2300\n",
      "Epoch: 3/15, Average loss: 115.1342\n",
      "Epoch: 4/15, Average loss: 113.6301\n",
      "Epoch: 5/15, Average loss: 112.6094\n",
      "Epoch: 6/15, Average loss: 111.6296\n",
      "Epoch: 7/15, Average loss: 110.8688\n",
      "Epoch: 8/15, Average loss: 110.2754\n",
      "Epoch: 9/15, Average loss: 109.8956\n",
      "Epoch: 10/15, Average loss: 109.2989\n",
      "Epoch: 11/15, Average loss: 108.7665\n",
      "Epoch: 12/15, Average loss: 108.1630\n",
      "Epoch: 13/15, Average loss: 108.0868\n",
      "Epoch: 14/15, Average loss: 107.7894\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 132.3936\n",
      "Epoch: 1/15, Average loss: 121.1099\n",
      "Epoch: 2/15, Average loss: 117.7558\n",
      "Epoch: 3/15, Average loss: 115.5335\n",
      "Epoch: 4/15, Average loss: 114.1492\n",
      "Epoch: 5/15, Average loss: 112.9768\n",
      "Epoch: 6/15, Average loss: 112.2053\n",
      "Epoch: 7/15, Average loss: 111.4527\n",
      "Epoch: 8/15, Average loss: 110.5202\n",
      "Epoch: 9/15, Average loss: 110.0666\n",
      "Epoch: 10/15, Average loss: 109.5930\n",
      "Epoch: 11/15, Average loss: 109.1886\n",
      "Epoch: 12/15, Average loss: 108.6555\n",
      "Epoch: 13/15, Average loss: 108.5454\n",
      "Epoch: 14/15, Average loss: 107.9828\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 140.7163\n",
      "Epoch: 1/15, Average loss: 129.3556\n",
      "Epoch: 2/15, Average loss: 125.7106\n",
      "Epoch: 3/15, Average loss: 123.3617\n",
      "Epoch: 4/15, Average loss: 121.5969\n",
      "Epoch: 5/15, Average loss: 120.2030\n",
      "Epoch: 6/15, Average loss: 119.2593\n",
      "Epoch: 7/15, Average loss: 118.2916\n",
      "Epoch: 8/15, Average loss: 117.4886\n",
      "Epoch: 9/15, Average loss: 116.8479\n",
      "Epoch: 10/15, Average loss: 116.1349\n",
      "Epoch: 11/15, Average loss: 115.5264\n",
      "Epoch: 12/15, Average loss: 115.0161\n",
      "Epoch: 13/15, Average loss: 114.5984\n",
      "Epoch: 14/15, Average loss: 114.2253\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/EMNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a491db66",
   "metadata": {},
   "source": [
    "### 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f57addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b995627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 168.1074\n",
      "Epoch: 1/15, Average loss: 157.9048\n",
      "Epoch: 2/15, Average loss: 154.9265\n",
      "Epoch: 3/15, Average loss: 153.2129\n",
      "Epoch: 4/15, Average loss: 151.9832\n",
      "Epoch: 5/15, Average loss: 150.9830\n",
      "Epoch: 6/15, Average loss: 150.3146\n",
      "Epoch: 7/15, Average loss: 149.7127\n",
      "Epoch: 8/15, Average loss: 149.1699\n",
      "Epoch: 9/15, Average loss: 148.6808\n",
      "Epoch: 10/15, Average loss: 148.3465\n",
      "Epoch: 11/15, Average loss: 147.9847\n",
      "Epoch: 12/15, Average loss: 147.6339\n",
      "Epoch: 13/15, Average loss: 147.2490\n",
      "Epoch: 14/15, Average loss: 146.8601\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 161.4670\n",
      "Epoch: 1/15, Average loss: 150.4652\n",
      "Epoch: 2/15, Average loss: 147.3623\n",
      "Epoch: 3/15, Average loss: 145.4953\n",
      "Epoch: 4/15, Average loss: 144.2479\n",
      "Epoch: 5/15, Average loss: 143.1333\n",
      "Epoch: 6/15, Average loss: 142.4476\n",
      "Epoch: 7/15, Average loss: 141.6326\n",
      "Epoch: 8/15, Average loss: 140.9852\n",
      "Epoch: 9/15, Average loss: 140.5891\n",
      "Epoch: 10/15, Average loss: 140.1519\n",
      "Epoch: 11/15, Average loss: 139.6741\n",
      "Epoch: 12/15, Average loss: 139.1920\n",
      "Epoch: 13/15, Average loss: 138.9380\n",
      "Epoch: 14/15, Average loss: 138.5747\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 163.2038\n",
      "Epoch: 1/15, Average loss: 152.2257\n",
      "Epoch: 2/15, Average loss: 149.0665\n",
      "Epoch: 3/15, Average loss: 147.1936\n",
      "Epoch: 4/15, Average loss: 145.7688\n",
      "Epoch: 5/15, Average loss: 144.5255\n",
      "Epoch: 6/15, Average loss: 143.5279\n",
      "Epoch: 7/15, Average loss: 142.9947\n",
      "Epoch: 8/15, Average loss: 142.1002\n",
      "Epoch: 9/15, Average loss: 141.6346\n",
      "Epoch: 10/15, Average loss: 141.0501\n",
      "Epoch: 11/15, Average loss: 140.6280\n",
      "Epoch: 12/15, Average loss: 140.1687\n",
      "Epoch: 13/15, Average loss: 139.8108\n",
      "Epoch: 14/15, Average loss: 139.3231\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 169.1245\n",
      "Epoch: 1/15, Average loss: 158.5942\n",
      "Epoch: 2/15, Average loss: 155.2144\n",
      "Epoch: 3/15, Average loss: 152.8738\n",
      "Epoch: 4/15, Average loss: 151.2621\n",
      "Epoch: 5/15, Average loss: 149.9865\n",
      "Epoch: 6/15, Average loss: 149.0508\n",
      "Epoch: 7/15, Average loss: 148.3559\n",
      "Epoch: 8/15, Average loss: 147.4163\n",
      "Epoch: 9/15, Average loss: 146.8933\n",
      "Epoch: 10/15, Average loss: 146.3767\n",
      "Epoch: 11/15, Average loss: 145.6512\n",
      "Epoch: 12/15, Average loss: 145.1953\n",
      "Epoch: 13/15, Average loss: 144.8304\n",
      "Epoch: 14/15, Average loss: 144.4839\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 170.9077\n",
      "Epoch: 1/15, Average loss: 159.8842\n",
      "Epoch: 2/15, Average loss: 156.6826\n",
      "Epoch: 3/15, Average loss: 154.8419\n",
      "Epoch: 4/15, Average loss: 153.3940\n",
      "Epoch: 5/15, Average loss: 152.3461\n",
      "Epoch: 6/15, Average loss: 151.2014\n",
      "Epoch: 7/15, Average loss: 150.4247\n",
      "Epoch: 8/15, Average loss: 149.6140\n",
      "Epoch: 9/15, Average loss: 149.0386\n",
      "Epoch: 10/15, Average loss: 148.4432\n",
      "Epoch: 11/15, Average loss: 147.8556\n",
      "Epoch: 12/15, Average loss: 147.3405\n",
      "Epoch: 13/15, Average loss: 146.9495\n",
      "Epoch: 14/15, Average loss: 146.7316\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 176.2220\n",
      "Epoch: 1/15, Average loss: 164.9950\n",
      "Epoch: 2/15, Average loss: 161.8325\n",
      "Epoch: 3/15, Average loss: 159.7446\n",
      "Epoch: 4/15, Average loss: 158.2351\n",
      "Epoch: 5/15, Average loss: 156.9886\n",
      "Epoch: 6/15, Average loss: 156.0197\n",
      "Epoch: 7/15, Average loss: 154.9983\n",
      "Epoch: 8/15, Average loss: 154.4287\n",
      "Epoch: 9/15, Average loss: 153.5127\n",
      "Epoch: 10/15, Average loss: 153.0980\n",
      "Epoch: 11/15, Average loss: 152.2131\n",
      "Epoch: 12/15, Average loss: 151.8847\n",
      "Epoch: 13/15, Average loss: 151.2500\n",
      "Epoch: 14/15, Average loss: 150.9008\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 10, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=10, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "my_model.load_state_dict(torch.load('/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth', map_location=device))\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/FineGrainMNIST/2MNIST/ld10_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1037f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images(my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913cfef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
