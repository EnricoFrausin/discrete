{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d041b96",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512e8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e1f5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import VAE_priorCategorical, VAE_priorHFM\n",
    "import metadata as md\n",
    "from train import train\n",
    "from datasets import Dataset_HFM, Dataset_pureHFM, MNISTDigit2Dataset\n",
    "from utilities import sample_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d58f0132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(md.seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(md.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf1647",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe9c74",
   "metadata": {},
   "source": [
    "## train over ExtendedMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db29123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "train_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_EMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.EMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        split='balanced',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36eef56",
   "metadata": {},
   "source": [
    "## train over 2MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3bba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5958 original samples of digit '2'\n",
      "Generated 60000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n",
      "\n",
      "––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
      "\n",
      "Found 1032 original samples of digit '2'\n",
      "Generated 10000 augmented samples\n",
      "Dataset size: 60000\n",
      "Image shape: torch.Size([1, 28, 28])\n",
      "Label: 2\n",
      "All labels are 2: True\n",
      "Batch images shape: torch.Size([32, 1, 28, 28])\n",
      "Batch labels shape: torch.Size([32])\n",
      "All labels are 2: True\n"
     ]
    }
   ],
   "source": [
    "dataset_2MNIST_train = MNISTDigit2Dataset(train=True, download=True, target_size=60000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "train_loader_2MNIST = DataLoader(dataset_2MNIST_train, batch_size=32, shuffle=True)\n",
    "batch_images, batch_labels = next(iter(train_loader_2MNIST))\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "\n",
    "print(\"\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\")\n",
    "\n",
    "dataset_2MNIST_val = MNISTDigit2Dataset(train=False, download=True, target_size=10000)\n",
    "print(f\"Dataset size: {len(dataset_2MNIST_train)}\")\n",
    "print(f\"Image shape: {dataset_2MNIST_train[0][0].shape}\")\n",
    "print(f\"Label: {dataset_2MNIST_train[0][1]}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")\n",
    "val_loader_2MNIST = DataLoader(dataset_2MNIST_val, batch_size=32, shuffle=True)\n",
    "print(f\"Batch images shape: {batch_images.shape}\")\n",
    "print(f\"Batch labels shape: {batch_labels.shape}\")\n",
    "print(f\"All labels are 2: {torch.all(batch_labels == 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09921586",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_MNIST = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f0bae",
   "metadata": {},
   "source": [
    "## train over FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2094def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "val_loader_FashionMNIST = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST(\n",
    "        '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6682e2",
   "metadata": {},
   "source": [
    "## train over pureHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec3b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_pureHFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM/512features/glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_pureHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2822c7",
   "metadata": {},
   "source": [
    "## train over expandedHFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2fa0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/16_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10db343",
   "metadata": {},
   "source": [
    "## train over expandedHFM 32-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f505c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_HFM_train = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_train60000.pt',\n",
    "                        root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "train_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_train,\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataset_HFM_val = Dataset_HFM(csv_file='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/expandedHFM/32_1024features/2hl_glog2_validation10000.pt',\n",
    "                            root_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/data/pureHFM')\n",
    "\n",
    "val_loader_expandedHFM_32_1024 = DataLoader(\n",
    "    dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "    batch_size=md.batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9adabe",
   "metadata": {},
   "source": [
    "# Prior Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98cd69",
   "metadata": {},
   "source": [
    "## train over Extended_MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c091a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_EMNIST\n",
    "val_loader = val_loader_EMNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6ed9f",
   "metadata": {},
   "source": [
    "\n",
    "### lambda = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 354.4975\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 1/15, Average loss: 257.1719\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 2/15, Average loss: 225.2962\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 3/15, Average loss: 207.9109\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 4/15, Average loss: 197.5804\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 5/15, Average loss: 191.0185\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 6/15, Average loss: 186.4766\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 7/15, Average loss: 183.1223\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 8/15, Average loss: 180.7083\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 9/15, Average loss: 178.7871\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 10/15, Average loss: 177.4256\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 11/15, Average loss: 176.2543\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 12/15, Average loss: 175.2246\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 13/15, Average loss: 174.4136\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Epoch: 14/15, Average loss: 173.7319\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 0/15, Average loss: 357.0172\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 1/15, Average loss: 257.2348\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 2/15, Average loss: 223.6578\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 3/15, Average loss: 206.0207\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 4/15, Average loss: 196.1440\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 5/15, Average loss: 190.0211\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 6/15, Average loss: 186.0779\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 7/15, Average loss: 183.0478\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 8/15, Average loss: 180.7881\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 9/15, Average loss: 178.9309\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 10/15, Average loss: 177.5075\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 11/15, Average loss: 176.4061\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 12/15, Average loss: 175.3630\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 13/15, Average loss: 174.6080\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Epoch: 14/15, Average loss: 173.8146\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 0/15, Average loss: 360.7562\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 1/15, Average loss: 260.2068\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 2/15, Average loss: 227.7190\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 3/15, Average loss: 211.1857\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 4/15, Average loss: 202.2917\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 5/15, Average loss: 196.7209\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 6/15, Average loss: 192.8299\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 7/15, Average loss: 189.9975\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 8/15, Average loss: 187.7652\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 9/15, Average loss: 185.9054\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 10/15, Average loss: 184.5237\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 11/15, Average loss: 183.3227\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 12/15, Average loss: 182.2553\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 13/15, Average loss: 181.2799\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Epoch: 14/15, Average loss: 180.6138\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 0/15, Average loss: 364.7600\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 1/15, Average loss: 262.5306\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 2/15, Average loss: 230.2273\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 3/15, Average loss: 213.7390\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 4/15, Average loss: 204.6073\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 5/15, Average loss: 199.1586\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 6/15, Average loss: 195.4411\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 7/15, Average loss: 192.5253\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 8/15, Average loss: 190.3402\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 9/15, Average loss: 188.4525\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 10/15, Average loss: 186.9982\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 11/15, Average loss: 185.8102\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 12/15, Average loss: 184.7382\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 13/15, Average loss: 184.0085\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Epoch: 14/15, Average loss: 183.2183\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    " #### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/EMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ad702",
   "metadata": {},
   "source": [
    "## train over 2Digit_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16bd9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_2MNIST\n",
    "val_loader = val_loader_2MNIST\n",
    "input_dim = 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c19a7f",
   "metadata": {},
   "source": [
    "### lambda = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b37487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 285.8402\n",
      "Epoch: 1/15, Average loss: 192.3872\n",
      "Epoch: 2/15, Average loss: 173.1508\n",
      "Epoch: 3/15, Average loss: 165.8688\n",
      "Epoch: 4/15, Average loss: 161.6645\n",
      "Epoch: 5/15, Average loss: 158.7423\n",
      "Epoch: 6/15, Average loss: 156.5480\n",
      "Epoch: 7/15, Average loss: 154.9280\n",
      "Epoch: 8/15, Average loss: 153.5633\n",
      "Epoch: 9/15, Average loss: 152.5198\n",
      "Epoch: 10/15, Average loss: 151.5924\n",
      "Epoch: 11/15, Average loss: 150.8315\n",
      "Epoch: 12/15, Average loss: 150.1471\n",
      "Epoch: 13/15, Average loss: 149.6125\n",
      "Epoch: 14/15, Average loss: 149.1944\n",
      "Training completato e dati scritti su tensorboard\n",
      "Epoch: 0/15, Average loss: 287.4816\n",
      "Epoch: 1/15, Average loss: 190.9797\n",
      "Epoch: 2/15, Average loss: 170.6302\n",
      "Epoch: 3/15, Average loss: 163.1861\n",
      "Epoch: 4/15, Average loss: 158.7912\n",
      "Epoch: 5/15, Average loss: 155.4250\n",
      "Epoch: 6/15, Average loss: 152.9431\n",
      "Epoch: 7/15, Average loss: 150.7422\n",
      "Epoch: 8/15, Average loss: 148.8840\n",
      "Epoch: 9/15, Average loss: 147.4339\n",
      "Epoch: 10/15, Average loss: 146.1615\n",
      "Epoch: 11/15, Average loss: 145.0763\n",
      "Epoch: 12/15, Average loss: 144.4174\n",
      "Epoch: 13/15, Average loss: 143.5539\n",
      "Epoch: 14/15, Average loss: 142.8752\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 290.9001\n",
      "Epoch: 1/15, Average loss: 196.5124\n",
      "Epoch: 2/15, Average loss: 173.9782\n",
      "Epoch: 3/15, Average loss: 165.1162\n",
      "Epoch: 4/15, Average loss: 160.2100\n",
      "Epoch: 5/15, Average loss: 156.8794\n",
      "Epoch: 6/15, Average loss: 154.1877\n",
      "Epoch: 7/15, Average loss: 152.0424\n",
      "Epoch: 8/15, Average loss: 150.1410\n",
      "Epoch: 9/15, Average loss: 148.6678\n",
      "Epoch: 10/15, Average loss: 147.4285\n",
      "Epoch: 11/15, Average loss: 146.3758\n",
      "Epoch: 12/15, Average loss: 145.3628\n",
      "Epoch: 13/15, Average loss: 144.6261\n",
      "Epoch: 14/15, Average loss: 143.9215\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 291.8053\n",
      "Epoch: 1/15, Average loss: 198.0273\n",
      "Epoch: 2/15, Average loss: 177.2334\n",
      "Epoch: 3/15, Average loss: 169.5986\n",
      "Epoch: 4/15, Average loss: 165.3874\n",
      "Epoch: 5/15, Average loss: 162.4505\n",
      "Epoch: 6/15, Average loss: 160.1371\n",
      "Epoch: 7/15, Average loss: 158.2938\n",
      "Epoch: 8/15, Average loss: 156.7026\n",
      "Epoch: 9/15, Average loss: 155.3247\n",
      "Epoch: 10/15, Average loss: 154.0009\n",
      "Epoch: 11/15, Average loss: 153.0138\n",
      "Epoch: 12/15, Average loss: 151.8354\n",
      "Epoch: 13/15, Average loss: 151.0117\n",
      "Epoch: 14/15, Average loss: 150.2594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 297.5956\n",
      "Epoch: 1/15, Average loss: 205.3879\n",
      "Epoch: 2/15, Average loss: 185.1229\n",
      "Epoch: 3/15, Average loss: 176.3958\n",
      "Epoch: 4/15, Average loss: 170.8192\n",
      "Epoch: 5/15, Average loss: 167.5699\n",
      "Epoch: 6/15, Average loss: 165.2748\n",
      "Epoch: 7/15, Average loss: 163.5643\n",
      "Epoch: 8/15, Average loss: 162.0262\n",
      "Epoch: 9/15, Average loss: 160.7202\n",
      "Epoch: 10/15, Average loss: 159.7310\n",
      "Epoch: 11/15, Average loss: 158.6797\n",
      "Epoch: 12/15, Average loss: 157.6331\n",
      "Epoch: 13/15, Average loss: 156.6957\n",
      "Epoch: 14/15, Average loss: 156.0575\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 305.9850\n",
      "Epoch: 1/15, Average loss: 233.6360\n",
      "Epoch: 2/15, Average loss: 234.9691\n",
      "Epoch: 3/15, Average loss: 236.2286\n",
      "Epoch: 4/15, Average loss: 235.9187\n",
      "Epoch: 5/15, Average loss: 235.7412\n",
      "Epoch: 6/15, Average loss: 235.6988\n",
      "Epoch: 7/15, Average loss: 235.6745\n",
      "Epoch: 8/15, Average loss: 235.6689\n",
      "Epoch: 9/15, Average loss: 235.6626\n",
      "Epoch: 10/15, Average loss: 235.6579\n",
      "Epoch: 11/15, Average loss: 235.6528\n",
      "Epoch: 12/15, Average loss: 235.6548\n",
      "Epoch: 13/15, Average loss: 235.6487\n",
      "Epoch: 14/15, Average loss: 235.6504\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f5400e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 368.7919\n",
      "Epoch: 1/15, Average loss: 268.6005\n",
      "Epoch: 1/15, Average loss: 268.6005\n",
      "Epoch: 2/15, Average loss: 240.0480\n",
      "Epoch: 2/15, Average loss: 240.0480\n",
      "Epoch: 3/15, Average loss: 224.2530\n",
      "Epoch: 3/15, Average loss: 224.2530\n",
      "Epoch: 4/15, Average loss: 214.7301\n",
      "Epoch: 4/15, Average loss: 214.7301\n",
      "Epoch: 5/15, Average loss: 208.3547\n",
      "Epoch: 5/15, Average loss: 208.3547\n",
      "Epoch: 6/15, Average loss: 204.1407\n",
      "Epoch: 6/15, Average loss: 204.1407\n",
      "Epoch: 7/15, Average loss: 201.0423\n",
      "Epoch: 7/15, Average loss: 201.0423\n",
      "Epoch: 8/15, Average loss: 198.1865\n",
      "Epoch: 8/15, Average loss: 198.1865\n",
      "Epoch: 9/15, Average loss: 196.3216\n",
      "Epoch: 9/15, Average loss: 196.3216\n",
      "Epoch: 10/15, Average loss: 194.5197\n",
      "Epoch: 10/15, Average loss: 194.5197\n",
      "Epoch: 11/15, Average loss: 193.1328\n",
      "Epoch: 11/15, Average loss: 193.1328\n",
      "Epoch: 12/15, Average loss: 191.9021\n",
      "Epoch: 12/15, Average loss: 191.9021\n",
      "Epoch: 13/15, Average loss: 190.4889\n",
      "Epoch: 13/15, Average loss: 190.4889\n",
      "Epoch: 14/15, Average loss: 189.2890\n",
      "Epoch: 14/15, Average loss: 189.2890\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a0c5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 375.6724\n",
      "Epoch: 1/15, Average loss: 288.8285\n",
      "Epoch: 1/15, Average loss: 288.8285\n",
      "Epoch: 2/15, Average loss: 276.6543\n",
      "Epoch: 2/15, Average loss: 276.6543\n",
      "Epoch: 3/15, Average loss: 278.5518\n",
      "Epoch: 3/15, Average loss: 278.5518\n",
      "Epoch: 4/15, Average loss: 278.8145\n",
      "Epoch: 4/15, Average loss: 278.8145\n",
      "Epoch: 5/15, Average loss: 278.2678\n",
      "Epoch: 5/15, Average loss: 278.2678\n",
      "Epoch: 6/15, Average loss: 277.9788\n",
      "Epoch: 6/15, Average loss: 277.9788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=\u001b[32m8\u001b[39m, categorical_dim=\u001b[32m2\u001b[39m, decrease_rate=\u001b[32m0.6\u001b[39m, device=device, num_hidden_layers=\u001b[32m7\u001b[39m, LayerNorm=\u001b[38;5;28;01mTrue\u001b[39;00m).to(device)\n\u001b[32m      5\u001b[39m optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m torch.save(my_model.state_dict(), \u001b[33m'\u001b[39m\u001b[33m/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mParameters saved\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/code/train.py:21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, _lambda, writer, train_loader, val_loader, optimizer, device, epochs, g, calculate_KL_HFM, save_tb_parameters)\u001b[39m\n\u001b[32m     19\u001b[39m optimizer.zero_grad()\n\u001b[32m     20\u001b[39m loss, KL, rec_loss = model(data, temp, _lambda, hard=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m train_loss += loss.item() * \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[32m     23\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/2MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7130fe",
   "metadata": {},
   "source": [
    "## train over MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b489f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_MNIST\n",
    "val_loader = val_loader_MNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbdaa5",
   "metadata": {},
   "source": [
    "### lambda = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e4983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 386.3299\n",
      "Epoch: 1/15, Average loss: 269.6630\n",
      "Epoch: 2/15, Average loss: 220.6156\n",
      "Epoch: 3/15, Average loss: 192.3674\n",
      "Epoch: 4/15, Average loss: 172.4641\n",
      "Epoch: 5/15, Average loss: 158.6117\n",
      "Epoch: 6/15, Average loss: 148.8546\n",
      "Epoch: 7/15, Average loss: 141.7337\n",
      "Epoch: 8/15, Average loss: 136.4395\n",
      "Epoch: 9/15, Average loss: 132.2159\n",
      "Epoch: 10/15, Average loss: 128.9444\n",
      "Epoch: 11/15, Average loss: 126.1562\n",
      "Epoch: 12/15, Average loss: 123.7926\n",
      "Epoch: 13/15, Average loss: 121.9730\n",
      "Epoch: 14/15, Average loss: 120.2335\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 386.8003\n",
      "Epoch: 1/15, Average loss: 270.2383\n",
      "Epoch: 2/15, Average loss: 221.5904\n",
      "Epoch: 3/15, Average loss: 193.3287\n",
      "Epoch: 4/15, Average loss: 173.1111\n",
      "Epoch: 5/15, Average loss: 158.6518\n",
      "Epoch: 6/15, Average loss: 148.4285\n",
      "Epoch: 7/15, Average loss: 140.9530\n",
      "Epoch: 8/15, Average loss: 135.4636\n",
      "Epoch: 9/15, Average loss: 131.2472\n",
      "Epoch: 10/15, Average loss: 127.9704\n",
      "Epoch: 11/15, Average loss: 125.2929\n",
      "Epoch: 12/15, Average loss: 123.1115\n",
      "Epoch: 13/15, Average loss: 121.3816\n",
      "Epoch: 14/15, Average loss: 119.8358\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 388.8020\n",
      "Epoch: 1/15, Average loss: 271.3259\n",
      "Epoch: 2/15, Average loss: 222.7423\n",
      "Epoch: 3/15, Average loss: 194.5868\n",
      "Epoch: 4/15, Average loss: 174.3165\n",
      "Epoch: 5/15, Average loss: 160.1383\n",
      "Epoch: 6/15, Average loss: 150.3916\n",
      "Epoch: 7/15, Average loss: 143.2736\n",
      "Epoch: 8/15, Average loss: 137.8577\n",
      "Epoch: 9/15, Average loss: 133.7821\n",
      "Epoch: 10/15, Average loss: 130.6661\n",
      "Epoch: 11/15, Average loss: 128.2295\n",
      "Epoch: 12/15, Average loss: 126.0127\n",
      "Epoch: 13/15, Average loss: 124.4271\n",
      "Epoch: 14/15, Average loss: 123.0456\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST//ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c328525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 401.4765\n",
      "Epoch: 1/15, Average loss: 280.9602\n",
      "Epoch: 1/15, Average loss: 280.9602\n",
      "Epoch: 2/15, Average loss: 233.9060\n",
      "Epoch: 2/15, Average loss: 233.9060\n",
      "Epoch: 3/15, Average loss: 208.7653\n",
      "Epoch: 3/15, Average loss: 208.7653\n",
      "Epoch: 4/15, Average loss: 189.5505\n",
      "Epoch: 4/15, Average loss: 189.5505\n",
      "Epoch: 5/15, Average loss: 175.1715\n",
      "Epoch: 5/15, Average loss: 175.1715\n",
      "Epoch: 6/15, Average loss: 164.4091\n",
      "Epoch: 6/15, Average loss: 164.4091\n",
      "Epoch: 7/15, Average loss: 156.1525\n",
      "Epoch: 7/15, Average loss: 156.1525\n",
      "Epoch: 8/15, Average loss: 149.9218\n",
      "Epoch: 8/15, Average loss: 149.9218\n",
      "Epoch: 9/15, Average loss: 144.9282\n",
      "Epoch: 9/15, Average loss: 144.9282\n",
      "Epoch: 10/15, Average loss: 141.4847\n",
      "Epoch: 10/15, Average loss: 141.4847\n",
      "Epoch: 11/15, Average loss: 138.7908\n",
      "Epoch: 11/15, Average loss: 138.7908\n",
      "Epoch: 12/15, Average loss: 136.6273\n",
      "Epoch: 12/15, Average loss: 136.6273\n",
      "Epoch: 13/15, Average loss: 134.9579\n",
      "Epoch: 13/15, Average loss: 134.9579\n",
      "Epoch: 14/15, Average loss: 133.7436\n",
      "Epoch: 14/15, Average loss: 133.7436\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 7\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=7, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_NL_7hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 402.5911\n",
      "Epoch: 1/15, Average loss: 275.8961\n",
      "Epoch: 2/15, Average loss: 226.4554\n",
      "Epoch: 3/15, Average loss: 198.4908\n",
      "Epoch: 4/15, Average loss: 177.8793\n",
      "Epoch: 5/15, Average loss: 163.3509\n",
      "Epoch: 6/15, Average loss: 153.2155\n",
      "Epoch: 7/15, Average loss: 146.2363\n",
      "Epoch: 8/15, Average loss: 141.0705\n",
      "Epoch: 9/15, Average loss: 137.0449\n",
      "Epoch: 10/15, Average loss: 134.0465\n",
      "Epoch: 11/15, Average loss: 131.8412\n",
      "Epoch: 12/15, Average loss: 130.0399\n",
      "Epoch: 13/15, Average loss: 128.2418\n",
      "Epoch: 14/15, Average loss: 127.0624\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac8c5d",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7f0dc",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b53d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 389.5810\n",
      "Epoch: 1/15, Average loss: 272.9245\n",
      "Epoch: 2/15, Average loss: 224.3924\n",
      "Epoch: 3/15, Average loss: 196.1950\n",
      "Epoch: 4/15, Average loss: 175.8703\n",
      "Epoch: 5/15, Average loss: 161.4573\n",
      "Epoch: 6/15, Average loss: 151.1268\n",
      "Epoch: 7/15, Average loss: 143.5675\n",
      "Epoch: 8/15, Average loss: 137.9279\n",
      "Epoch: 9/15, Average loss: 133.4935\n",
      "Epoch: 10/15, Average loss: 130.0795\n",
      "Epoch: 11/15, Average loss: 127.3539\n",
      "Epoch: 12/15, Average loss: 125.0256\n",
      "Epoch: 13/15, Average loss: 123.2869\n",
      "Epoch: 14/15, Average loss: 121.7311\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 396.2289\n",
      "Epoch: 1/15, Average loss: 274.0455\n",
      "Epoch: 2/15, Average loss: 225.6387\n",
      "Epoch: 3/15, Average loss: 197.5839\n",
      "Epoch: 4/15, Average loss: 177.1822\n",
      "Epoch: 5/15, Average loss: 162.4309\n",
      "Epoch: 6/15, Average loss: 152.1034\n",
      "Epoch: 7/15, Average loss: 144.3967\n",
      "Epoch: 8/15, Average loss: 138.8854\n",
      "Epoch: 9/15, Average loss: 134.5667\n",
      "Epoch: 10/15, Average loss: 131.3657\n",
      "Epoch: 11/15, Average loss: 128.9836\n",
      "Epoch: 12/15, Average loss: 126.8194\n",
      "Epoch: 13/15, Average loss: 125.1062\n",
      "Epoch: 14/15, Average loss: 123.7093\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 401.6955\n",
      "Epoch: 1/15, Average loss: 277.8787\n",
      "Epoch: 2/15, Average loss: 231.1103\n",
      "Epoch: 3/15, Average loss: 205.0764\n",
      "Epoch: 4/15, Average loss: 184.1718\n",
      "Epoch: 5/15, Average loss: 169.0195\n",
      "Epoch: 6/15, Average loss: 158.4487\n",
      "Epoch: 7/15, Average loss: 151.1160\n",
      "Epoch: 8/15, Average loss: 145.3018\n",
      "Epoch: 9/15, Average loss: 140.4248\n",
      "Epoch: 10/15, Average loss: 137.0231\n",
      "Epoch: 11/15, Average loss: 134.2711\n",
      "Epoch: 12/15, Average loss: 132.0752\n",
      "Epoch: 13/15, Average loss: 130.4365\n",
      "Epoch: 14/15, Average loss: 129.0076\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 406.0553\n",
      "Epoch: 1/15, Average loss: 286.5170\n",
      "Epoch: 2/15, Average loss: 245.0898\n",
      "Epoch: 3/15, Average loss: 228.9989\n",
      "Epoch: 4/15, Average loss: 220.7183\n",
      "Epoch: 5/15, Average loss: 215.8021\n",
      "Epoch: 6/15, Average loss: 212.7106\n",
      "Epoch: 7/15, Average loss: 210.7006\n",
      "Epoch: 8/15, Average loss: 209.3399\n",
      "Epoch: 9/15, Average loss: 208.4070\n",
      "Epoch: 10/15, Average loss: 207.7568\n",
      "Epoch: 11/15, Average loss: 207.2948\n",
      "Epoch: 12/15, Average loss: 206.9638\n",
      "Epoch: 13/15, Average loss: 206.7283\n",
      "Epoch: 14/15, Average loss: 206.5594\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n",
      "Epoch: 0/15, Average loss: 406.0930\n",
      "Epoch: 1/15, Average loss: 286.7583\n",
      "Epoch: 2/15, Average loss: 245.0405\n",
      "Epoch: 3/15, Average loss: 228.9681\n",
      "Epoch: 4/15, Average loss: 220.7014\n",
      "Epoch: 5/15, Average loss: 215.8057\n",
      "Epoch: 6/15, Average loss: 212.7268\n",
      "Epoch: 7/15, Average loss: 210.7016\n",
      "Epoch: 8/15, Average loss: 209.3477\n",
      "Epoch: 9/15, Average loss: 208.4078\n",
      "Epoch: 10/15, Average loss: 207.7578\n",
      "Epoch: 11/15, Average loss: 207.2985\n",
      "Epoch: 12/15, Average loss: 206.9670\n",
      "Epoch: 13/15, Average loss: 206.7290\n",
      "Epoch: 14/15, Average loss: 206.5562\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06af107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788d8b3",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8290d",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde285d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e003fb",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc35d91",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a01306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291bead4",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12468f72",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a69ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c69b4",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967eb3e",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7436bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71befb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9bb83",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061a6c3",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbfb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c33bc",
   "metadata": {},
   "source": [
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefce547",
   "metadata": {},
   "source": [
    "**Oss**: Il training con $\\lambda = 1$ per i layers 5 e 6 impedisce l'addestramento corretto dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr055_gKLlog2_NL_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb1_dr06_gKLlog2_NL_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This has been made to check the validity of the kl for 5 hidden layers, as it seems to be an aberrant value.\n",
    "# Magically, it fixed itself.\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_Categorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1')\n",
    "my_model = VAE_priorCategorical(input_dim=input_dim, latent_dim=8, categorical_dim=2, decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, _lambda=0.1)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorCategorical/MNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_NL_5hl_1.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fab93e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_FashionMNIST\n",
    "val_loader = val_loader_FashionMNIST\n",
    "input_dim = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0e04e",
   "metadata": {},
   "source": [
    "### latent_dim = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7328867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 390.5580\n",
      "Epoch: 1/15, Average loss: 328.8307\n",
      "Epoch: 2/15, Average loss: 301.4959\n",
      "Epoch: 3/15, Average loss: 282.8894\n",
      "Epoch: 4/15, Average loss: 270.0157\n",
      "Epoch: 5/15, Average loss: 261.8850\n",
      "Epoch: 6/15, Average loss: 256.6333\n",
      "Epoch: 7/15, Average loss: 252.8290\n",
      "Epoch: 8/15, Average loss: 250.1731\n",
      "Epoch: 9/15, Average loss: 248.1693\n",
      "Epoch: 10/15, Average loss: 246.2765\n",
      "Epoch: 11/15, Average loss: 244.8467\n",
      "Epoch: 12/15, Average loss: 243.6671\n",
      "Epoch: 13/15, Average loss: 242.5018\n",
      "Epoch: 14/15, Average loss: 241.4765\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17430dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 390.7023\n",
      "Epoch: 1/15, Average loss: 329.8947\n",
      "Epoch: 2/15, Average loss: 301.9531\n",
      "Epoch: 3/15, Average loss: 283.2363\n",
      "Epoch: 4/15, Average loss: 270.1758\n",
      "Epoch: 5/15, Average loss: 261.6072\n",
      "Epoch: 6/15, Average loss: 256.2617\n",
      "Epoch: 7/15, Average loss: 252.4125\n",
      "Epoch: 8/15, Average loss: 249.4742\n",
      "Epoch: 9/15, Average loss: 247.0539\n",
      "Epoch: 10/15, Average loss: 245.2985\n",
      "Epoch: 11/15, Average loss: 243.9185\n",
      "Epoch: 12/15, Average loss: 242.5683\n",
      "Epoch: 13/15, Average loss: 241.5314\n",
      "Epoch: 14/15, Average loss: 240.5676\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e0e285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 392.7611\n",
      "Epoch: 1/15, Average loss: 332.3344\n",
      "Epoch: 2/15, Average loss: 304.8491\n",
      "Epoch: 3/15, Average loss: 285.1961\n",
      "Epoch: 4/15, Average loss: 272.0609\n",
      "Epoch: 5/15, Average loss: 263.5342\n",
      "Epoch: 6/15, Average loss: 257.9186\n",
      "Epoch: 7/15, Average loss: 253.8324\n",
      "Epoch: 8/15, Average loss: 250.7719\n",
      "Epoch: 9/15, Average loss: 248.5465\n",
      "Epoch: 10/15, Average loss: 246.4548\n",
      "Epoch: 11/15, Average loss: 244.7143\n",
      "Epoch: 12/15, Average loss: 243.6010\n",
      "Epoch: 13/15, Average loss: 242.4122\n",
      "Epoch: 14/15, Average loss: 241.4052\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cfb242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 396.8559\n",
      "Epoch: 1/15, Average loss: 334.7537\n",
      "Epoch: 2/15, Average loss: 309.1033\n",
      "Epoch: 3/15, Average loss: 288.9122\n",
      "Epoch: 4/15, Average loss: 274.7021\n",
      "Epoch: 5/15, Average loss: 265.8321\n",
      "Epoch: 6/15, Average loss: 259.9738\n",
      "Epoch: 7/15, Average loss: 256.1328\n",
      "Epoch: 8/15, Average loss: 252.9517\n",
      "Epoch: 9/15, Average loss: 250.8496\n",
      "Epoch: 10/15, Average loss: 249.0158\n",
      "Epoch: 11/15, Average loss: 247.4768\n",
      "Epoch: 12/15, Average loss: 246.1288\n",
      "Epoch: 13/15, Average loss: 245.3457\n",
      "Epoch: 14/15, Average loss: 244.0072\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 12, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=12, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld12_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee527f",
   "metadata": {},
   "source": [
    "\n",
    "### latent_dim = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335bf3c",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a60ff162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 393.8527\n",
      "Epoch: 1/15, Average loss: 333.0312\n",
      "Epoch: 2/15, Average loss: 307.6510\n",
      "Epoch: 3/15, Average loss: 290.6462\n",
      "Epoch: 4/15, Average loss: 277.5916\n",
      "Epoch: 5/15, Average loss: 268.8914\n",
      "Epoch: 6/15, Average loss: 263.5379\n",
      "Epoch: 7/15, Average loss: 259.7359\n",
      "Epoch: 8/15, Average loss: 256.8149\n",
      "Epoch: 9/15, Average loss: 254.4613\n",
      "Epoch: 10/15, Average loss: 252.5904\n",
      "Epoch: 11/15, Average loss: 250.8536\n",
      "Epoch: 12/15, Average loss: 249.6486\n",
      "Epoch: 13/15, Average loss: 248.5295\n",
      "Epoch: 14/15, Average loss: 247.4250\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep7_lmb01_dr05_gKLlog2_LN_1hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6d532d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf09cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 394.6638\n",
      "Epoch: 1/15, Average loss: 333.7562\n",
      "Epoch: 2/15, Average loss: 307.5900\n",
      "Epoch: 3/15, Average loss: 289.5058\n",
      "Epoch: 4/15, Average loss: 276.9208\n",
      "Epoch: 5/15, Average loss: 268.5197\n",
      "Epoch: 6/15, Average loss: 262.9799\n",
      "Epoch: 7/15, Average loss: 259.2604\n",
      "Epoch: 8/15, Average loss: 256.2368\n",
      "Epoch: 9/15, Average loss: 253.8237\n",
      "Epoch: 10/15, Average loss: 251.8655\n",
      "Epoch: 11/15, Average loss: 250.1550\n",
      "Epoch: 12/15, Average loss: 248.6765\n",
      "Epoch: 13/15, Average loss: 247.2953\n",
      "Epoch: 14/15, Average loss: 246.0844\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8d3169",
   "metadata": {},
   "source": [
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layer = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "078fdb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15, Average loss: 397.8299\n",
      "Epoch: 1/15, Average loss: 335.6916\n",
      "Epoch: 2/15, Average loss: 310.4764\n",
      "Epoch: 3/15, Average loss: 292.3859\n",
      "Epoch: 4/15, Average loss: 278.5663\n",
      "Epoch: 5/15, Average loss: 269.3754\n",
      "Epoch: 6/15, Average loss: 263.2122\n",
      "Epoch: 7/15, Average loss: 258.7293\n",
      "Epoch: 8/15, Average loss: 255.4832\n",
      "Epoch: 9/15, Average loss: 253.1279\n",
      "Epoch: 10/15, Average loss: 250.9017\n",
      "Epoch: 11/15, Average loss: 249.3471\n",
      "Epoch: 12/15, Average loss: 247.9899\n",
      "Epoch: 13/15, Average loss: 246.8600\n",
      "Epoch: 14/15, Average loss: 245.4575\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_NL_3hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269280c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.5, g_HFM per KL = log2, hidden layers = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea10f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr05_gKLlog2_LN_4hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.55, g_HFM per KL = log2, hidden layers = 5\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.55, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr055_gKLlog2_LN_5hl_0.pth')\n",
    "print('Parameters saved')\n",
    "\n",
    "\n",
    "#### features = 8, g_model = log(2), epochs = 15, decrease_rate = 0.6, g_HFM per KL = log2, hidden layers = 6\n",
    "\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=8, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/FashionMNIST/ld8_glog2_ep15_lmb01_dr06_gKLlog2_LN_6hl_0.pth')\n",
    "print('Parameters saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161bc2f",
   "metadata": {},
   "source": [
    "## train over expandedHFM 16-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a60380",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = train_loader_expandedHFM\n",
    "val_loader = val_loader_expandedHFM\n",
    "input_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b4f7f",
   "metadata": {},
   "source": [
    "### latent_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4ab71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 369.4557\n",
      "Epoch: 1/20, Average loss: 253.0922\n",
      "Epoch: 2/20, Average loss: 202.8551\n",
      "Epoch: 3/20, Average loss: 172.7191\n",
      "Epoch: 4/20, Average loss: 152.2356\n",
      "Epoch: 5/20, Average loss: 137.6990\n",
      "Epoch: 6/20, Average loss: 126.6692\n",
      "Epoch: 7/20, Average loss: 118.0560\n",
      "Epoch: 8/20, Average loss: 111.1311\n",
      "Epoch: 9/20, Average loss: 105.9597\n",
      "Epoch: 10/20, Average loss: 101.8351\n",
      "Epoch: 11/20, Average loss: 98.4729\n",
      "Epoch: 12/20, Average loss: 95.4016\n",
      "Epoch: 13/20, Average loss: 92.9106\n",
      "Epoch: 14/20, Average loss: 90.9402\n",
      "Epoch: 15/20, Average loss: 89.1270\n",
      "Epoch: 16/20, Average loss: 87.6784\n",
      "Epoch: 17/20, Average loss: 86.5530\n",
      "Epoch: 18/20, Average loss: 85.3882\n",
      "Epoch: 19/20, Average loss: 84.4254\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=1, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_1hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c6e5b3",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 2, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b165db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 393.1789\n",
      "Epoch: 1/20, Average loss: 300.7643\n",
      "Epoch: 2/20, Average loss: 241.3813\n",
      "Epoch: 3/20, Average loss: 207.7646\n",
      "Epoch: 4/20, Average loss: 179.9620\n",
      "Epoch: 5/20, Average loss: 160.1453\n",
      "Epoch: 6/20, Average loss: 144.9132\n",
      "Epoch: 7/20, Average loss: 133.3975\n",
      "Epoch: 8/20, Average loss: 125.1795\n",
      "Epoch: 9/20, Average loss: 118.3757\n",
      "Epoch: 10/20, Average loss: 112.1573\n",
      "Epoch: 11/20, Average loss: 107.3208\n",
      "Epoch: 12/20, Average loss: 103.3839\n",
      "Epoch: 13/20, Average loss: 99.9120\n",
      "Epoch: 14/20, Average loss: 96.7396\n",
      "Epoch: 15/20, Average loss: 94.4459\n",
      "Epoch: 16/20, Average loss: 92.6706\n",
      "Epoch: 17/20, Average loss: 90.7141\n",
      "Epoch: 18/20, Average loss: 88.7004\n",
      "Epoch: 19/20, Average loss: 87.0102\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=2, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_2hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ce1eed",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 3, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea231cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 399.2147\n",
      "Epoch: 1/20, Average loss: 324.6712\n",
      "Epoch: 2/20, Average loss: 275.4050\n",
      "Epoch: 3/20, Average loss: 239.8818\n",
      "Epoch: 4/20, Average loss: 216.3649\n",
      "Epoch: 5/20, Average loss: 198.0273\n",
      "Epoch: 6/20, Average loss: 182.2649\n",
      "Epoch: 7/20, Average loss: 170.8574\n",
      "Epoch: 8/20, Average loss: 162.9704\n",
      "Epoch: 9/20, Average loss: 154.4810\n",
      "Epoch: 10/20, Average loss: 146.7258\n",
      "Epoch: 11/20, Average loss: 139.4719\n",
      "Epoch: 12/20, Average loss: 134.2167\n",
      "Epoch: 13/20, Average loss: 129.6427\n",
      "Epoch: 14/20, Average loss: 125.4191\n",
      "Epoch: 15/20, Average loss: 121.2396\n",
      "Epoch: 16/20, Average loss: 118.2277\n",
      "Epoch: 17/20, Average loss: 115.6689\n",
      "Epoch: 18/20, Average loss: 112.8884\n",
      "Epoch: 19/20, Average loss: 111.4160\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=3, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_3hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde52c1d",
   "metadata": {},
   "source": [
    "#### features = 32, g_model = log(2), epochs = 20, hidden layers = 4, decrease_rate = 0.5, g_HFM per KL = log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47314339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 405.6318\n",
      "Epoch: 1/20, Average loss: 331.2717\n",
      "Epoch: 2/20, Average loss: 293.0868\n",
      "Epoch: 3/20, Average loss: 260.4425\n",
      "Epoch: 4/20, Average loss: 235.3888\n",
      "Epoch: 5/20, Average loss: 217.5635\n",
      "Epoch: 6/20, Average loss: 207.8059\n",
      "Epoch: 7/20, Average loss: 200.4518\n",
      "Epoch: 8/20, Average loss: 194.3792\n",
      "Epoch: 9/20, Average loss: 189.3980\n",
      "Epoch: 10/20, Average loss: 184.9584\n",
      "Epoch: 11/20, Average loss: 179.7132\n",
      "Epoch: 12/20, Average loss: 174.9633\n",
      "Epoch: 13/20, Average loss: 170.7941\n",
      "Epoch: 14/20, Average loss: 167.3850\n",
      "Epoch: 15/20, Average loss: 164.2246\n",
      "Epoch: 16/20, Average loss: 161.3431\n",
      "Epoch: 17/20, Average loss: 159.5106\n",
      "Epoch: 18/20, Average loss: 156.5435\n",
      "Epoch: 19/20, Average loss: 154.0239\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/runs/discrete_VAE/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=4, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_4hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26814b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.6061\n",
      "Epoch: 1/20, Average loss: 368.8077\n",
      "Epoch: 2/20, Average loss: 357.9416\n",
      "Epoch: 3/20, Average loss: 353.8225\n",
      "Epoch: 4/20, Average loss: 351.9900\n",
      "Epoch: 5/20, Average loss: 351.0825\n",
      "Epoch: 6/20, Average loss: 350.5930\n",
      "Epoch: 7/20, Average loss: 350.3083\n",
      "Epoch: 8/20, Average loss: 350.1508\n",
      "Epoch: 9/20, Average loss: 350.0325\n",
      "Epoch: 10/20, Average loss: 349.9596\n",
      "Epoch: 11/20, Average loss: 349.9249\n",
      "Epoch: 12/20, Average loss: 349.8968\n",
      "Epoch: 13/20, Average loss: 349.8726\n",
      "Epoch: 14/20, Average loss: 349.8558\n",
      "Epoch: 15/20, Average loss: 349.8398\n",
      "Epoch: 16/20, Average loss: 349.8369\n",
      "Epoch: 17/20, Average loss: 349.8276\n",
      "Epoch: 18/20, Average loss: 349.8294\n",
      "Epoch: 19/20, Average loss: 349.8175\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 5 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.5, device=device, num_hidden_layers=5, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr05_gKLlog2_5hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be372af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/20, Average loss: 408.4123\n",
      "Epoch: 1/20, Average loss: 369.0100\n",
      "Epoch: 2/20, Average loss: 357.9196\n",
      "Epoch: 3/20, Average loss: 353.9314\n",
      "Epoch: 4/20, Average loss: 351.9628\n",
      "Epoch: 5/20, Average loss: 351.0653\n",
      "Epoch: 6/20, Average loss: 350.5855\n",
      "Epoch: 7/20, Average loss: 350.3135\n",
      "Epoch: 8/20, Average loss: 350.1441\n",
      "Epoch: 9/20, Average loss: 350.0360\n",
      "Epoch: 10/20, Average loss: 349.9780\n",
      "Epoch: 11/20, Average loss: 349.9259\n",
      "Epoch: 12/20, Average loss: 349.9004\n",
      "Epoch: 13/20, Average loss: 349.8688\n",
      "Epoch: 14/20, Average loss: 349.8592\n",
      "Epoch: 15/20, Average loss: 349.8426\n",
      "Epoch: 16/20, Average loss: 349.8388\n",
      "Epoch: 17/20, Average loss: 349.8324\n",
      "Epoch: 18/20, Average loss: 349.8224\n",
      "Epoch: 19/20, Average loss: 349.8243\n",
      "Training completato e dati scritti su tensorboard\n",
      "Parameters saved\n"
     ]
    }
   ],
   "source": [
    "# 6 hidden layers\n",
    "writer = SummaryWriter(log_dir='/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/runs/prior_HFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0')\n",
    "my_model = VAE_priorHFM(input_dim=input_dim, latent_dim=32, g=np.log(2), decrease_rate=0.6, device=device, num_hidden_layers=6, LayerNorm=True).to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=md.learning_rate)\n",
    "train(my_model, _lambda=0.1, writer=writer, train_loader=train_loader, val_loader=val_loader, optimizer=optimizer, device=device, epochs=15, calculate_KL_HFM=True)\n",
    "torch.save(my_model.state_dict(), '/Users/enricofrausin/Programmazione/PythonProjects/Fisica/Architetture/VAE/discrete/models_parameters/priorHFM/expandedHFM/ld32_glog2_ep15_lmb01_dr06_gKLlog2_6hl_0.pth')\n",
    "print('Parameters saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
