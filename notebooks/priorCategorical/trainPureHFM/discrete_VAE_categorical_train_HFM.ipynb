{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae25902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44d50636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60468007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(512 * (7/8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160137ab",
   "metadata": {},
   "source": [
    "# Metadata e inizializzazione dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c58c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 128\n",
    "temperature = 1.0\n",
    "seed = 0\n",
    "log_interval = 600\n",
    "log_interval_writer = 100\n",
    "hard = False\n",
    "latent_dim = 15\n",
    "categorical_dim = 2\n",
    "temp_min = 0.5\n",
    "ANNEAL_RATE = 0.00003\n",
    "n_start = 512\n",
    "num_initial_bits = 512\n",
    "\n",
    "g = np.log(2)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device.type == \"cuda\" or device.type == \"mps\" else {} # pin_memory puÃ² essere utile anche per MPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289e367",
   "metadata": {},
   "source": [
    "# Gumbel-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf79c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    # sample from a uniform distribution\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U.to(device) + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "    \n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # skip the gradient of y_hard\n",
    "    y_hard = (y_hard - y).detach() + y \n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f19ee0",
   "metadata": {},
   "source": [
    "# Class VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e51b88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, latent_dim * categorical_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim * categorical_dim, 128)\n",
    "        self.fc5 = nn.Linear(128, 256)\n",
    "        self.fc6 = nn.Linear(256, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def sample_img(self, img, temp, random=True):\n",
    "        with torch.no_grad():\n",
    "            logits_z = self.encode(img.view(-1, num_initial_bits))\n",
    "            logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n",
    "            if random:\n",
    "                latent_z = gumbel_softmax(logits_z, temp, True)\n",
    "            else:\n",
    "                latent_z = logits_z.view(-1, latent_dim * categorical_dim)\n",
    "            logits_x = self.decode(latent_z)\n",
    "            dist_x = torch.distributions.Bernoulli(probs=logits_x)\n",
    "            sampled_img = dist_x.sample()\n",
    "        return sampled_img\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        return self.relu(self.fc3(h2))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc4(z))\n",
    "        h5 = self.relu(self.fc5(h4))\n",
    "        return self.sigmoid(self.fc6(h5))\n",
    "\n",
    "    def forward(self, data, temp, hard):\n",
    "        logits_z = self.encode(data.view(-1, num_initial_bits))\n",
    "        logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n",
    "\n",
    "        probs_z = F.softmax(logits_z, dim=-1)\n",
    "        posterior_distrib = torch.distributions.Categorical(probs=probs_z)\n",
    "        probs_prior = torch.ones_like(logits_z)/categorical_dim\n",
    "        prior_distrib = torch.distributions.Categorical(probs=probs_prior)\n",
    "\n",
    "        latent_z = gumbel_softmax(logits_z, temp)\n",
    "        latent_z = latent_z.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "        probs_x = self.decode(latent_z)\n",
    "        dist_x = torch.distributions.Bernoulli(probs=probs_x, validate_args=False)\n",
    "\n",
    "        rec_loss = dist_x.log_prob(data.view(-1, num_initial_bits)).sum(dim=-1)\n",
    "        logits_z_log = F.log_softmax(logits_z, dim=-1)\n",
    "\n",
    "        KL = (posterior_distrib.probs * (logits_z_log - prior_distrib.probs.log())).view(-1, latent_dim * categorical_dim).sum(dim=-1)\n",
    "        elbo = rec_loss - KL\n",
    "        loss = -elbo.mean()\n",
    "        return loss, KL.mean(), rec_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce559f0d",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs):\n",
    "    global_batch_idx = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        temp = temperature\n",
    "\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            global_batch_idx += 1\n",
    "            # Sposta i dati sul device corretto\n",
    "            data = data['example'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss, KL, rec_loss = model(data, temp, hard)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * len(data)\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 1:\n",
    "                temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "\n",
    "            '''\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader),\n",
    "                        loss.item()))\n",
    "                print(\"Temperature : \", temp)\n",
    "            '''\n",
    "\n",
    "            if global_batch_idx % log_interval_writer == 0:\n",
    "                writer.add_scalar('KL/Train', KL, global_step=global_batch_idx)\n",
    "                writer.add_scalar('rec_loss/Train', rec_loss, global_step=global_batch_idx)\n",
    "\n",
    "\n",
    "        writer.add_scalar('Loss/Train', train_loss/len(train_loader.dataset), global_step=epoch)\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        # Sposta l'immagine campionata sulla CPU per la visualizzazione con matplotlib\n",
    "        sampled = model.sample_img(data[0].view(-1, 28*28), temp).view(28, 28).detach().cpu()\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,4))\n",
    "        fig.suptitle('Reconstructed vs Real')\n",
    "        axs[0].imshow(sampled.reshape(28,28))\n",
    "        axs[0].axis('off')\n",
    "        axs[1].imshow(data[0].reshape(28,28).detach().cpu())\n",
    "        axs[1].axis('off')\n",
    "        plt.show()\n",
    "        '''\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "            epoch, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data in enumerate(val_loader):\n",
    "                data = data['example'].to(device)\n",
    "                loss, KL, rec_loss = model(data, temp, hard=True)\n",
    "                val_loss_sum += loss.item() * len(data)\n",
    "\n",
    "        writer.add_scalar('Loss/Validation', val_loss_sum/len(val_loader.dataset), global_step=epoch)\n",
    "\n",
    "        # Log histogram of weights and gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f'Weights/{name}', param, global_step=epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'Grads/{name}', param.grad, global_step=epoch)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training completato e dati scritti su tensorboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae7ce8a",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228b6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_HFM(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.data['examples'] = self.data['examples'].apply(lambda x: torch.tensor(np.fromstring(x.strip(\"[]\"), sep=' '), dtype=torch.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        example = self.data.iloc[idx,1]\n",
    "        sample = {'example': example}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728b50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/discrete_VAE_original_HFM_train/_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Qui importi i tuoi moduli personalizzati\n",
    "from dataset import Dataset_HFM  # se li hai in moduli separati\n",
    "from model import VAE_model      # idem\n",
    "from train import train          # funzione di training\n",
    "\n",
    "# ð Rilevamento automatico del device\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"â Using MPS (Apple Silicon)\")\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        print(f\"â Using CUDA (GPU {torch.cuda.get_device_name(0)})\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"â ï¸ Using CPU (no GPU available)\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# âï¸ Impostazione sicura dei DataLoader kwargs\n",
    "def get_dataloader_kwargs(device):\n",
    "    if device.type == \"mps\":\n",
    "        return {'num_workers': 0, 'pin_memory': False}\n",
    "    elif device.type == \"cuda\":\n",
    "        return {'num_workers': 4, 'pin_memory': True}  # valori tipici per CUDA\n",
    "    else:\n",
    "        return {'num_workers': 2, 'pin_memory': False} # fallback per CPU\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ð¦ Hyperparametri\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "\n",
    "    # ð± Device & loader options\n",
    "    device = get_device()\n",
    "    kwargs = get_dataloader_kwargs(device)\n",
    "\n",
    "    # ð Dataset & DataLoader\n",
    "    dataset_HFM = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_60000.csv',\n",
    "                              root_dir='data')\n",
    "    train_loader = DataLoader(dataset_HFM, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    dataset_HFM_val = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_10000.csv',\n",
    "                                  root_dir='data')\n",
    "    val_loader = DataLoader(dataset_HFM_val, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # ð§  Modello & ottimizzatore\n",
    "    model = VAE_model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ð Training\n",
    "    train(model, optimizer, train_loader, val_loader, device=device, epochs=epochs)\n",
    "\n",
    "# Qui importi i tuoi moduli personalizzati\n",
    "from dataset import Dataset_HFM  # se li hai in moduli separati\n",
    "from model import VAE_model      # idem\n",
    "from train import train          # funzione di training\n",
    "\n",
    "# ð Rilevamento automatico del device\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"â Using MPS (Apple Silicon)\")\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        print(f\"â Using CUDA (GPU {torch.cuda.get_device_name(0)})\")\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"â ï¸ Using CPU (no GPU available)\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# âï¸ Impostazione sicura dei DataLoader kwargs\n",
    "def get_dataloader_kwargs(device):\n",
    "    if device.type == \"mps\":\n",
    "        return {'num_workers': 0, 'pin_memory': False}\n",
    "    elif device.type == \"cuda\":\n",
    "        return {'num_workers': 4, 'pin_memory': True}  # valori tipici per CUDA\n",
    "    else:\n",
    "        return {'num_workers': 2, 'pin_memory': False} # fallback per CPU\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ð¦ Hyperparametri\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "\n",
    "    # ð± Device & loader options\n",
    "    device = get_device()\n",
    "    kwargs = get_dataloader_kwargs(device)\n",
    "\n",
    "    # ð Dataset & DataLoader\n",
    "    dataset_HFM = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_60000.csv',\n",
    "                              root_dir='data')\n",
    "    train_loader = DataLoader(dataset_HFM, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    dataset_HFM_val = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_10000.csv',\n",
    "                                  root_dir='data')\n",
    "    val_loader = DataLoader(dataset_HFM_val, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    # ð§  Modello & ottimizzatore\n",
    "    model = VAE_model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # ð Training\n",
    "    train(model, optimizer, train_loader, val_loader, device=device, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40d255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Apple Silicon (MPS). num_workers set to 0 to avoid pickling issues.\n",
      "====> Epoch: 0 Average loss: 259.4679\n",
      "====> Epoch: 1 Average loss: 257.0390\n",
      "====> Epoch: 2 Average loss: 256.9899\n",
      "====> Epoch: 3 Average loss: 256.9596\n",
      "====> Epoch: 4 Average loss: 256.9341\n",
      "====> Epoch: 5 Average loss: 256.8949\n",
      "====> Epoch: 6 Average loss: 256.8497\n",
      "====> Epoch: 7 Average loss: 256.8235\n",
      "====> Epoch: 8 Average loss: 256.7887\n",
      "====> Epoch: 9 Average loss: 256.7370\n",
      "====> Epoch: 10 Average loss: 256.6819\n",
      "====> Epoch: 11 Average loss: 256.6329\n",
      "====> Epoch: 12 Average loss: 256.6014\n",
      "====> Epoch: 13 Average loss: 256.5752\n",
      "====> Epoch: 14 Average loss: 256.5535\n",
      "====> Epoch: 15 Average loss: 256.5384\n",
      "====> Epoch: 16 Average loss: 256.5250\n",
      "====> Epoch: 17 Average loss: 256.5127\n",
      "====> Epoch: 18 Average loss: 256.5019\n",
      "====> Epoch: 19 Average loss: 256.4953\n",
      "Training completato e dati scritti su tensorboard\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': False} if torch.backends.mps.is_available() else {}\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"Running on Apple Silicon (MPS). num_workers set to 0 to avoid pickling issues.\")\n",
    "        kwargs = {'num_workers': 0, 'pin_memory': False} # pin_memory a False per MPS con num_workers=0\n",
    "\n",
    "\n",
    "    # Inizializzazione del dataset e dei DataLoader\n",
    "    dataset_HFM = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_60000.csv',\n",
    "                            root_dir='data')\n",
    "    train_loader = DataLoader(\n",
    "        dataset_HFM,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    dataset_HFM_val = Dataset_HFM(csv_file='data/feat_512_g_log2_numex_10000.csv',\n",
    "                                root_dir='data')\n",
    "    val_loader = DataLoader(\n",
    "        dataset_HFM_val, # Importante: usa dataset_HFM_val qui, non dataset_HFM\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    my_model = VAE_model()\n",
    "    my_model.to(device)\n",
    "    optimizer = optim.Adam(my_model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Chiamata alla funzione train\n",
    "    train(my_model, optimizer, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdc2c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enricofrausin/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 195.8797\n",
      "====> Epoch: 1 Average loss: 161.6164\n",
      "====> Epoch: 2 Average loss: 145.2841\n",
      "====> Epoch: 3 Average loss: 137.5701\n",
      "====> Epoch: 4 Average loss: 133.0866\n",
      "====> Epoch: 5 Average loss: 129.0748\n",
      "====> Epoch: 6 Average loss: 126.0739\n",
      "====> Epoch: 7 Average loss: 123.8237\n",
      "====> Epoch: 8 Average loss: 121.8725\n",
      "====> Epoch: 9 Average loss: 120.0578\n",
      "====> Epoch: 10 Average loss: 118.0473\n",
      "====> Epoch: 11 Average loss: 116.4868\n",
      "====> Epoch: 12 Average loss: 115.2364\n",
      "====> Epoch: 13 Average loss: 114.0681\n",
      "====> Epoch: 14 Average loss: 112.9505\n",
      "Training completato e dati scritti su tensorboard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_model = VAE_model().to(device)\n",
    "\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=1e-3)\n",
    "\n",
    "train(my_model, optimizer, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
