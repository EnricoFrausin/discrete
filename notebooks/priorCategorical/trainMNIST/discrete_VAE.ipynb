{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae25902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c5167",
   "metadata": {},
   "source": [
    "# Metadata e inizializzazione datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947c43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo Apple Silicon GPU (MPS)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "batch_size = 128\n",
    "temperature = 1.0\n",
    "seed = 0\n",
    "log_interval = 100\n",
    "log_interval_writer = 100\n",
    "hard = False\n",
    "latent_dim = 20\n",
    "categorical_dim = 2\n",
    "temp_min = 0.5\n",
    "ANNEAL_RATE = 0.00003\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Utilizzo Apple Silicon GPU (MPS)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Utilizzo NVIDIA GPU (CUDA)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Utilizzo la CPU\")\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "if device.type == \"cuda\": \n",
    "    torch.cuda.manual_seed(seed)\n",
    "elif device.type == \"mps\": \n",
    "    torch.mps.manual_seed(seed)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device.type == \"cuda\" or device.type == \"mps\" else {} # pin_memory pu√≤ essere utile anche per MPS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa09a23",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29434334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data/MNIST',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    "    )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data/MNIST',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transforms.ToTensor()\n",
    "        ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c433024",
   "metadata": {},
   "source": [
    "# Gumbel-softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf79c205",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    # sample from a uniform distribution\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U.to(device) + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    \n",
    "    if not hard:\n",
    "        return y.view(-1, latent_dim * categorical_dim)\n",
    "    \n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # skip the gradient of y_hard\n",
    "    y_hard = (y_hard - y).detach() + y \n",
    "    return y_hard.view(-1, latent_dim * categorical_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c4923",
   "metadata": {},
   "source": [
    "## Gumbel-softmax alternativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gumbel_softmax(logits, tau, hard=False):\n",
    "    U = torch.rand_like(logits)\n",
    "    G = -torch.log(-torch.log(U + 1e-20) + 1e-20)\n",
    "    y = F.softmax((logits + G) / tau, dim=-1)\n",
    "\n",
    "    if hard:\n",
    "        y_hard = torch.zeros_like(y)\n",
    "        y_hard.scatter_(-1, y.argmax(dim=-1, keepdim=True), 1.0)\n",
    "        y = (y_hard - y).detach() + y  # straight-through estimator\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f19ee0",
   "metadata": {},
   "source": [
    "# Class VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, latent_dim * categorical_dim)\n",
    "        self.fc4 = nn.Linear(latent_dim * categorical_dim, 256)\n",
    "        self.fc5 = nn.Linear(256, 512)\n",
    "        self.fc6 = nn.Linear(512, 784)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def sample_img(self, img, temp, random=True):\n",
    "        with torch.no_grad():\n",
    "            logits_z = self.encode(img.view(-1, 784))\n",
    "            logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n",
    "            if random:\n",
    "                latent_z = gumbel_softmax(logits_z, temp, True)\n",
    "            else:\n",
    "                latent_z = logits_z.view(-1, latent_dim * categorical_dim)\n",
    "            logits_x = self.decode(latent_z)\n",
    "            dist_x = torch.distributions.Bernoulli(probs=logits_x)\n",
    "            sampled_img = dist_x.sample()\n",
    "        return sampled_img\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        return self.relu(self.fc3(h2))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc4(z))\n",
    "        h5 = self.relu(self.fc5(h4))\n",
    "        return self.sigmoid(self.fc6(h5))\n",
    "\n",
    "    def forward(self, data, temp, hard):\n",
    "        logits_z = self.encode(data.view(-1, 784))\n",
    "        logits_z = logits_z.view(-1, latent_dim, categorical_dim)\n",
    "\n",
    "        probs_z = F.softmax(logits_z, dim=-1)\n",
    "        posterior_distrib = torch.distributions.Categorical(probs=probs_z)\n",
    "        probs_prior = torch.ones_like(logits_z)/categorical_dim\n",
    "        prior_distrib = torch.distributions.Categorical(probs=probs_prior)\n",
    "\n",
    "        latent_z = gumbel_softmax(logits_z, temp)\n",
    "        latent_z = latent_z.view(-1, latent_dim * categorical_dim)\n",
    "\n",
    "        probs_x = self.decode(latent_z)\n",
    "        dist_x = torch.distributions.Bernoulli(probs=probs_x, validate_args=False)\n",
    "\n",
    "        rec_loss = dist_x.log_prob(data.view(-1, 784)).sum(dim=-1)\n",
    "        logits_z_log = F.log_softmax(logits_z, dim=-1)\n",
    "\n",
    "        KL = (posterior_distrib.probs * (logits_z_log - prior_distrib.probs.log())).view(-1, latent_dim * categorical_dim).sum(dim=-1)\n",
    "        elbo = rec_loss - KL\n",
    "        loss = -elbo.mean()\n",
    "        return loss, KL.mean(), rec_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce559f0d",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6f3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs):\n",
    "    global_batch_idx = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        temp = temperature\n",
    "        train_KL = 0\n",
    "\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            global_batch_idx += 1\n",
    "            # Sposta i dati sul device corretto\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss, KL, rec_loss = model(data, temp, hard)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item() * len(data)\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 1:\n",
    "                temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
    "\n",
    "            if global_batch_idx % log_interval_writer == 0:\n",
    "                writer.add_scalar('KL/Train', KL, global_step=global_batch_idx)\n",
    "                writer.add_scalar('rec_loss/Train', rec_loss, global_step=global_batch_idx)\n",
    "\n",
    "\n",
    "        writer.add_scalar('Loss/Train', train_loss/len(train_loader.dataset), global_step=epoch)\n",
    "\n",
    "\n",
    "        print('Epoch: {}/{}, Average loss: {:.4f}, Average KL: '.format(\n",
    "            epoch, epochs, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        # Validation\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, _) in enumerate(val_loader):\n",
    "                data = data.to(device)\n",
    "                loss, KL, rec_loss = model(data, temp, hard=True)\n",
    "                val_loss_sum += loss.item() * len(data)\n",
    "\n",
    "        writer.add_scalar('Loss/Validation', val_loss_sum/len(val_loader.dataset), global_step=epoch)\n",
    "\n",
    "        # Log histogram of weights and gradients\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f'Weights/{name}', param, global_step=epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f'Grads/{name}', param.grad, global_step=epoch)\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training completato e dati scritti su tensorboard\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe515aa",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/discrete_VAE_Categorical/_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2c450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enricofrausin/Programmazione/Python_nn/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 195.8797\n",
      "====> Epoch: 1 Average loss: 161.6164\n",
      "====> Epoch: 2 Average loss: 145.2841\n",
      "====> Epoch: 3 Average loss: 137.5701\n",
      "====> Epoch: 4 Average loss: 133.0866\n",
      "====> Epoch: 5 Average loss: 129.0748\n",
      "====> Epoch: 6 Average loss: 126.0739\n",
      "====> Epoch: 7 Average loss: 123.8237\n",
      "====> Epoch: 8 Average loss: 121.8725\n",
      "====> Epoch: 9 Average loss: 120.0578\n",
      "====> Epoch: 10 Average loss: 118.0473\n",
      "====> Epoch: 11 Average loss: 116.4868\n",
      "====> Epoch: 12 Average loss: 115.2364\n",
      "====> Epoch: 13 Average loss: 114.0681\n",
      "====> Epoch: 14 Average loss: 112.9505\n",
      "Training completato e dati scritti su tensorboard\n"
     ]
    }
   ],
   "source": [
    "\n",
    "my_model = VAE_model().to(device)\n",
    "optimizer = optim.Adam(my_model.parameters(), lr=1e-3)\n",
    "train(my_model, optimizer, epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Python_nn)",
   "language": "python",
   "name": "python_nn_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
